{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys,os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "import glob\n",
    "import xarray as xr\n",
    "import datetime\n",
    "\n",
    "# import yaml\n",
    "import tqdm\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import logging\n",
    "import random\n",
    "\n",
    "from typing import List, Dict, Callable, Union, Any, TypeVar, Tuple\n",
    "# from multiprocessing import cpu_count\n",
    "\n",
    "import torch.fft\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")\n",
    "\n",
    "if is_cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the data into memory\n",
    "images = []\n",
    "labels = []\n",
    "masks = []\n",
    "\n",
    "loaded = 0\n",
    "max_images = 10\n",
    "\n",
    "start_time = time.time()\n",
    "with open(\"training_512x512_128_50000.pkl\", \"rb\") as fid:\n",
    "    while True:\n",
    "        \n",
    "        try:\n",
    "            image, label, u_net_mask, image_tile_idx, image_tile_coors = pickle.load(fid)\n",
    "            images.append(np.expand_dims(image, 0))\n",
    "            labels.append(label)\n",
    "            masks.append(np.expand_dims(u_net_mask, 0))\n",
    "            \n",
    "            loaded += 1\n",
    "            \n",
    "            if len(images) == max_images:\n",
    "                break\n",
    "            \n",
    "        except Exception as E:\n",
    "            break\n",
    "            \n",
    "images = np.vstack(images)\n",
    "labels = np.vstack(labels)\n",
    "masks = np.vstack(masks)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.11396145820617676 s to load 10 (x,y) points\n"
     ]
    }
   ],
   "source": [
    "print(f\"It took {end_time - start_time} s to load {loaded} (x,y) points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 2, 512, 512), (10, 1), (10, 512, 512))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the binary model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, fcl_layers = [], dr = 0.0, output_size = 1, resnet_model = 18, pretrained = True):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.pretrained = pretrained\n",
    "        self.resnet_model = resnet_model \n",
    "        if self.resnet_model == 18:\n",
    "            resnet = models.resnet18(pretrained=self.pretrained)\n",
    "        elif self.resnet_model == 34:\n",
    "            resnet = models.resnet34(pretrained=self.pretrained)\n",
    "        elif self.resnet_model == 50:\n",
    "            resnet = models.resnet50(pretrained=self.pretrained)\n",
    "        elif self.resnet_model == 101:\n",
    "            resnet = models.resnet101(pretrained=self.pretrained)\n",
    "        elif self.resnet_model == 152:\n",
    "            resnet = models.resnet152(pretrained=self.pretrained)\n",
    "        resnet.conv1 = torch.nn.Conv1d(2, 64, (7, 7), (2, 2), (3, 3), bias=False) # Manually change color dim to match our data\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet_output_dim = resnet.fc.in_features\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fcn = self.make_fcn(self.resnet_output_dim, output_size, fcl_layers, dr)\n",
    "        \n",
    "    def make_fcn(self, input_size, output_size, fcl_layers, dr):\n",
    "        if len(fcl_layers) > 0:\n",
    "            fcn = [\n",
    "                nn.Dropout(dr),\n",
    "                nn.Linear(input_size, fcl_layers[0]),\n",
    "                nn.BatchNorm1d(fcl_layers[0]),\n",
    "                torch.nn.LeakyReLU()\n",
    "            ]\n",
    "            if len(fcl_layers) == 1:\n",
    "                fcn.append(nn.Linear(fcl_layers[0], output_size))\n",
    "            else:\n",
    "                for i in range(len(fcl_layers)-1):\n",
    "                    fcn += [\n",
    "                        nn.Linear(fcl_layers[i], fcl_layers[i+1]),\n",
    "                        nn.BatchNorm1d(fcl_layers[i+1]),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        nn.Dropout(dr)\n",
    "                    ]\n",
    "                fcn.append(nn.Linear(fcl_layers[i+1], output_size))\n",
    "        else:\n",
    "            fcn = [\n",
    "                nn.Dropout(dr),\n",
    "                nn.Linear(input_size, output_size)\n",
    "            ]\n",
    "        if output_size > 1:\n",
    "            fcn.append(torch.nn.LogSoftmax(dim=1))\n",
    "        return nn.Sequential(*fcn)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.fcn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "train_batch_size = 32\n",
    "valid_batch_size = 32\n",
    "batches_per_epoch = 500\n",
    "\n",
    "stopping_patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_train), \n",
    "    torch.from_numpy(y_train)\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_test), \n",
    "    torch.from_numpy(y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size, \n",
    "    #num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    #num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcl_layers = []\n",
    "dropout = 0.2\n",
    "output_size = 2\n",
    "resnet_model = 50\n",
    "pretrained = True\n",
    "\n",
    "model = ResNet(fcl_layers, \n",
    "               dr = dropout, \n",
    "               output_size = output_size, \n",
    "               resnet_model=resnet_model, \n",
    "               pretrained = pretrained).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-04\n",
    "weight_decay = 0.0\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_criterion = torch.nn.CrossEntropyLoss()\n",
    "test_criterion = torch.nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    patience = 1, \n",
    "    min_lr = 1.0e-10,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 0.6744 train_acc: 0.5534 lr: 0.000100000000:   1%|          | 4/500 [00:04<10:05,  1.22s/it]\n",
      "Epoch 0 test_loss: 0.6814 test_acc: 0.5146: : 4it [00:00,  5.47it/s]\n",
      "Epoch 1 train_loss: 0.4664 train_acc: 0.8155 lr: 0.000100000000:   1%|          | 4/500 [00:01<03:37,  2.28it/s]\n",
      "Epoch 1 test_loss: 0.9785 test_acc: 0.5146: : 4it [00:00,  5.33it/s]\n",
      "Epoch 2 train_loss: 0.2500 train_acc: 0.9709 lr: 0.000100000000:   1%|          | 4/500 [00:01<03:31,  2.35it/s]\n",
      "Epoch 2 test_loss: 0.4034 test_acc: 0.8155: : 4it [00:00,  5.38it/s]\n",
      "Epoch 3 train_loss: 0.2367 train_acc: 0.9806 lr: 0.000100000000:   1%|          | 4/500 [00:01<03:31,  2.35it/s]\n",
      "Epoch 3 test_loss: 0.6478 test_acc: 0.7573: : 4it [00:00,  5.71it/s]\n",
      "Epoch 4 train_loss: 0.0837 train_acc: 0.9903 lr: 0.000100000000:   1%|          | 4/500 [00:01<03:31,  2.34it/s]\n",
      "Epoch 4 test_loss: 0.4390 test_acc: 0.7476: : 4it [00:00,  5.35it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     5: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.0517 train_acc: 1.0000 lr: 0.000010000000:   1%|          | 4/500 [00:01<03:37,  2.28it/s]\n",
      "Epoch 5 test_loss: 0.3030 test_acc: 0.9029: : 4it [00:00,  5.35it/s]\n",
      "Epoch 6 train_loss: 0.1068 train_acc: 1.0000 lr: 0.000010000000:   1%|          | 4/500 [00:01<03:32,  2.34it/s]\n",
      "Epoch 6 test_loss: 0.1472 test_acc: 1.0000: : 4it [00:00,  5.70it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch_test_losses = []\n",
    "results_dict = defaultdict(list)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ### Train the model \n",
    "    model.train()\n",
    "\n",
    "    batch_loss = []\n",
    "    accuracy = [] \n",
    "        \n",
    "    # set up a custom tqdm\n",
    "    batch_group_generator = tqdm.tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=batches_per_epoch,\n",
    "        leave=True\n",
    "    )\n",
    " \n",
    "    for k, (inputs, y) in batch_group_generator:\n",
    "        \n",
    "        # Move data to the GPU, if not there already\n",
    "        inputs = inputs.to(device).float()\n",
    "        y = y.to(device).long()\n",
    "        \n",
    "        # Clear gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        pred_z_logits = model(inputs)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = train_criterion(pred_z_logits, y.squeeze(-1))\n",
    "        \n",
    "        # compute the top-1 accuracy\n",
    "        pred_z_labels = torch.argmax(pred_z_logits, 1)\n",
    "        accuracy += list((pred_z_labels == y.squeeze(1)).float().detach().cpu().numpy())\n",
    "        \n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # update tqdm\n",
    "        to_print = \"Epoch {} train_loss: {:.4f}\".format(epoch, np.mean(batch_loss))\n",
    "        to_print += \" train_acc: {:.4f}\".format(np.mean(accuracy))\n",
    "        to_print += \" lr: {:.12f}\".format(optimizer.param_groups[0]['lr'])\n",
    "        batch_group_generator.set_description(to_print)\n",
    "        batch_group_generator.update()\n",
    "                     \n",
    "        # stop the training epoch when train_batches_per_epoch have been used to update \n",
    "        # the weights to the model\n",
    "        if k >= batches_per_epoch and k > 0:\n",
    "            break\n",
    "            \n",
    "        #lr_scheduler.step(epoch + k / batches_per_epoch)\n",
    "        \n",
    "    # Compuate final performance metrics before doing validation\n",
    "    train_loss = np.mean(batch_loss)\n",
    "    train_acc = np.mean(accuracy)\n",
    "        \n",
    "    # clear the cached memory from the gpu\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ### Test the model \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_loss = []\n",
    "        accuracy = []\n",
    "        \n",
    "        # set up a custom tqdm\n",
    "        batch_group_generator = tqdm.tqdm(\n",
    "            enumerate(train_loader),\n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        for k, (inputs, y) in batch_group_generator:\n",
    "            # Move data to the GPU, if not there already\n",
    "            inputs = inputs.to(device).float()\n",
    "            y = y.to(device).long()\n",
    "            # get output from the model, given the inputs\n",
    "            pred_z_logits = model(inputs)\n",
    "            # get loss for the predicted output\n",
    "            loss = test_criterion(pred_z_logits, y.squeeze(-1))\n",
    "            batch_loss.append(loss.item())\n",
    "            # compute the accuracy\n",
    "            pred_z_labels = torch.argmax(pred_z_logits, 1)\n",
    "            accuracy += list((pred_z_labels == y.squeeze(1)).float().detach().cpu().numpy())\n",
    "            # update tqdm\n",
    "            to_print = \"Epoch {} test_loss: {:.4f}\".format(epoch, np.mean(batch_loss))\n",
    "            to_print += \" test_acc: {:.4f}\".format(np.mean(accuracy))\n",
    "            batch_group_generator.set_description(to_print)\n",
    "            batch_group_generator.update()\n",
    "\n",
    "    # Use the accuracy as the performance metric to toggle learning rate and early stopping\n",
    "    test_loss = 1 - np.mean(accuracy)\n",
    "    epoch_test_losses.append(test_loss)\n",
    "    \n",
    "    # Lower the learning rate if we are not improving\n",
    "    lr_scheduler.step(test_loss)\n",
    "\n",
    "    # Save the model if its the best so far.\n",
    "    if test_loss == min(epoch_test_losses):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': test_loss\n",
    "        }\n",
    "        torch.save(state_dict, \"best_resnet.pt\")\n",
    "        \n",
    "    # Get the last learning rate\n",
    "    learning_rate = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "    # Put things into a results dictionary -> dataframe\n",
    "    results_dict['epoch'].append(epoch)\n",
    "    results_dict['train_loss'].append(train_loss)\n",
    "    results_dict['valid_loss'].append(np.mean(batch_loss))\n",
    "    results_dict['train_accuracy'].append(train_acc)\n",
    "    results_dict['valid_accuracy'].append(np.mean(accuracy))\n",
    "    results_dict[\"learning_rate\"].append(learning_rate)\n",
    "    df = pd.DataFrame.from_dict(results_dict).reset_index()\n",
    "\n",
    "    # Save the dataframe to disk\n",
    "    df.to_csv(\"training_log_resnet.csv\", index = False)\n",
    "        \n",
    "    # Stop training if we have not improved after X epochs\n",
    "    best_epoch = [i for i,j in enumerate(epoch_test_losses) if j == min(epoch_test_losses)][0]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= stopping_patience:\n",
    "        break\n",
    "        \n",
    "    if results_dict['valid_accuracy'][-1] == 1.0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
