{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import tqdm\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "import pickle\n",
    "import joblib\n",
    "import random\n",
    "import sklearn\n",
    "import logging\n",
    "import datetime\n",
    "import torch.fft\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "#torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from scipy.signal import convolve2d\n",
    "from torch.optim.lr_scheduler import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from typing import List, Dict, Callable, Union, Any, TypeVar, Tuple\n",
    "\n",
    "from unet_losses import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the GPU device id, or CPU if no GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")\n",
    "\n",
    "if is_cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader(Dataset):\n",
    "    \n",
    "    def __init__(self, fn, max_buffer_size = 5000, max_images = 40000, shuffle = True, normalize = True):\n",
    "        self.fn = fn\n",
    "        self.buffer = []\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        self.shuffle = shuffle\n",
    "        self.max_images = max_images\n",
    "            \n",
    "        self.fid = open(self.fn, \"rb\")\n",
    "        self.loaded = 0 \n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.normalize = normalize\n",
    "        self.mean = np.mean([0.485, 0.456, 0.406])\n",
    "        self.std = np.mean([0.229, 0.224, 0.225])\n",
    "        \n",
    "    def __getitem__(self, idx):    \n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        while True:\n",
    "        \n",
    "            try:\n",
    "                data = joblib.load(self.fid)\n",
    "                image, label, mask = data\n",
    "                \n",
    "                image /= 255.0\n",
    "                \n",
    "                if self.normalize:\n",
    "                    image /= 255.0\n",
    "                    image = (image - self.mean) / self.std\n",
    "                \n",
    "                image = torch.FloatTensor(image)\n",
    "                #label = torch.LongTensor([label])\n",
    "                mask = torch.FloatTensor(mask.toarray())\n",
    "                \n",
    "                data = (image, mask)\n",
    "\n",
    "                self.loaded += 1\n",
    "\n",
    "                if not self.shuffle:\n",
    "                    return data\n",
    "                \n",
    "                self.buffer.append(data)\n",
    "                random.shuffle(self.buffer)\n",
    "\n",
    "                if len(self.buffer) > self.max_buffer_size:\n",
    "                    self.buffer = self.buffer[:self.max_buffer_size]\n",
    "\n",
    "                if self.epoch > 0:\n",
    "                    return self.buffer.pop()\n",
    "\n",
    "                else: # wait until all data has been seen before sampling from the buffer\n",
    "                    return data\n",
    "\n",
    "\n",
    "            except EOFError:\n",
    "                self.fid = open(self.fn, \"rb\")\n",
    "                self.loaded = 0\n",
    "                continue\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return self.max_images\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.loaded == self.__len__():\n",
    "            self.fid = open(self.fn, \"rb\")\n",
    "            self.loaded = 0\n",
    "            self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the binary model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_chs=(2, 64, 128, 256, 512, 1024),\n",
    "                 dec_chs=(1024, 512, 256, 128, 64),\n",
    "                 num_class=1,\n",
    "                 retain_dim=False, out_sz=(572,572)):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "        self.out_sz = out_sz\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "        out = torch.nn.Sigmoid()(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_channels)\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class, color_dim = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_model.conv1 = torch.nn.Conv2d(color_dim, 64, (7, 7), (2, 2), (3, 3), bias=False) \n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        \n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(color_dim , 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "        out = torch.nn.Sigmoid()(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/unet_double_compare/holo_data.yml\") as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = conf[\"data\"][\"tile_size\"]\n",
    "step_size = conf[\"data\"][\"step_size\"]\n",
    "data_path = conf[\"data\"][\"output_path\"]\n",
    "\n",
    "fn_train = f\"{data_path}/training_{tile_size}_{step_size}.pkl\"\n",
    "fn_valid = f\"{data_path}/validation_{tile_size}_{step_size}.pkl\"\n",
    "\n",
    "epochs = conf[\"trainer\"][\"epochs\"]\n",
    "train_batch_size = conf[\"trainer\"][\"train_batch_size\"]\n",
    "valid_batch_size = conf[\"trainer\"][\"valid_batch_size\"]\n",
    "batches_per_epoch = conf[\"trainer\"][\"batches_per_epoch\"]\n",
    "stopping_patience = conf[\"trainer\"][\"stopping_patience\"]\n",
    "model_loc = conf[\"trainer\"][\"output_path\"]\n",
    "\n",
    "fcl_layers = conf[\"resnet\"][\"fcl_layers\"]\n",
    "dropout = conf[\"resnet\"][\"dropout\"]\n",
    "output_size = conf[\"resnet\"][\"output_size\"]\n",
    "resnet_model = conf[\"resnet\"][\"resnet_model\"]\n",
    "pretrained = conf[\"resnet\"][\"pretrained\"]\n",
    "\n",
    "learning_rate = conf[\"optimizer\"][\"learning_rate\"]\n",
    "weight_decay = conf[\"optimizer\"][\"weight_decay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataReader(\n",
    "    fn_train, \n",
    "    max_images = int(0.8 * conf[\"data\"][\"total_training\"]), \n",
    "    max_buffer_size = int(0.1 * conf[\"data\"][\"total_training\"]), \n",
    "    shuffle = True, \n",
    "    normalize = False\n",
    ")\n",
    "\n",
    "test_dataset = DataReader(\n",
    "    fn_valid, \n",
    "    max_images = int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size = int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    shuffle = False, \n",
    "    normalize = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size, \n",
    "    #num_workers=8, # can increase to number of CPUs you asked for in launch script; usually 8\n",
    "    pin_memory=True,\n",
    "    shuffle=False) # let the reader do the shuffling\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    #num_workers=8,\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_dim = True\n",
    "mask_size = (512,512)\n",
    "\n",
    "encoder_channels = (2, 16, 32, 64, 128, 256)\n",
    "decoder_channels = [256, 128, 64, 32, 16]\n",
    "\n",
    "# unet = UNet(\n",
    "#     enc_chs = encoder_channels,\n",
    "#     dec_chs = decoder_channels,\n",
    "#     retain_dim = retain_dim,\n",
    "#     out_sz = mask_size,\n",
    "#     num_class = 1\n",
    "# ).to(device)\n",
    "\n",
    "unet = ResNetUNet(n_class = 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in unet.parameters())\n",
    "trainable_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18313833"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    unet.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = 0.001 #1e-1\n",
    "\n",
    "def element_weighted_MSELoss(y_hat, y):\n",
    "    weights = (1-mix)*y + mix\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "    loss = criterion(y_hat, y)\n",
    "    loss = loss * weights\n",
    "    return loss.sum() / weights.sum()\n",
    "\n",
    "\n",
    "train_criterion = DiceBCELoss() #element_weighted_MSELoss #FocalTverskyLoss()\n",
    "test_criterion = DiceLoss() #DiceBCELoss()\n",
    "\n",
    "#train_criterion = torch.nn.SmoothL1Loss() # Huber (MSE, but once converges, MAE)\n",
    "#test_criterion = torch.nn.L1Loss() # MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_patience = 5\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    patience = 1, \n",
    "    min_lr = 1.0e-10,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 1.6077 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 0 test_loss: 0.9984: : 157it [01:37,  1.61it/s]\n",
      "Epoch 1 train_loss: 1.5367 lr: 0.000100000000: 100%|██████████| 200/200 [01:19<00:00,  2.50it/s]\n",
      "Epoch 1 test_loss: 0.9982: : 157it [01:36,  1.63it/s]\n",
      "Epoch 2 train_loss: 1.4405 lr: 0.000100000000: 100%|██████████| 200/200 [01:21<00:00,  2.46it/s]\n",
      "Epoch 2 test_loss: 0.9978: : 157it [01:32,  1.69it/s]\n",
      "Epoch 3 train_loss: 1.3414 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 3 test_loss: 0.9971: : 157it [01:31,  1.72it/s]\n",
      "Epoch 4 train_loss: 1.2503 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 4 test_loss: 0.9962: : 157it [01:32,  1.69it/s]\n",
      "Epoch 5 train_loss: 1.1847 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 5 test_loss: 0.9948: : 157it [01:30,  1.73it/s]\n",
      "Epoch 6 train_loss: 1.1267 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 6 test_loss: 0.9931: : 157it [01:32,  1.70it/s]\n",
      "Epoch 7 train_loss: 1.0906 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 7 test_loss: 0.9905: : 157it [01:32,  1.69it/s]\n",
      "Epoch 8 train_loss: 1.0581 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 8 test_loss: 0.9873: : 157it [01:32,  1.70it/s]\n",
      "Epoch 9 train_loss: 1.0381 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 9 test_loss: 0.9836: : 157it [01:32,  1.70it/s]\n",
      "Epoch 10 train_loss: 1.0199 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.41it/s]\n",
      "Epoch 10 test_loss: 0.9765: : 157it [01:32,  1.70it/s]\n",
      "Epoch 11 train_loss: 1.0038 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 11 test_loss: 0.9647: : 157it [01:33,  1.68it/s]\n",
      "Epoch 12 train_loss: 0.9857 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 12 test_loss: 0.9494: : 157it [01:33,  1.68it/s]\n",
      "Epoch 13 train_loss: 0.9559 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 13 test_loss: 0.9028: : 157it [01:32,  1.70it/s]\n",
      "Epoch 14 train_loss: 0.8737 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 14 test_loss: 0.7421: : 157it [01:33,  1.69it/s]\n",
      "Epoch 15 train_loss: 0.6454 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 15 test_loss: 0.3260: : 157it [01:29,  1.76it/s]\n",
      "Epoch 16 train_loss: 0.3287 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 16 test_loss: 0.1643: : 157it [01:32,  1.70it/s]\n",
      "Epoch 17 train_loss: 0.1874 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 17 test_loss: 0.3873: : 157it [01:30,  1.73it/s]\n",
      "Epoch 18 train_loss: 0.1299 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 18 test_loss: 0.1095: : 157it [01:33,  1.68it/s]\n",
      "Epoch 19 train_loss: 0.0909 lr: 0.000100000000: 100%|██████████| 200/200 [01:23<00:00,  2.40it/s]\n",
      "Epoch 19 test_loss: 0.0756: : 157it [01:32,  1.69it/s]\n",
      "Epoch 20 train_loss: 0.0955 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 20 test_loss: 0.0639: : 157it [01:34,  1.66it/s]\n",
      "Epoch 21 train_loss: 0.0884 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 21 test_loss: 0.0611: : 157it [01:33,  1.67it/s]\n",
      "Epoch 22 train_loss: 0.0834 lr: 0.000100000000: 100%|██████████| 200/200 [01:23<00:00,  2.40it/s]\n",
      "Epoch 22 test_loss: 0.0654: : 157it [01:32,  1.71it/s]\n",
      "Epoch 23 train_loss: 0.0702 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 23 test_loss: 0.0543: : 157it [01:33,  1.68it/s]\n",
      "Epoch 24 train_loss: 0.0601 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 24 test_loss: 0.0517: : 157it [01:30,  1.73it/s]\n",
      "Epoch 25 train_loss: 0.0559 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 25 test_loss: 0.0465: : 157it [01:31,  1.71it/s]\n",
      "Epoch 26 train_loss: 0.0684 lr: 0.000100000000: 100%|██████████| 200/200 [01:22<00:00,  2.44it/s]\n",
      "Epoch 26 test_loss: 0.0643: : 157it [01:30,  1.73it/s]\n",
      "Epoch 27 train_loss: 0.0975 lr: 0.000100000000: 100%|██████████| 200/200 [01:21<00:00,  2.45it/s]\n",
      "Epoch 27 test_loss: 0.0784: : 157it [01:31,  1.71it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 train_loss: 0.0626 lr: 0.000010000000: 100%|██████████| 200/200 [01:21<00:00,  2.44it/s]\n",
      "Epoch 28 test_loss: 0.0463: : 157it [01:33,  1.67it/s]\n",
      "Epoch 29 train_loss: 0.0666 lr: 0.000010000000: 100%|██████████| 200/200 [01:22<00:00,  2.44it/s]\n",
      "Epoch 29 test_loss: 0.0468: : 157it [01:33,  1.67it/s]\n",
      "Epoch 30 train_loss: 0.0650 lr: 0.000010000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 30 test_loss: 0.0459: : 157it [01:31,  1.72it/s]\n",
      "Epoch 31 train_loss: 0.0664 lr: 0.000010000000: 100%|██████████| 200/200 [01:22<00:00,  2.44it/s]\n",
      "Epoch 31 test_loss: 0.0451: : 157it [01:33,  1.68it/s]\n",
      "Epoch 32 train_loss: 0.0683 lr: 0.000010000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 32 test_loss: 0.0443: : 157it [01:44,  1.51it/s]\n",
      "Epoch 33 train_loss: 0.0463 lr: 0.000010000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 33 test_loss: 0.0440: : 157it [01:34,  1.66it/s]\n",
      "Epoch 34 train_loss: 0.0475 lr: 0.000010000000: 100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n",
      "Epoch 34 test_loss: 0.0432: : 157it [01:31,  1.71it/s]\n",
      "Epoch 35 train_loss: 0.0518 lr: 0.000010000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 35 test_loss: 0.0432: : 157it [01:32,  1.69it/s]\n",
      "Epoch 36 train_loss: 0.0558 lr: 0.000010000000: 100%|██████████| 200/200 [01:21<00:00,  2.44it/s]\n",
      "Epoch 36 test_loss: 0.0423: : 157it [01:33,  1.67it/s]\n",
      "Epoch 37 train_loss: 0.0558 lr: 0.000010000000: 100%|██████████| 200/200 [01:21<00:00,  2.44it/s]\n",
      "Epoch 37 test_loss: 0.0426: : 157it [01:31,  1.71it/s]\n",
      "Epoch 38 train_loss: 0.0388 lr: 0.000010000000: 100%|██████████| 200/200 [01:21<00:00,  2.44it/s]\n",
      "Epoch 38 test_loss: 0.0426: : 157it [01:38,  1.60it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 train_loss: 0.0528 lr: 0.000001000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 39 test_loss: 0.0425: : 157it [01:35,  1.64it/s]\n",
      "Epoch 40 train_loss: 0.0546 lr: 0.000001000000: 100%|██████████| 200/200 [01:22<00:00,  2.43it/s]\n",
      "Epoch 40 test_loss: 0.0424: : 157it [01:37,  1.62it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 train_loss: 0.0674 lr: 0.000000100000: 100%|██████████| 200/200 [01:22<00:00,  2.44it/s]\n",
      "Epoch 41 test_loss: 0.0424: : 157it [01:32,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch_test_losses = []\n",
    "results_dict = defaultdict(list)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ### Train the model \n",
    "    unet.train()\n",
    "\n",
    "    batch_loss = []\n",
    "        \n",
    "    # set up a custom tqdm\n",
    "    batch_group_generator = tqdm.tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=batches_per_epoch,\n",
    "        leave=True\n",
    "    )\n",
    " \n",
    "    for k, (inputs, y) in batch_group_generator:\n",
    "        \n",
    "        # Move data to the GPU, if not there already\n",
    "        inputs = inputs.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        # Clear gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        pred_mask = unet(inputs)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = train_criterion(pred_mask, y)\n",
    "                \n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # update tqdm\n",
    "        to_print = \"Epoch {} train_loss: {:.4f}\".format(epoch, np.mean(batch_loss))\n",
    "        to_print += \" lr: {:.12f}\".format(optimizer.param_groups[0]['lr'])\n",
    "        batch_group_generator.set_description(to_print)\n",
    "        batch_group_generator.update()\n",
    "                     \n",
    "        # stop the training epoch when train_batches_per_epoch have been used to update \n",
    "        # the weights to the model\n",
    "        if k >= batches_per_epoch and k > 0:\n",
    "            break\n",
    "            \n",
    "        #lr_scheduler.step(epoch + k / batches_per_epoch)\n",
    "        \n",
    "    # Compuate final performance metrics before doing validation\n",
    "    train_loss = np.mean(batch_loss)\n",
    "        \n",
    "    # clear the cached memory from the gpu\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ### Test the model \n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_loss = []\n",
    "        \n",
    "        # set up a custom tqdm\n",
    "        batch_group_generator = tqdm.tqdm(\n",
    "            enumerate(test_loader),\n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        for k, (inputs, y) in batch_group_generator:\n",
    "            # Move data to the GPU, if not there already\n",
    "            inputs = inputs.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            # get output from the model, given the inputs\n",
    "            pred_mask = unet(inputs)\n",
    "            # get loss for the predicted output\n",
    "            loss = test_criterion(pred_mask, y)\n",
    "            batch_loss.append(loss.item())\n",
    "            # update tqdm\n",
    "            to_print = \"Epoch {} test_loss: {:.4f}\".format(epoch, np.mean(batch_loss))\n",
    "            batch_group_generator.set_description(to_print)\n",
    "            batch_group_generator.update()\n",
    "\n",
    "    # Use the accuracy as the performance metric to toggle learning rate and early stopping\n",
    "    test_loss = np.mean(batch_loss)\n",
    "    epoch_test_losses.append(test_loss)\n",
    "    \n",
    "    # Lower the learning rate if we are not improving\n",
    "    lr_scheduler.step(test_loss)\n",
    "\n",
    "    # Save the model if its the best so far.\n",
    "    if test_loss == min(epoch_test_losses):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unet.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': test_loss\n",
    "        }\n",
    "        #TODO: add directory\n",
    "        torch.save(state_dict, f\"{model_loc}/best.pt\")\n",
    "        \n",
    "    # Get the last learning rate\n",
    "    learning_rate = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "    # Put things into a results dictionary -> dataframe\n",
    "    results_dict['epoch'].append(epoch)\n",
    "    results_dict['train_loss'].append(train_loss)\n",
    "    results_dict['valid_loss'].append(np.mean(batch_loss))\n",
    "    results_dict[\"learning_rate\"].append(learning_rate)\n",
    "    df = pd.DataFrame.from_dict(results_dict).reset_index()\n",
    "\n",
    "    # Save the dataframe to disk\n",
    "    #TODO: add directory\n",
    "    df.to_csv(f\"{model_loc}/training_log.csv\", index = False)\n",
    "        \n",
    "    # Stop training if we have not improved after X epochs (stopping patience)\n",
    "    best_epoch = [i for i,j in enumerate(epoch_test_losses) if j == min(epoch_test_losses)][0]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= stopping_patience:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
