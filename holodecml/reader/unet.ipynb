{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375ae937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys,os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "import glob\n",
    "import xarray as xr\n",
    "import datetime\n",
    "\n",
    "# import yaml\n",
    "import tqdm\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import logging\n",
    "import random\n",
    "\n",
    "from typing import List, Dict, Callable, Union, Any, TypeVar, Tuple\n",
    "# from multiprocessing import cpu_count\n",
    "\n",
    "import torch.fft\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15680208",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")\n",
    "\n",
    "if is_cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf5585",
   "metadata": {},
   "source": [
    "### Load data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57a50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the data into memory\n",
    "images = []\n",
    "labels = []\n",
    "masks = []\n",
    "\n",
    "loaded = 0\n",
    "# when training, load 10k to 20k images at once\n",
    "max_images = 10\n",
    "\n",
    "start_time = time.time()\n",
    "with open(\"/glade/work/schreck/repos/HOLO/holodec-ml/holodecml/reader/training_512x512_128_50000.pkl\", \"rb\") as fid:\n",
    "    while True:\n",
    "        \n",
    "        try:\n",
    "            image, label, u_net_mask, image_tile_idx, image_tile_coors = pickle.load(fid)\n",
    "            images.append(np.expand_dims(image, 0))\n",
    "            labels.append(label)\n",
    "            masks.append(np.expand_dims(u_net_mask, 0))\n",
    "            \n",
    "            loaded += 1\n",
    "            \n",
    "            if len(images) == max_images:\n",
    "                break\n",
    "            \n",
    "        except Exception as E:\n",
    "            break\n",
    "            \n",
    "images = np.vstack(images)\n",
    "labels = np.vstack(labels)\n",
    "masks = np.vstack(masks)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964ffbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.10592508316040039 s to load 10 (x,y) points\n"
     ]
    }
   ],
   "source": [
    "print(f\"It took {end_time - start_time} s to load {loaded} (x,y) points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8190dbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 2, 512, 512), (10, 1), (10, 512, 512))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5837aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa2b485",
   "metadata": {},
   "source": [
    "### Load the binary model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88126365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_chs=(2, 64, 128, 256, 512, 1024),\n",
    "                 dec_chs=(1024, 512, 256, 128, 64),\n",
    "                 num_class=1,\n",
    "                 retain_dim=False, out_sz=(572,572)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "        self.out_sz = out_sz\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86741cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "train_batch_size = 32\n",
    "valid_batch_size = 32\n",
    "batches_per_epoch = 100\n",
    "\n",
    "stopping_patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b790315",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_train), \n",
    "    torch.from_numpy(y_train)\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(X_test), \n",
    "    torch.from_numpy(y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af04668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size, \n",
    "    #num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    #num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c79cb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcl_layers = []\n",
    "dropout = 0.2\n",
    "output_size = 2\n",
    "resnet_model = 50\n",
    "pretrained = True\n",
    "\n",
    "unet = UNet(retain_dim = True,\n",
    "            out_sz = (512,512)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb13082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-04\n",
    "weight_decay = 0.0\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    unet.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1420ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_criterion = torch.nn.SmoothL1Loss() # Huber (MSE, but once converges, MAE)\n",
    "test_criterion = torch.nn.L1Loss() # MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f3fd0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    patience = 1, \n",
    "    min_lr = 1.0e-10,\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 1.3953 lr: 0.000100000000:   0%|          | 1/500 [00:40<5:39:44, 40.85s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "epoch_test_losses = []\n",
    "results_dict = defaultdict(list)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ### Train the model \n",
    "    unet.train()\n",
    "\n",
    "    batch_loss = []\n",
    "        \n",
    "    # set up a custom tqdm\n",
    "    batch_group_generator = tqdm.tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=batches_per_epoch,\n",
    "        leave=True\n",
    "    )\n",
    " \n",
    "    for k, (inputs, y) in batch_group_generator:\n",
    "        \n",
    "        # Move data to the GPU, if not there already\n",
    "        inputs = inputs.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        # Clear gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        pred_mask = unet(inputs)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = train_criterion(pred_mask, y)\n",
    "                \n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # update tqdm\n",
    "        to_print = \"Epoch {} train_loss: {:.4f}\".format(epoch, np.mean(batch_loss))\n",
    "        to_print += \" lr: {:.12f}\".format(optimizer.param_groups[0]['lr'])\n",
    "        batch_group_generator.set_description(to_print)\n",
    "        batch_group_generator.update()\n",
    "                     \n",
    "        # stop the training epoch when train_batches_per_epoch have been used to update \n",
    "        # the weights to the model\n",
    "        if k >= batches_per_epoch and k > 0:\n",
    "            break\n",
    "            \n",
    "        #lr_scheduler.step(epoch + k / batches_per_epoch)\n",
    "        \n",
    "    # Compuate final performance metrics before doing validation\n",
    "    train_loss = np.mean(batch_loss)\n",
    "        \n",
    "    # clear the cached memory from the gpu\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ### Test the model \n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_loss = []\n",
    "        \n",
    "        # set up a custom tqdm\n",
    "        batch_group_generator = tqdm.tqdm(\n",
    "            enumerate(train_loader),\n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        for k, (inputs, y) in batch_group_generator:\n",
    "            # Move data to the GPU, if not there already\n",
    "            inputs = inputs.to(device).float()\n",
    "            y = y.to(device).long()\n",
    "            # get output from the model, given the inputs\n",
    "            pred_mask = unet(inputs)\n",
    "            # get loss for the predicted output\n",
    "            loss = test_criterion(pred_mask, y)\n",
    "            batch_loss.append(loss.item())\n",
    "            # update tqdm\n",
    "            to_print = \"Epoch {} test_loss: {:.4f}\".format(epoch, np.mean(batch_loss))\n",
    "            batch_group_generator.set_description(to_print)\n",
    "            batch_group_generator.update()\n",
    "\n",
    "    # Use the accuracy as the performance metric to toggle learning rate and early stopping\n",
    "    test_loss = np.mean(batch_loss)\n",
    "    epoch_test_losses.append(test_loss)\n",
    "    \n",
    "    # Lower the learning rate if we are not improving\n",
    "    lr_scheduler.step(test_loss)\n",
    "\n",
    "    # Save the model if its the best so far.\n",
    "    if test_loss == min(epoch_test_losses):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unet.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': test_loss\n",
    "        }\n",
    "        #TODO: add directory\n",
    "        torch.save(state_dict, \"best_unet.pt\")\n",
    "        \n",
    "    # Get the last learning rate\n",
    "    learning_rate = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "    # Put things into a results dictionary -> dataframe\n",
    "    results_dict['epoch'].append(epoch)\n",
    "    results_dict['train_loss'].append(train_loss)\n",
    "    results_dict['valid_loss'].append(np.mean(batch_loss))\n",
    "    results_dict[\"learning_rate\"].append(learning_rate)\n",
    "    df = pd.DataFrame.from_dict(results_dict).reset_index()\n",
    "\n",
    "    # Save the dataframe to disk\n",
    "    #TODO: add directory\n",
    "    df.to_csv(\"training_log_unet.csv\", index = False)\n",
    "        \n",
    "    # Stop training if we have not improved after X epochs (stopping patience)\n",
    "    best_epoch = [i for i,j in enumerate(epoch_test_losses) if j == min(epoch_test_losses)][0]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= stopping_patience:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7253d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
