{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.disable_eager_execution()\n",
    "\n",
    "import sys \n",
    "import yaml\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy.fft as FFT\n",
    "from typing import List, Dict\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from tensorflow.keras.layers import (Input, Conv2D, Dense, Flatten, BatchNormalization,\n",
    "                                     MaxPool2D, RepeatVector, Lambda, Activation,\n",
    "                                     LeakyReLU, Dropout)\n",
    "from tensorflow.keras.models import Model, save_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from keras_radam import RAdam\n",
    "from keras_radam.training import RAdamOptimizer\n",
    "\n",
    "from holodecml.library.losses import SymmetricCrossEntropy\n",
    "from holodecml.library.callbacks import get_callbacks\n",
    "from holodecml.library.FourierOpticsLib import OpticsFFT, OpticsIFFT\n",
    "\n",
    "from multiprocessing import cpu_count, Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file) as config_file:\n",
    "    config = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(config[\"path_save\"])\n",
    "except Exception as E:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up some globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = config[\"path_data\"]\n",
    "num_particles = config[\"num_particles\"]\n",
    "split = 'train'\n",
    "subset = False\n",
    "output_cols = [\"x\", \"y\", \"z\", \"d\", \"hid\"] \n",
    "\n",
    "batch_size = config[\"conv2d_network\"][\"batch_size\"]\n",
    "\n",
    "input_shape = (600, 400, 4)\n",
    "\n",
    "n_particles = config[\"num_particles\"]\n",
    "output_channels = len(output_cols) - 1\n",
    "\n",
    "maxnum_particles = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "# Stream output to stdout\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "root.addHandler(ch)\n",
    "#########\n",
    "\n",
    "# Save the log file\n",
    "fp = os.path.join('run2/log.txt')\n",
    "fh = logging.FileHandler(fp,\n",
    "                         mode='a+',\n",
    "                         encoding='utf-8')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "root.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles_dict = {\n",
    "    1: '1particle',\n",
    "    3: '3particle',\n",
    "    'multi': 'multiparticle',\n",
    "    '50-100': '50-100'}\n",
    "\n",
    "split_dict = {\n",
    "    'train' : 'training',\n",
    "    'test'   : 'test',\n",
    "    'valid': 'validation'}\n",
    "\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(\n",
    "        \n",
    "        self, \n",
    "        path_data: str, \n",
    "        num_particles: int, \n",
    "        split: str, \n",
    "        subset: bool, \n",
    "        output_cols: List[str], \n",
    "        batch_size: int, \n",
    "        shuffle: bool = True,\n",
    "        maxnum_particles: int = False,\n",
    "        scaler: Dict[str, str] = False) -> None:\n",
    "        \n",
    "        'Initialization'\n",
    "        self.ds = self.open_dataset(path_data, num_particles, split)\n",
    "        self.batch_size = batch_size\n",
    "        self.output_cols = [x for x in output_cols if x != 'hid']        \n",
    "        self.subset = subset\n",
    "        self.hologram_numbers = self.ds.hologram_number.values\n",
    "        if shuffle:\n",
    "            random.shuffle(self.hologram_numbers)\n",
    "        self.num_particles = num_particles\n",
    "        self.xsize = len(self.ds.xsize.values)\n",
    "        self.ysize = len(self.ds.ysize.values)\n",
    "        self.shuffle = shuffle\n",
    "        self.maxnum_particles = maxnum_particles\n",
    "                \n",
    "        if not scaler:\n",
    "            self.scaler = {col: StandardScaler() for col in output_cols}\n",
    "            for col in output_cols:\n",
    "                scale = self.ds[col].values\n",
    "#                 if col == \"x\":\n",
    "#                     scale = np.array([(x + 888) / 2.96 for x in scale])\n",
    "#                 if col == \"y\":\n",
    "#                     scale = np.array([(y + 592) / 2.96 for y in scale])\n",
    "                self.scaler[col].fit(scale.reshape(scale.shape[-1], -1))\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "        \n",
    "    def get_transform(self):\n",
    "        return self.scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return math.ceil(len(self.hologram_numbers) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        'Generate one batch of data'\n",
    "        holograms = self.hologram_numbers[\n",
    "            idx * self.batch_size: (idx + 1) * self.batch_size\n",
    "        ]\n",
    "        x_out, y_out, w_out = self._batch(holograms)\n",
    "        return x_out, y_out\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            random.shuffle(self.hologram_numbers)\n",
    "            \n",
    "    def standardize(self, X):\n",
    "        X = (X-np.mean(X)) / (np.std(X))\n",
    "        return X\n",
    "    \n",
    "    def reshape(self, X):\n",
    "        x, y = X.shape\n",
    "        return X.reshape((x,y,1))\n",
    "            \n",
    "    def _batch(self, holograms: List[int]):\n",
    "        'Create a batch of data'\n",
    "        try:\n",
    "            x_out = np.zeros((\n",
    "                len(holograms), self.xsize, self.ysize, 4\n",
    "            ))\n",
    "            y_out = np.zeros((\n",
    "                len(holograms), \n",
    "                self.maxnum_particles if self.maxnum_particles else self.num_particles, \n",
    "                len(self.output_cols)\n",
    "            ))\n",
    "            b_out = np.zeros((\n",
    "                len(holograms), \n",
    "                self.maxnum_particles if self.maxnum_particles else self.num_particles, \n",
    "                1\n",
    "            ))\n",
    "            a = time.time()\n",
    "            for k, hologram in enumerate(holograms):\n",
    "                #im = self.ds[\"image\"][hologram].values\n",
    "                #x_out[k] = (im-np.mean(im)) / (np.std(im))\n",
    "                \n",
    "                im = self.ds[\"image\"][hologram].values\n",
    "                A = self.standardize(im)\n",
    "                F = OpticsFFT(A)                     \n",
    "                R = self.reshape(self.standardize(np.log(np.abs(F))))\n",
    "                P = self.reshape(self.standardize(np.real(F)))\n",
    "                Q = self.reshape(self.standardize(np.imag(F)))\n",
    "                x_out[k] = np.concatenate((self.reshape(A), R, P, Q), axis = 2)\n",
    "                \n",
    "                particles = np.where(self.ds[\"hid\"] == hologram + 1)[0]  \n",
    "                for l, p in enumerate(particles):\n",
    "                    for m, col in enumerate(self.output_cols):\n",
    "                        val = self.ds[col].values[p]\n",
    "#                         if col == \"x\":\n",
    "#                             val = (val + 888) / 2.96\n",
    "#                         if col == \"y\":\n",
    "#                             val = (val + 592) / 2.96\n",
    "                        \n",
    "                        y_out[k, l, m] = self.scaler[col].transform(\n",
    "                            val.reshape(1, -1)\n",
    "                        )\n",
    "                    b_out[k, l, 0] = 1\n",
    "                    \n",
    "                if self.maxnum_particles and len(particles) < self.maxnum_particles:\n",
    "                    for l in range(len(particles), self.maxnum_particles):\n",
    "                        for m, col in enumerate(self.output_cols):\n",
    "                            val = y_out[k, l, m]\n",
    "                            y_out[k, l, m] = self.scaler[col].transform(\n",
    "                                val.reshape(1, -1)\n",
    "                            )\n",
    "                            \n",
    "                    #b_out[k, l, 0] = 0\n",
    "            #x_out = np.expand_dims(x_out, axis=-1)\n",
    "            return x_out, [b_out, y_out], [[None], [None]] #class weights option\n",
    "        \n",
    "        except:\n",
    "            print(traceback.print_exc())\n",
    "    \n",
    "    def open_dataset(self, path_data, num_particles, split):\n",
    "        \"\"\"\n",
    "        Opens a HOLODEC file\n",
    "\n",
    "        Args: \n",
    "            path_data: (str) Path to dataset directory\n",
    "            num_particles: (int or str) Number of particles per hologram\n",
    "            split: (str) Dataset split of either 'train', 'valid', or 'test'\n",
    "\n",
    "        Returns:\n",
    "            ds: (xarray Dataset) Opened dataset\n",
    "        \"\"\"\n",
    "        path_data = os.path.join(path_data, self.dataset_name(num_particles, split))\n",
    "\n",
    "        if not os.path.isfile(path_data):\n",
    "            print(f\"Data file does not exist at {path_data}. Exiting.\")\n",
    "            raise \n",
    "\n",
    "        ds = xr.open_dataset(path_data)\n",
    "        return ds\n",
    "    \n",
    "    def dataset_name(self, num_particles, split, file_extension='nc'):\n",
    "        \"\"\"\n",
    "        Return the dataset filename given user inputs\n",
    "\n",
    "        Args: \n",
    "            num_particles: (int or str) Number of particles per hologram\n",
    "            split: (str) Dataset split of either 'train', 'valid', or 'test'\n",
    "            file_extension: (str) Dataset file extension\n",
    "\n",
    "        Returns:\n",
    "            ds_name: (str) Dataset name\n",
    "        \"\"\"\n",
    "\n",
    "        valid = [1,3,'multi','50-100']\n",
    "        if num_particles not in valid:\n",
    "            raise ValueError(\"results: num_particles must be one of %r.\" % valid)\n",
    "        num_particles = num_particles_dict[num_particles]\n",
    "\n",
    "        valid = ['train','test','valid']\n",
    "        if split not in valid:\n",
    "            raise ValueError(\"results: split must be one of %r.\" % valid)\n",
    "        split = split_dict[split]\n",
    "        \n",
    "        if num_particles == \"50-100\":\n",
    "            ds_name = f'synthetic_holograms_{num_particles}particle_monodisperse_{split}.{file_extension}'\n",
    "        else:\n",
    "            ds_name = f'synthetic_holograms_{num_particles}_{split}.{file_extension}'\n",
    "        return ds_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(\n",
    "    path_data, num_particles, \"train\", subset, \n",
    "    output_cols, batch_size, maxnum_particles = maxnum_particles, \n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = {col: StandardScaler() for col in train_gen.output_cols}\n",
    "# for col in train_gen.output_cols:\n",
    "#     scale = train_gen.ds[col].values\n",
    "#     scaler[col].fit(scale.reshape(scale.shape[-1], -1))\n",
    "#     result = scaler[col].transform(scale.reshape(scale.shape[-1], -1))\n",
    "#     print(col, min(result), max(result), np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scalers = train_gen.get_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = DataGenerator(\n",
    "    path_data, num_particles, \"test\", subset, \n",
    "    output_cols, batch_size, scaler = train_scalers, \n",
    "    maxnum_particles = maxnum_particles, shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#@tf.function\n",
    "def R2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return SS_res/(SS_tot + K.epsilon())\n",
    "\n",
    "#@tf.function\n",
    "def keras_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "#@tf.function\n",
    "def wsme(y_true, y_pred):\n",
    "    #w = K.abs(K.mean(y_true[1]))\n",
    "    #w = w / (1 - w)\n",
    "    # w = K.sum(K.cast(K.greater(y_true[1], 0), \"float\")) # Number actually not zero\n",
    "    #error = K.square(y_true - y_pred)\n",
    "    #error = K.switch(K.equal(y_true, 0), w * error, error)\n",
    "    y_true = y_true * (y_true != 0) \n",
    "    y_pred = y_pred * (y_true != 0)\n",
    "    return keras_mse(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = maxnum_particles\n",
    "\n",
    "custom_losses = {\n",
    "    \"sce\": SymmetricCrossEntropy(0.5, 0.5),\n",
    "    \"weighted_mse\": wsme,\n",
    "    \"r2\": R2,\n",
    "    \"rmse\": rmse\n",
    "}\n",
    "\n",
    "class Conv2DNeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    A Conv2D Neural Network Model that can support an arbitrary numbers of\n",
    "    layers.\n",
    "\n",
    "    Attributes:\n",
    "        filters: List of number of filters in each Conv2D layer\n",
    "        kernel_sizes: List of kernel sizes in each Conv2D layer\n",
    "        conv2d_activation: Type of activation function for conv2d layers\n",
    "        pool_sizes: List of Max Pool sizes\n",
    "        dense_sizes: Sizes of dense layers\n",
    "        dense_activation: Type of activation function for dense layers\n",
    "        output_activation: Type of activation function for output layer\n",
    "        lr: Optimizer learning rate\n",
    "        optimizer: Name of optimizer or optimizer object.\n",
    "        adam_beta_1: Exponential decay rate for the first moment estimates\n",
    "        adam_beta_2: Exponential decay rate for the first moment estimates\n",
    "        sgd_momentum: Stochastic Gradient Descent momentum\n",
    "        decay: Optimizer decay\n",
    "        loss: Name of loss function or loss object\n",
    "        batch_size: Number of examples per batch\n",
    "        epochs: Number of epochs to train\n",
    "        verbose: Level of detail to provide during training\n",
    "        model: Keras Model object\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        filters=(8,), \n",
    "        kernel_sizes=(5,),\n",
    "        conv2d_activation=\"relu\", \n",
    "        pool_sizes=(4,), \n",
    "        pool_dropout=0.0,\n",
    "        dense_sizes=(64,),\n",
    "        dense_activation=\"relu\", \n",
    "        dense_dropout = 0.0,\n",
    "        output_activation=\"linear\",\n",
    "        lr=0.001, \n",
    "        optimizer=\"adam\", \n",
    "        adam_beta_1=0.9,\n",
    "        adam_beta_2=0.999, \n",
    "        sgd_momentum=0.9, \n",
    "        decay=0, \n",
    "        loss=\"mse\",\n",
    "        metrics = [], \n",
    "        batch_size=32, \n",
    "        epochs=2, \n",
    "        verbose=0\n",
    "    ):\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_sizes = [tuple((v,v)) for v in kernel_sizes]\n",
    "        self.conv2d_activation = conv2d_activation\n",
    "        self.pool_sizes = [tuple((v,v)) for v in pool_sizes]\n",
    "        self.pool_dropout = pool_dropout\n",
    "        self.dense_sizes = dense_sizes\n",
    "        self.dense_activation = dense_activation\n",
    "        self.dense_dropout = dense_dropout\n",
    "        self.output_activation = output_activation\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_obj = None\n",
    "        self.adam_beta_1 = adam_beta_1\n",
    "        self.adam_beta_2 = adam_beta_2\n",
    "        self.sgd_momentum = sgd_momentum\n",
    "        self.decay = decay\n",
    "        self.loss = custom_losses[loss] if loss in custom_losses else loss\n",
    "        self.metrics = []\n",
    "        for m in metrics:\n",
    "            if m in custom_losses:\n",
    "                self.metrics.append(custom_losses[m])\n",
    "            else:\n",
    "                self.metrics.append(m)\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        \n",
    "        if self.conv2d_activation == \"leakyrelu\":\n",
    "            self.conv2d_activation = LeakyReLU(alpha=0.1)\n",
    "        if self.dense_activation == \"leakyrelu\":\n",
    "            self.dense_activation = LeakyReLU(alpha=0.1)\n",
    "        if self.output_activation == \"leakyrelu\":\n",
    "            self.output_activation = LeakyReLU(alpha=0.1)\n",
    "\n",
    "    def build_neural_network(self, input_shape, n_particles, output_shape):\n",
    "        \"\"\"Create Keras neural network model and compile it.\"\"\"\n",
    "        \n",
    "        # function for creating a vgg block\n",
    "        def vgg_block(layer_in, n_filters, n_conv):\n",
    "            # add convolutional layers\n",
    "            for _ in range(n_conv):\n",
    "                layer_in = Conv2D(n_filters, (3, 3), \n",
    "                                  padding='same', activation='linear')(layer_in)\n",
    "                layer_in = Activation('relu')(layer_in)\n",
    "            \n",
    "            # Batch norm\n",
    "            layer_in = BatchNormalization(axis=-1)(layer_in)\n",
    "            # Dropout\n",
    "            #layer_in = Dropout(0.2)(layer_in)\n",
    "            # Max pooling \n",
    "            layer_in = MaxPool2D((2, 2), strides=(2, 2))(layer_in)\n",
    "            return layer_in\n",
    "        \n",
    "        # Input\n",
    "        conv_input = Input(shape=(input_shape), name=\"input\")\n",
    "        \n",
    "        nn_model = vgg_block(conv_input, 32, 2)\n",
    "        nn_model = vgg_block(nn_model, 64, 2)\n",
    "        nn_model = vgg_block(nn_model, 128, 4)\n",
    "        #nn_model = vgg_block(nn_model, 256, 4)        \n",
    "        nn_model = Flatten()(nn_model)\n",
    "        \n",
    "#         nn_model = BatchNormalization(axis=-1)(nn_model)\n",
    "#         nn_model = Dropout(0.5)(nn_model)\n",
    "        \n",
    "#         # Classifier\n",
    "#         for h in range(len(self.dense_sizes)):\n",
    "#             nn_model = Dense(self.dense_sizes[h],\n",
    "#                              activation=self.dense_activation,\n",
    "#                              kernel_initializer='he_uniform',\n",
    "#                              name=f\"dense_{h:02d}\")(nn_model)\n",
    "#             if self.dense_dropout > 0.0:\n",
    "#                 nn_model = Dropout(self.dense_dropout, \n",
    "#                                    name=f\"dense_dr_{h:02d}\")(nn_model)\n",
    "        \n",
    "        nn_model = RepeatVector(n_particles, name = \"repeat\")(nn_model)\n",
    "                \n",
    "        # Output 1 - particle prediction\n",
    "        binary_model = Dense(1, \n",
    "                             activation=\"sigmoid\", \n",
    "                             kernel_initializer='he_uniform',\n",
    "                             name=f\"binary\")(nn_model)\n",
    "        \n",
    "        # Output 2 - configuration prediction\n",
    "        nn_model = Dense(output_shape,\n",
    "                         activation=\"linear\",\n",
    "                         kernel_initializer='he_uniform',\n",
    "                         name=f\"coordinate\")(nn_model)\n",
    "        \n",
    "#         nn_model = Lambda(\n",
    "#             self.LastLayer,\n",
    "#             input_shape = (n_particles, output_shape),\n",
    "#             name=\"coordinate\"\n",
    "#         )(nn_model)\n",
    "        \n",
    "        self.model = Model(\n",
    "            inputs = conv_input, \n",
    "            outputs = [binary_model, nn_model]\n",
    "        )\n",
    "        \n",
    "        losses = {\n",
    "            \"binary\": \"binary_crossentropy\",\n",
    "            \"coordinate\": \"mse\" #custom_losses[\"weighted_mse\"]\n",
    "        }\n",
    "        lossWeights = {\"binary\": 1.0, \"coordinate\": 1.0}\n",
    "        \n",
    "        if self.optimizer == \"adam\":\n",
    "            self.optimizer_obj = Adam(lr=self.lr, clipnorm = 5.0)\n",
    "        elif self.optimizer == \"sgd\":\n",
    "            self.optimizer_obj = SGD(lr=self.lr, momentum=self.sgd_momentum,\n",
    "                                     decay=self.decay)\n",
    "            \n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer_obj, \n",
    "            loss=losses, #self.loss,\n",
    "            loss_weights=lossWeights,\n",
    "            metrics=self.metrics\n",
    "        )\n",
    "        #self.model.summary()\n",
    "\n",
    "    def fit(self, x, y, xv=None, yv=None, callbacks=None):\n",
    "        \n",
    "        if len(x.shape[1:])==2:\n",
    "            x = np.expand_dims(x, axis=-1)\n",
    "        if len(y.shape) == 1:\n",
    "            output_shape = 1\n",
    "        else:\n",
    "            output_shape = y.shape[1]\n",
    "        \n",
    "        input_shape = x.shape[1:]\n",
    "        self.build_neural_network(input_shape, output_shape)\n",
    "        self.model.fit(x, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose, validation_data=(xv, yv), callbacks=callbacks)\n",
    "        return self.model.history.history\n",
    "    \n",
    "    def LastLayer(self, x):\n",
    "        return 1.75 * K.tanh(x / 100) \n",
    "\n",
    "    def predict(self, x):\n",
    "        y_out = self.model.predict(np.expand_dims(x, axis=-1),\n",
    "                                   batch_size=self.batch_size)\n",
    "        return y_out\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        y_prob = self.model.predict(x, batch_size=self.batch_size)\n",
    "        return y_prob\n",
    "    \n",
    "    def load_weights(self, weights):\n",
    "        try:\n",
    "            self.model.load_weights(weights)\n",
    "            self.model.compile(\n",
    "                optimizer=self.optimizer, \n",
    "                loss=self.loss, \n",
    "                metrics=self.metrics\n",
    "            )\n",
    "        except:\n",
    "            print(\"You must first call build_neural_network before loading weights. Exiting.\")\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Conv2DNeuralNetwork(**config[\"conv2d_network\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.build_neural_network(input_shape, n_particles, output_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 600, 400, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 600, 400, 32) 1184        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 600, 400, 32) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 600, 400, 32) 9248        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 600, 400, 32) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 600, 400, 32) 128         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 300, 200, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 300, 200, 64) 18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 300, 200, 64) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 300, 200, 64) 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 300, 200, 64) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 300, 200, 64) 256         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 150, 100, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 960000)       0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "repeat (RepeatVector)           (None, 3, 960000)    0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary (Dense)                  (None, 3, 1)         960001      repeat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "coordinate (Dense)              (None, 3, 4)         3840004     repeat[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,866,245\n",
      "Trainable params: 4,866,053\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[128,32,600,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node activation/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Func/training/Adam/gradients/gradients/batch_normalization_1/cond_grad/StatelessIf/else/_57/input/_231/_345]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[128,32,600,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node activation/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8f9e965e8b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1121\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3566\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3567\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3568\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[128,32,600,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node activation/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Func/training/Adam/gradients/gradients/batch_normalization_1/cond_grad/StatelessIf/else/_57/input/_231/_345]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[128,32,600,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node activation/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "mod.model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    validation_data=valid_gen,\n",
    "    epochs=config[\"conv2d_network\"][\"epochs\"],\n",
    "    verbose=True,\n",
    "    callbacks=callbacks,\n",
    "    #steps_per_epoch=20,\n",
    "    use_multiprocessing=True,\n",
    "    workers=16,\n",
    "    max_queue_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
