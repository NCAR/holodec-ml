{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import yaml\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy.fft as FFT\n",
    "import tensorflow as tf\n",
    "from typing import List, Dict\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from tensorflow.keras.layers import (Input, Conv2D, Dense, Flatten, \n",
    "                                     MaxPool2D, RepeatVector, Lambda,\n",
    "                                     LeakyReLU, Dropout)\n",
    "from tensorflow.keras.models import Model, save_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from keras_radam import RAdam\n",
    "from keras_radam.training import RAdamOptimizer\n",
    "\n",
    "sys.path.append(\"/glade/work/schreck/repos/holodec-ml\")\n",
    "from library.losses import SymmetricCrossEntropy\n",
    "from library.callbacks import get_callbacks\n",
    "\n",
    "from multiprocessing import cpu_count, Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"notebook.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file) as config_file:\n",
    "    config = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(config[\"path_save\"])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up some globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = config[\"path_data\"]\n",
    "num_particles = config[\"num_particles\"]\n",
    "split = 'train'\n",
    "subset = False\n",
    "output_cols = [\"x\", \"y\", \"z\", \"d\", \"hid\"]\n",
    "\n",
    "batch_size = config[\"conv2d_network\"][\"batch_size\"]\n",
    "\n",
    "input_shape = (600, 400, 1)\n",
    "\n",
    "n_particles = config[\"num_particles\"]\n",
    "output_channels = len(output_cols) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpticsFFT(Ain):\n",
    "    \"\"\"\n",
    "    Apply 2D fft to input matrix\n",
    "    \"\"\"\n",
    "    Aout = np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(Ain)))/np.sqrt(np.size(Ain))\n",
    "    return Aout\n",
    "\n",
    "def OpticsIFFT(Ain):\n",
    "    \"\"\"\n",
    "    Apply 2D inverse fft to input matrix\n",
    "    \"\"\"\n",
    "    Aout = np.fft.ifftshift(np.fft.ifft2(np.fft.fftshift(Ain)))*np.sqrt(np.size(Ain))\n",
    "    return Aout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles_dict = {\n",
    "    1 : '1particle',\n",
    "    3 : '3particle',\n",
    "    'multi': 'multiparticle',\n",
    "    '50-100': '50-100'}\n",
    "\n",
    "split_dict = {\n",
    "    'train' : 'training',\n",
    "    'test'   : 'test',\n",
    "    'valid': 'validation'}\n",
    "\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(\n",
    "        self, \n",
    "        path_data: str, \n",
    "        num_particles: int, \n",
    "        split: str, \n",
    "        subset: bool, \n",
    "        output_cols: List[str], \n",
    "        batch_size: int, \n",
    "        shuffle: bool = True,\n",
    "        maxnum_particles: int = False,\n",
    "        scaler: Dict[str, str] = False) -> None:\n",
    "        \n",
    "        'Initialization'\n",
    "        self.ds = self.open_dataset(path_data, num_particles, split)\n",
    "        self.batch_size = batch_size\n",
    "        self.output_cols = [x for x in output_cols if x != 'hid']        \n",
    "        self.subset = subset\n",
    "        self.hologram_numbers = self.ds.hologram_number.values\n",
    "        if shuffle:\n",
    "            random.shuffle(self.hologram_numbers)\n",
    "        self.num_particles = num_particles\n",
    "        self.xsize = len(self.ds.xsize.values)\n",
    "        self.ysize = len(self.ds.ysize.values)\n",
    "        self.shuffle = shuffle\n",
    "        self.maxnum_particles = maxnum_particles\n",
    "                \n",
    "        if not scaler:\n",
    "            self.scaler = {col: StandardScaler() for col in output_cols}\n",
    "            for col in output_cols:\n",
    "                scale = self.ds[col].values\n",
    "                self.scaler[col].fit(scale.reshape(scale.shape[-1], -1))\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "            \n",
    "#         self.images = np.vstack(self.ds[\"image\"].values for col in output_cols).T\n",
    "#         self.columns = np.vstack(self.ds[col].values for col in output_cols).T\n",
    "#         self.df = zip(self.images, self.columns)\n",
    "        \n",
    "    def get_transform(self):\n",
    "        return self.scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return math.ceil(len(self.hologram_numbers) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        'Generate one batch of data'\n",
    "        holograms = self.hologram_numbers[\n",
    "            idx * self.batch_size: (idx + 1) * self.batch_size\n",
    "        ]\n",
    "        x_out, y_out = self._batch(holograms)\n",
    "        return x_out, y_out\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            random.shuffle(self.hologram_numbers)\n",
    "            \n",
    "    def _batch(self, holograms):\n",
    "        'Create a batch of data'\n",
    "        try:\n",
    "        \n",
    "            x_out = np.zeros((\n",
    "                len(holograms), self.xsize, self.ysize\n",
    "            ))\n",
    "            y_out = np.zeros((\n",
    "                len(holograms), \n",
    "                self.maxnum_particles if self.maxnum_particles else self.num_particles, \n",
    "                len(self.output_cols)\n",
    "            ))\n",
    "            a = time.time()\n",
    "            for k, hologram in enumerate(holograms):\n",
    "                im = self.ds[\"image\"][hologram].values\n",
    "                x_out[k] = (im-np.mean(im)) / (np.std(im))\n",
    "                #A = np.log(np.abs(OpticsFFT(A)))                    \n",
    "                particles = np.where(self.ds[\"hid\"] == hologram + 1)[0]  \n",
    "                for l, p in enumerate(particles):\n",
    "                    for m, col in enumerate(self.output_cols):\n",
    "                        val = self.ds[col][p].values\n",
    "                        y_out[k, l, m] = self.scaler[col].transform(\n",
    "                            val.reshape(1, -1)\n",
    "                        )\n",
    "                if self.maxnum_particles and len(particles) < self.maxnum_particles:\n",
    "                    for l in range(len(particles), self.maxnum_particles):\n",
    "                        for m, col in enumerate(self.output_cols):\n",
    "                            val = y_out[k, l, m]\n",
    "                            y_out[k, l, m] = self.scaler[col].transform(\n",
    "                                val.reshape(1, -1)\n",
    "                            )\n",
    "            #\n",
    "            # convert y_out to sparse if we are using padding\n",
    "#             if self.maxnum_particles:\n",
    "#                 y_out = sparse_vstack([\n",
    "#                     csr_matrix(y_out[i]) for i in y_out.shape[0]\n",
    "#                 ])\n",
    "            \n",
    "            x_out = np.expand_dims(x_out, axis=-1)\n",
    "            return x_out, y_out\n",
    "        \n",
    "        except:\n",
    "            print(traceback.print_exc())\n",
    "    \n",
    "    def open_dataset(self, path_data, num_particles, split):\n",
    "        \"\"\"\n",
    "        Opens a HOLODEC file\n",
    "\n",
    "        Args: \n",
    "            path_data: (str) Path to dataset directory\n",
    "            num_particles: (int or str) Number of particles per hologram\n",
    "            split: (str) Dataset split of either 'train', 'valid', or 'test'\n",
    "\n",
    "        Returns:\n",
    "            ds: (xarray Dataset) Opened dataset\n",
    "        \"\"\"\n",
    "        path_data = os.path.join(path_data, self.dataset_name(num_particles, split))\n",
    "\n",
    "        if not os.path.isfile(path_data):\n",
    "            print(f\"Data file does not exist at {path_data}. Exiting.\")\n",
    "            raise \n",
    "\n",
    "        ds = xr.open_dataset(path_data)\n",
    "        return ds\n",
    "    \n",
    "    def dataset_name(self, num_particles, split, file_extension='nc'):\n",
    "        \"\"\"\n",
    "        Return the dataset filename given user inputs\n",
    "\n",
    "        Args: \n",
    "            num_particles: (int or str) Number of particles per hologram\n",
    "            split: (str) Dataset split of either 'train', 'valid', or 'test'\n",
    "            file_extension: (str) Dataset file extension\n",
    "\n",
    "        Returns:\n",
    "            ds_name: (str) Dataset name\n",
    "        \"\"\"\n",
    "\n",
    "        valid = [1,3,'multi','50-100']\n",
    "        if num_particles not in valid:\n",
    "            raise ValueError(\"results: num_particles must be one of %r.\" % valid)\n",
    "        num_particles = num_particles_dict[num_particles]\n",
    "\n",
    "        valid = ['train','test','valid']\n",
    "        if split not in valid:\n",
    "            raise ValueError(\"results: split must be one of %r.\" % valid)\n",
    "        split = split_dict[split]\n",
    "        ds_name = f'synthetic_holograms_{num_particles}_{split}.{file_extension}'\n",
    "\n",
    "        return ds_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(\n",
    "    path_data, num_particles, \"train\", subset, \n",
    "    output_cols, batch_size, maxnum_particles = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = {col: StandardScaler() for col in train_gen.output_cols}\n",
    "# for col in train_gen.output_cols:\n",
    "#     scale = train_gen.ds[col].values\n",
    "#     scaler[col].fit(scale.reshape(scale.shape[-1], -1))\n",
    "#     result = scaler[col].transform(scale.reshape(scale.shape[-1], -1))\n",
    "#     print(col, min(result), max(result), np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scalers = train_gen.get_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = DataGenerator(\n",
    "    path_data, num_particles, \"test\", subset, \n",
    "    output_cols, batch_size, scaler = train_scalers, maxnum_particles = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def R2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return SS_res/(SS_tot + K.epsilon())\n",
    "\n",
    "def keras_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wsme(y_true, y_pred):\n",
    "#     cond1 = K.equal(y_true, 0.0)\n",
    "#     zero = K.switch(cond1,K.square(y_pred - y_true), \n",
    "#                              K.zeros_like(y_true))\n",
    "#     cond2 = K.greater(y_true, 0.0)\n",
    "#     real = K.switch(cond2, \n",
    "#                              K.square(y_pred - y_true), \n",
    "#                              K.zeros_like(y_true))\n",
    "#     w1 = K.sum(K.cast(cond1, \"float\"))\n",
    "#     w2 = K.sum(K.cast(cond2, \"float\"))\n",
    "#     total = w1 + w2\n",
    "#     zero = K.sum(zero) / w1\n",
    "#     real = K.sum(real) / w2\n",
    "#     return (real + zero)\n",
    "\n",
    "def wsme(y_true, y_pred):\n",
    "    \n",
    "    w = K.abs(K.mean(y_true[1]))\n",
    "    w = w / (1 - w)\n",
    "    \n",
    "    # w = K.sum(K.cast(K.greater(y_true[1], 0), \"float\")) # Number actually not zero\n",
    "    \n",
    "    error = K.square(y_true - y_pred)\n",
    "    error = K.switch(K.equal(y_true, 0), w * error, error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 3\n",
    "\n",
    "custom_losses = {\n",
    "    \"sce\": SymmetricCrossEntropy(1.0, 1.0),\n",
    "    \"weighted_mse\": wsme,\n",
    "    \"r2\": R2,\n",
    "    \"rmse\": rmse\n",
    "}\n",
    "\n",
    "class Conv2DNeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    A Conv2D Neural Network Model that can support an arbitrary numbers of\n",
    "    layers.\n",
    "\n",
    "    Attributes:\n",
    "        filters: List of number of filters in each Conv2D layer\n",
    "        kernel_sizes: List of kernel sizes in each Conv2D layer\n",
    "        conv2d_activation: Type of activation function for conv2d layers\n",
    "        pool_sizes: List of Max Pool sizes\n",
    "        dense_sizes: Sizes of dense layers\n",
    "        dense_activation: Type of activation function for dense layers\n",
    "        output_activation: Type of activation function for output layer\n",
    "        lr: Optimizer learning rate\n",
    "        optimizer: Name of optimizer or optimizer object.\n",
    "        adam_beta_1: Exponential decay rate for the first moment estimates\n",
    "        adam_beta_2: Exponential decay rate for the first moment estimates\n",
    "        sgd_momentum: Stochastic Gradient Descent momentum\n",
    "        decay: Optimizer decay\n",
    "        loss: Name of loss function or loss object\n",
    "        batch_size: Number of examples per batch\n",
    "        epochs: Number of epochs to train\n",
    "        verbose: Level of detail to provide during training\n",
    "        model: Keras Model object\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        filters=(8,), \n",
    "        kernel_sizes=(5,),\n",
    "        conv2d_activation=\"relu\", \n",
    "        pool_sizes=(4,), \n",
    "        pool_dropout=0.0,\n",
    "        dense_sizes=(64,),\n",
    "        dense_activation=\"relu\", \n",
    "        dense_dropout = 0.0,\n",
    "        output_activation=\"linear\",\n",
    "        lr=0.001, \n",
    "        optimizer=\"adam\", \n",
    "        adam_beta_1=0.9,\n",
    "        adam_beta_2=0.999, \n",
    "        sgd_momentum=0.9, \n",
    "        decay=0, \n",
    "        loss=\"mse\",\n",
    "        metrics = [], \n",
    "        batch_size=32, \n",
    "        epochs=2, \n",
    "        verbose=0\n",
    "    ):\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_sizes = [tuple((v,v)) for v in kernel_sizes]\n",
    "        self.conv2d_activation = conv2d_activation\n",
    "        self.pool_sizes = [tuple((v,v)) for v in pool_sizes]\n",
    "        self.pool_dropout = pool_dropout\n",
    "        self.dense_sizes = dense_sizes\n",
    "        self.dense_activation = dense_activation\n",
    "        self.dense_dropout = dense_dropout\n",
    "        self.output_activation = output_activation\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_obj = None\n",
    "        self.adam_beta_1 = adam_beta_1\n",
    "        self.adam_beta_2 = adam_beta_2\n",
    "        self.sgd_momentum = sgd_momentum\n",
    "        self.decay = decay\n",
    "        self.loss = custom_losses[loss] if loss in custom_losses else loss\n",
    "        self.metrics = []\n",
    "        for m in metrics:\n",
    "            if m in custom_losses:\n",
    "                self.metrics.append(custom_losses[m])\n",
    "            else:\n",
    "                self.metrics.append(m)\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        \n",
    "        if self.conv2d_activation == \"leakyrelu\":\n",
    "            self.conv2d_activation = LeakyReLU(alpha=0.1)\n",
    "        if self.dense_activation == \"leakyrelu\":\n",
    "            self.dense_activation = LeakyReLU(alpha=0.1)\n",
    "        if self.output_activation == \"leakyrelu\":\n",
    "            self.output_activation = LeakyReLU(alpha=0.1)\n",
    "\n",
    "    def build_neural_network(self, input_shape, n_particles, output_shape):\n",
    "        \"\"\"Create Keras neural network model and compile it.\"\"\"\n",
    "        \n",
    "        # Input\n",
    "        conv_input = Input(shape=(input_shape), name=\"input\")\n",
    "        \n",
    "        # ConvNet encoder\n",
    "        nn_model = conv_input\n",
    "        for h in range(len(self.filters)):\n",
    "            nn_model = Conv2D(self.filters[h],\n",
    "                              self.kernel_sizes[h],\n",
    "                              padding=\"same\",\n",
    "                              activation=self.conv2d_activation,\n",
    "                              kernel_initializer='he_uniform',\n",
    "                              name=f\"conv2D_{h:02d}\")(nn_model)\n",
    "            nn_model = MaxPool2D(self.pool_sizes[h],\n",
    "                                 name=f\"maxpool2D_{h:02d}\")(nn_model)\n",
    "            nn_model = Dropout(self.pool_dropout, \n",
    "                               name = f\"maxpool2D_dr_{h:02d}\")(nn_model)\n",
    "        nn_model = Flatten()(nn_model)\n",
    "        \n",
    "        # Classifier\n",
    "        for h in range(len(self.dense_sizes)):\n",
    "            nn_model = Dense(self.dense_sizes[h],\n",
    "                             activation=self.dense_activation,\n",
    "                             kernel_initializer='he_uniform',\n",
    "                             name=f\"dense_{h:02d}\")(nn_model)\n",
    "            nn_model = Dropout(self.dense_dropout, \n",
    "                               name=f\"dense_dr_{h:02d}\")(nn_model)\n",
    "        \n",
    "        # Output\n",
    "        nn_model = RepeatVector(n_particles, name = \"repeat\")(nn_model)\n",
    "        nn_model = Dense(output_shape,\n",
    "                         activation=self.output_activation,\n",
    "                         name=f\"dense_output\")(nn_model)\n",
    "        nn_model = Lambda(\n",
    "            self.LastLayer,\n",
    "            input_shape = (n_particles, output_shape)\n",
    "        )(nn_model)\n",
    "        \n",
    "        self.model = Model(conv_input, nn_model)\n",
    "        \n",
    "        if self.optimizer == \"adam\":\n",
    "            self.optimizer_obj = Adam(lr=self.lr, clipnorm = 1.0)\n",
    "        elif self.optimizer == \"sgd\":\n",
    "            self.optimizer_obj = SGD(lr=self.lr, momentum=self.sgd_momentum,\n",
    "                                     decay=self.decay)\n",
    "        elif self.optimizer == \"radam\":\n",
    "            self.optimizer_obj = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=1e-7)\n",
    "            \n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer_obj, \n",
    "            loss=self.loss,\n",
    "            metrics=self.metrics\n",
    "        )\n",
    "        self.model.summary()\n",
    "\n",
    "    def fit(self, x, y, xv=None, yv=None, callbacks=None):\n",
    "        \n",
    "        if len(x.shape[1:])==2:\n",
    "            x = np.expand_dims(x, axis=-1)\n",
    "        if len(y.shape) == 1:\n",
    "            output_shape = 1\n",
    "        else:\n",
    "            output_shape = y.shape[1]\n",
    "        \n",
    "        input_shape = x.shape[1:]\n",
    "        self.build_neural_network(input_shape, output_shape)\n",
    "        self.model.fit(x, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose, validation_data=(xv, yv), callbacks=callbacks)\n",
    "        return self.model.history.history\n",
    "    \n",
    "    def LastLayer(self, x):\n",
    "        return 1.75 * K.tanh(x / 100) \n",
    "\n",
    "    def predict(self, x):\n",
    "        y_out = self.model.predict(np.expand_dims(x, axis=-1),\n",
    "                                   batch_size=self.batch_size)\n",
    "        return y_out\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        y_prob = self.model.predict(x, batch_size=self.batch_size)\n",
    "        return y_prob\n",
    "    \n",
    "    def load_weights(self, weights):\n",
    "        try:\n",
    "            self.model.load_weights(weights)\n",
    "            self.model.compile(\n",
    "                optimizer=self.optimizer, \n",
    "                loss=self.loss, \n",
    "                metrics=self.metrics\n",
    "            )\n",
    "        except:\n",
    "            print(\"You must first call build_neural_network before loading weights. Exiting.\")\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Conv2DNeuralNetwork(**config[\"conv2d_network\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 600, 400, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2D_00 (Conv2D)           (None, 600, 400, 8)       208       \n",
      "_________________________________________________________________\n",
      "maxpool2D_00 (MaxPooling2D)  (None, 120, 80, 8)        0         \n",
      "_________________________________________________________________\n",
      "maxpool2D_dr_00 (Dropout)    (None, 120, 80, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2D_01 (Conv2D)           (None, 120, 80, 12)       2412      \n",
      "_________________________________________________________________\n",
      "maxpool2D_01 (MaxPooling2D)  (None, 24, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "maxpool2D_dr_01 (Dropout)    (None, 24, 16, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2D_02 (Conv2D)           (None, 24, 16, 16)        4816      \n",
      "_________________________________________________________________\n",
      "maxpool2D_02 (MaxPooling2D)  (None, 4, 3, 16)          0         \n",
      "_________________________________________________________________\n",
      "maxpool2D_dr_02 (Dropout)    (None, 4, 3, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_00 (Dense)             (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_dr_00 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_01 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_dr_01 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "repeat (RepeatVector)        (None, 3, 50)             0         \n",
      "_________________________________________________________________\n",
      "dense_output (Dense)         (None, 3, 4)              204       \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 3, 4)              0         \n",
      "=================================================================\n",
      "Total params: 31,990\n",
      "Trainable params: 31,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.build_neural_network(input_shape, n_particles, output_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.6871 - mae: 1.0297 - R2: 1.0239\n",
      "391/391 [==============================] - 339s 866ms/step - loss: 1.6867 - mae: 1.0296 - R2: 1.0235 - val_loss: 1.7235 - val_mae: 0.9881 - val_R2: 1.0454\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 316s 809ms/step - loss: 1.4390 - mae: 0.9613 - R2: 0.8735 - val_loss: 1.4666 - val_mae: 0.9290 - val_R2: 0.8898\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 314s 804ms/step - loss: 1.3447 - mae: 0.9259 - R2: 0.8161 - val_loss: 1.3171 - val_mae: 0.8924 - val_R2: 0.7992\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 1.3029 - mae: 0.9070 - R2: 0.7907 - val_loss: 1.2763 - val_mae: 0.8823 - val_R2: 0.7745\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 317s 812ms/step - loss: 1.2749 - mae: 0.8943 - R2: 0.7737 - val_loss: 1.2574 - val_mae: 0.8775 - val_R2: 0.7630\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 320s 819ms/step - loss: 1.2527 - mae: 0.8842 - R2: 0.7604 - val_loss: 1.2469 - val_mae: 0.8758 - val_R2: 0.7567\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 329s 840ms/step - loss: 1.2326 - mae: 0.8761 - R2: 0.7481 - val_loss: 1.2433 - val_mae: 0.8758 - val_R2: 0.7546\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 324s 829ms/step - loss: 1.2161 - mae: 0.8702 - R2: 0.7383 - val_loss: 1.2564 - val_mae: 0.8814 - val_R2: 0.7626\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 326s 833ms/step - loss: 1.2033 - mae: 0.8655 - R2: 0.7303 - val_loss: 1.2846 - val_mae: 0.8898 - val_R2: 0.7797\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 300s 768ms/step - loss: 1.1955 - mae: 0.8630 - R2: 0.7257 - val_loss: 1.2808 - val_mae: 0.8884 - val_R2: 0.7774\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 303s 776ms/step - loss: 1.1933 - mae: 0.8623 - R2: 0.7243 - val_loss: 1.2827 - val_mae: 0.8888 - val_R2: 0.7786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2acb5ee4f0d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    validation_data=valid_gen,\n",
    "    epochs=config[\"conv2d_network\"][\"epochs\"],\n",
    "    verbose=True,\n",
    "    callbacks=callbacks,\n",
    "    use_multiprocessing=True,\n",
    "    workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Pool(12) as p:\n",
    "#     x_train, y_train = [], []\n",
    "#     _total = train_gen.__len__()\n",
    "#     for (x, y) in tqdm(p.imap(train_gen.__getitem__, range(_total)), total = _total):\n",
    "#         x_train.append(x)\n",
    "#         y_train.append(y)\n",
    "# x_train = np.vstack(x_train)\n",
    "# y_train = np.vstack(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Pool(12) as p:\n",
    "#     x_valid, y_valid = [], []\n",
    "#     _total = valid_gen.__len__()\n",
    "#     for (x, y) in tqdm(p.imap(valid_gen.__getitem__, range(_total)), total = _total):\n",
    "#         x_valid.append(x)\n",
    "#         y_valid.append(y)\n",
    "# x_valid = np.vstack(x_valid)\n",
    "# y_valid = np.vstack(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod.model.fit(\n",
    "#     x_train, y_train,\n",
    "#     validation_data=(x_valid, y_valid),\n",
    "#     epochs=config[\"conv2d_network\"][\"epochs\"],\n",
    "#     verbose=True,\n",
    "#     callbacks=callbacks,\n",
    "#     shuffle=True,\n",
    "#     batch_size=batch_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-583b9eb3207f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "def fitness(learning_rate, num_dense_layers, \n",
    "            num_input_nodes, num_dense_nodes, \n",
    "            activation, batch_size, adam_decay):\n",
    "    \n",
    "    # Reload generators\n",
    "    \n",
    "    # Update the configuration\n",
    "    \n",
    "    # Load the model\n",
    "    model = \n",
    "    \n",
    "    # train\n",
    "    blackbox = model.model.fit_generator(\n",
    "        generator=train_gen,\n",
    "        validation_data=valid_gen,\n",
    "        epochs=config[\"conv2d_network\"][\"epochs\"],\n",
    "        verbose=True,\n",
    "        callbacks=callbacks,\n",
    "        use_multiprocessing=True,\n",
    "        workers=16\n",
    "    )\n",
    "    \n",
    "    #return the validation accuracy for the last epoch.\n",
    "    objective = blackbox.history['val_loss'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # the optimizer aims for the lowest score, so we return our negative accuracy\n",
    "    return objective"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
