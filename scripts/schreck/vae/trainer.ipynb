{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from typing import List, Dict, Callable, Union, Any, TypeVar, Tuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# custom\n",
    "from holodecml.vae.checkpointer import *\n",
    "from holodecml.vae.data_loader import *\n",
    "from holodecml.vae.optimizers import *\n",
    "from holodecml.vae.transforms import *\n",
    "from holodecml.vae.models import *\n",
    "from holodecml.vae.visual import *\n",
    "from holodecml.vae.losses import *\n",
    "\n",
    "from torch import nn\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = criterion(recon_x, x)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 optimizer,\n",
    "                 train_gen, \n",
    "                 valid_gen, \n",
    "                 dataloader, \n",
    "                 valid_dataloader,\n",
    "                 batch_size,\n",
    "                 path_save,\n",
    "                 device,\n",
    "                 test_image = None):\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_gen = train_gen\n",
    "        self.valid_gen = valid_gen\n",
    "        self.dataloader = dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.batch_size = batch_size\n",
    "        self.path_save = path_save\n",
    "        self.device = device\n",
    "        self.test_image = test_image\n",
    "        \n",
    "        \n",
    "    def train_one_epoch(self, epoch):\n",
    "\n",
    "        self.model.train()\n",
    "        batches_per_epoch = int(np.ceil(self.train_gen.__len__() / self.batch_size))\n",
    "        batch_group_generator = tqdm.tqdm(\n",
    "            enumerate(self.dataloader),\n",
    "            total=batches_per_epoch, \n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        epoch_losses = {\"loss\": [], \"bce\": [], \"kld\": []}\n",
    "        for idx, images in batch_group_generator:\n",
    "\n",
    "            images = images.to(self.device)\n",
    "            recon_images, mu, logvar = self.model(images)\n",
    "            loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            batch_loss = loss.item() / batch_size\n",
    "            bce_loss = bce.item() / batch_size\n",
    "            kld_loss = kld.item() / batch_size\n",
    "\n",
    "            epoch_losses[\"loss\"].append(batch_loss)\n",
    "            epoch_losses[\"bce\"].append(bce_loss)\n",
    "            epoch_losses[\"kld\"].append(kld_loss)\n",
    "\n",
    "            loss = np.mean(epoch_losses[\"loss\"])\n",
    "            bce = np.mean(epoch_losses[\"bce\"])\n",
    "            kld = np.mean(epoch_losses[\"kld\"])\n",
    "\n",
    "            to_print = \"loss: {:.3f} bce: {:.3f} kld: {:.3f}\".format(loss, bce, kld)\n",
    "            batch_group_generator.set_description(to_print)\n",
    "            batch_group_generator.update()\n",
    "\n",
    "        return loss, bce, kld\n",
    "\n",
    "\n",
    "    def test(self, epoch):\n",
    "\n",
    "        self.model.eval()\n",
    "        batches_per_epoch = int(np.ceil(self.valid_gen.__len__() / self.batch_size))\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch_group_generator = tqdm.tqdm(\n",
    "                enumerate(self.valid_dataloader),\n",
    "                total=batches_per_epoch, \n",
    "                leave=True\n",
    "            )\n",
    "\n",
    "            epoch_losses = {\"loss\": [], \"bce\": [], \"kld\": []}\n",
    "            for idx, images in batch_group_generator:\n",
    "\n",
    "                images = images.to(self.device)\n",
    "                recon_images, mu, logvar = self.model(images)\n",
    "                loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "\n",
    "                batch_loss = loss.item() / batch_size\n",
    "                bce_loss = bce.item() / batch_size\n",
    "                kld_loss = kld.item() / batch_size\n",
    "\n",
    "                epoch_losses[\"loss\"].append(batch_loss)\n",
    "                epoch_losses[\"bce\"].append(bce_loss)\n",
    "                epoch_losses[\"kld\"].append(kld_loss)\n",
    "\n",
    "                loss = np.mean(epoch_losses[\"loss\"])\n",
    "                bce = np.mean(epoch_losses[\"bce\"])\n",
    "                kld = np.mean(epoch_losses[\"kld\"])\n",
    "\n",
    "                to_print = \"val_loss: {:.3f} val_bce: {:.3f} val_kld: {:.3f}\".format(loss, bce, kld)\n",
    "                batch_group_generator.set_description(to_print)\n",
    "                batch_group_generator.update()\n",
    "\n",
    "            if os.path.isfile(self.test_image):\n",
    "                with open(self.test_image, \"rb\") as fid:\n",
    "                    pic = pickle.load(fid)\n",
    "                self.compare(epoch, pic)\n",
    "\n",
    "        return loss, bce, kld\n",
    "    \n",
    "    \n",
    "    def compare(self, epoch, x):\n",
    "        x = x.to(self.device)\n",
    "        recon_x, _, _ = self.model(x)\n",
    "        compare_x = torch.cat([x, recon_x])\n",
    "        save_image(compare_x.data.cpu(), f'{self.path_save}/image_epoch_{epoch}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 optimizer,\n",
    "                 train_gen, \n",
    "                 valid_gen, \n",
    "                 dataloader, \n",
    "                 valid_dataloader,\n",
    "                 batch_size,\n",
    "                 path_save,\n",
    "                 device,\n",
    "                 test_image = None):\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_gen = train_gen\n",
    "        self.valid_gen = valid_gen\n",
    "        self.dataloader = dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.batch_size = batch_size\n",
    "        self.path_save = path_save\n",
    "        self.device = device\n",
    "        self.test_image = test_image\n",
    "        \n",
    "        \n",
    "    def train_one_epoch(self, epoch):\n",
    "\n",
    "        self.model.train()\n",
    "        batches_per_epoch = int(np.ceil(self.train_gen.__len__() / self.batch_size))\n",
    "        batch_group_generator = tqdm.tqdm(\n",
    "            enumerate(self.dataloader),\n",
    "            total=batches_per_epoch, \n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        epoch_losses = {\"loss\": [], \"bce\": [], \"kld\": []}\n",
    "        for idx, images in batch_group_generator:\n",
    "\n",
    "            images = images.to(self.device)\n",
    "            recon_images, mu, logvar = self.model(images)\n",
    "            \n",
    "            loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            batch_loss = loss.item() / batch_size\n",
    "            bce_loss = bce.item() / batch_size\n",
    "            kld_loss = kld.item() / batch_size\n",
    "\n",
    "            epoch_losses[\"loss\"].append(batch_loss)\n",
    "            epoch_losses[\"bce\"].append(bce_loss)\n",
    "            epoch_losses[\"kld\"].append(kld_loss)\n",
    "\n",
    "            loss = np.mean(epoch_losses[\"loss\"])\n",
    "            bce = np.mean(epoch_losses[\"bce\"])\n",
    "            kld = np.mean(epoch_losses[\"kld\"])\n",
    "\n",
    "            to_print = \"loss: {:.3f} bce: {:.3f} kld: {:.3f}\".format(loss, bce, kld)\n",
    "            batch_group_generator.set_description(to_print)\n",
    "            batch_group_generator.update()\n",
    "\n",
    "        return loss, bce, kld\n",
    "\n",
    "\n",
    "    def test(self, epoch):\n",
    "\n",
    "        self.model.eval()\n",
    "        batches_per_epoch = int(np.ceil(self.valid_gen.__len__() / self.batch_size))\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch_group_generator = tqdm.tqdm(\n",
    "                enumerate(self.valid_dataloader),\n",
    "                total=batches_per_epoch, \n",
    "                leave=True\n",
    "            )\n",
    "\n",
    "            epoch_losses = {\"loss\": [], \"bce\": [], \"kld\": []}\n",
    "            for idx, images in batch_group_generator:\n",
    "\n",
    "                images = images.to(self.device)\n",
    "                recon_images, mu, logvar = self.model(images)\n",
    "                loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "\n",
    "                batch_loss = loss.item() / batch_size\n",
    "                bce_loss = bce.item() / batch_size\n",
    "                kld_loss = kld.item() / batch_size\n",
    "\n",
    "                epoch_losses[\"loss\"].append(batch_loss)\n",
    "                epoch_losses[\"bce\"].append(bce_loss)\n",
    "                epoch_losses[\"kld\"].append(kld_loss)\n",
    "\n",
    "                loss = np.mean(epoch_losses[\"loss\"])\n",
    "                bce = np.mean(epoch_losses[\"bce\"])\n",
    "                kld = np.mean(epoch_losses[\"kld\"])\n",
    "\n",
    "                to_print = \"val_loss: {:.3f} val_bce: {:.3f} val_kld: {:.3f}\".format(loss, bce, kld)\n",
    "                batch_group_generator.set_description(to_print)\n",
    "                batch_group_generator.update()\n",
    "\n",
    "            if os.path.isfile(self.test_image):\n",
    "                with open(self.test_image, \"rb\") as fid:\n",
    "                    pic = pickle.load(fid)\n",
    "                self.compare(epoch, pic)\n",
    "\n",
    "        return loss, bce, kld\n",
    "    \n",
    "    \n",
    "    def compare(self, epoch, x):\n",
    "        x = x.to(self.device)\n",
    "        recon_x, _, _ = self.model(x)\n",
    "        compare_x = torch.cat([x, recon_x])\n",
    "        save_image(compare_x.data.cpu(), f'{self.path_save}/image_epoch_{epoch}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\") as config_file:\n",
    "    config = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(config[\"path_save\"])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "# Stream output to stdout\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "root.addHandler(ch)\n",
    "\n",
    "# Save the log file\n",
    "logger_name = os.path.join(config[\"path_save\"], \"log.txt\")\n",
    "fh = logging.FileHandler(logger_name,\n",
    "                         mode=\"w\",\n",
    "                         encoding='utf-8')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "root.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading parameters from config.yml\n"
     ]
    }
   ],
   "source": [
    "logging.info(f'Reading parameters from config.yml')\n",
    "    \n",
    "path_data = config[\"path_data\"]\n",
    "path_save = config[\"path_save\"]\n",
    "num_particles = config[\"num_particles\"]\n",
    "maxnum_particles = config[\"maxnum_particles\"]\n",
    "output_cols = config[\"output_cols\"]\n",
    "subset = config[\"subset\"]\n",
    "test_image = config[\"test_image\"]\n",
    "\n",
    "batch_size = config[\"batch_size\"]\n",
    "workers = min(config[\"workers\"], cpu_count())\n",
    "epochs = config[\"epochs\"]\n",
    "retrain = False if \"retrain\" not in config else config[\"retrain\"]\n",
    "\n",
    "model_save_path = os.path.join(f\"{path_save}\", \"checkpoint.pt\")\n",
    "\n",
    "start_epoch = 0\n",
    "if retrain:\n",
    "    saved_model_optimizer = torch.load(model_save_path)\n",
    "    start_epoch = saved_model_optimizer[\"epoch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Preparing to use device cuda:0\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")\n",
    "    \n",
    "logging.info(f'Preparing to use device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.vae.transforms:Loaded Rescale transformation with output size 384\n",
      "INFO:holodecml.vae.transforms:Loaded Normalize transformation that normalizes data in the range 0 to 1\n",
      "INFO:holodecml.vae.transforms:Loaded ToTensor transformation, putting tensors on device cuda:0\n"
     ]
    }
   ],
   "source": [
    "tforms = []\n",
    "transform_config = config[\"transforms\"]\n",
    "\n",
    "if \"Rescale\" in transform_config:\n",
    "    rescale = transform_config[\"Rescale\"]\n",
    "    tforms.append(Rescale(rescale))\n",
    "if \"Normalize\" in transform_config:\n",
    "    tforms.append(Normalize())\n",
    "if \"ToTensor\" in transform_config:\n",
    "    tforms.append(ToTensor(device))\n",
    "if \"RandomCrop\" in transform_config:\n",
    "    tforms.append(RandomCrop())\n",
    "if \"Standardize\" in transform_config:\n",
    "    tforms.append(Standardize())\n",
    "\n",
    "transform = transforms.Compose(tforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.vae.data_loader:Loaded train hologram data containing 50000 images\n",
      "INFO:holodecml.vae.data_loader:Loaded test hologram data containing 10000 images\n"
     ]
    }
   ],
   "source": [
    "train_gen = HologramDataset(\n",
    "    path_data, num_particles, \"train\", subset, \n",
    "    output_cols, maxnum_particles = maxnum_particles, \n",
    "    transform = transform\n",
    ")\n",
    "\n",
    "train_scalers = train_gen.get_transform()\n",
    "\n",
    "valid_gen = HologramDataset(\n",
    "    path_data, num_particles, \"test\", subset, \n",
    "    output_cols, scaler = train_scalers, \n",
    "    maxnum_particles = maxnum_particles,\n",
    "    transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training data iterator using 24 workers\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Loading training data iterator using {workers} workers\")\n",
    "    \n",
    "dataloader = DataLoader(\n",
    "    train_gen,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = workers\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_gen,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers = workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4\n",
    "stride = 1\n",
    "padding = 0\n",
    "init_kernel = 16 # initial number of filters\n",
    "\n",
    "# Based off of https://debuggercafe.com/face-image-generation-using-convolutional-variational-autoencoder-and-pytorch/\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc5 = nn.Conv2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        \n",
    "        # decoder \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=1, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        x = self.enc5(x)\n",
    "        # get `mu` and `log_var`\n",
    "        mu = x\n",
    "        log_var = x\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        reconstruction = torch.sigmoid(self.dec5(x))\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen.__getitem__(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The model contains 410609 parameters\n"
     ]
    }
   ],
   "source": [
    "vae = ConvVAE().to(device)\n",
    "    \n",
    "# Print the total number of model parameters\n",
    "logging.info(\n",
    "    f\"The model contains {count_parameters(vae)} parameters\"\n",
    ")\n",
    "\n",
    "if retrain:\n",
    "    vae = vae.load_state_dict(\n",
    "        saved_model_optimizer[\"model_state_dict\"], map_location=device\n",
    "    )\n",
    "    logging.info(f\"Loaded model weights from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded the lookahead-diffgrad optimizer with learning rate 0.001\n"
     ]
    }
   ],
   "source": [
    "optimizer_config = config[\"optimizer\"]\n",
    "learning_rate = optimizer_config[\"lr\"] if not retrain else saved_model_optimizer[\"lr\"]\n",
    "optimizer_type = optimizer_config[\"type\"]\n",
    "\n",
    "if optimizer_type == \"lookahead-diffgrad\":\n",
    "    optimizer = LookaheadDiffGrad(vae.parameters(), lr=learning_rate)\n",
    "elif optimizer_type == \"diffgrad\":\n",
    "    optimizer = DiffGrad(vae.parameters(), lr=learning_rate)\n",
    "elif optimizer_type == \"lookahead-radam\":\n",
    "    optimizer = LookaheadRAdam(vae.parameters(), lr=learning_rate)\n",
    "elif optimizer_type == \"radam\":\n",
    "    optimizer = RAdam(vae.parameters(), lr=learning_rate)\n",
    "elif optimizer_type == \"adam\":\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "elif optimizer_type == \"sgd\":\n",
    "    optimizer = torch.optim.SGD(vae.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    logging.warning(\n",
    "        f\"Optimzer type {optimizer_type} is unknown. Exiting with error.\"\n",
    "    )\n",
    "    sys.exit(1)\n",
    "\n",
    "logging.info(\n",
    "    f\"Loaded the {optimizer_type} optimizer with learning rate {learning_rate}\"\n",
    ")\n",
    "\n",
    "if retrain:\n",
    "    optimizer = optimizer.load_state_dict(\n",
    "        saved_model_optimizer[\"optimizer_state_dict\"], map_location=device\n",
    "    )\n",
    "    logging.info(f\"Loaded optimizer weights from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded ReduceLROnPlateau learning rate annealer with patience 5\n",
      "INFO:holodecml.vae.checkpointer:Loaded EarlyStopping checkpointer with patience 10\n",
      "INFO:holodecml.vae.checkpointer:Loaded a metrics logger test/training_log.csv to track the training results\n"
     ]
    }
   ],
   "source": [
    "# Initialize LR annealing scheduler \n",
    "schedule_config = config[\"callbacks\"][\"ReduceLROnPlateau\"]\n",
    "\n",
    "logging.info(\n",
    "    f\"Loaded ReduceLROnPlateau learning rate annealer with patience {schedule_config['patience']}\"\n",
    ")\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer,\n",
    "                              mode=schedule_config[\"mode\"],\n",
    "                              patience=schedule_config[\"patience\"],\n",
    "                              factor=schedule_config[\"factor\"],\n",
    "                              min_lr=schedule_config[\"min_lr\"],\n",
    "                              verbose=schedule_config[\"verbose\"])\n",
    "\n",
    "# Early stopping\n",
    "checkpoint_config = config[\"callbacks\"][\"EarlyStopping\"]\n",
    "early_stopping = EarlyStopping(path=model_save_path, \n",
    "                               patience=checkpoint_config[\"patience\"], \n",
    "                               verbose=checkpoint_config[\"verbose\"])\n",
    "\n",
    "# Write metrics to csv each epoch\n",
    "metrics_logger = MetricsLogger(path_save, reload = retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading trainer object\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Loading trainer object\")\n",
    "    \n",
    "trainer = Trainer(\n",
    "    vae,\n",
    "    optimizer,\n",
    "    train_gen,\n",
    "    valid_gen, \n",
    "    dataloader, \n",
    "    valid_dataloader,\n",
    "    batch_size,\n",
    "    path_save,\n",
    "    device,\n",
    "    test_image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training the model for up to 100 epochs starting at epoch 0\n",
      "loss: 67428.523 bce: 66929.622 kld: 498.902: 100%|██████████| 1563/1563 [16:55<00:00,  1.54it/s]\n",
      "val_loss: 67173.021 val_bce: 66552.117 val_kld: 620.904: 100%|██████████| 313/313 [01:08<00:00,  4.54it/s]\n",
      "INFO:holodecml.vae.checkpointer:Validation loss decreased on epoch 0 (inf --> 67173.021491).  Saving model.\n",
      "loss: 67182.075 bce: 66568.344 kld: 613.732:  63%|██████▎   | 978/1563 [10:37<06:20,  1.54it/s]"
     ]
    }
   ],
   "source": [
    "logging.info(\n",
    "    f\"Training the model for up to {epochs} epochs starting at epoch {start_epoch}\"\n",
    ")\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    train_loss, train_bce, train_kld = trainer.train_one_epoch(epoch)\n",
    "    test_loss, test_bce, test_kld = trainer.test(epoch)\n",
    "\n",
    "    scheduler.step(test_loss)\n",
    "    early_stopping(epoch, test_loss, trainer.model, trainer.optimizer)\n",
    "\n",
    "    # Write results to the callback logger \n",
    "    result = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_bce\": train_bce,\n",
    "        \"train_kld\": train_kld,\n",
    "        \"valid_loss\": test_loss,\n",
    "        \"valid_bce\": test_bce,\n",
    "        \"valid_kld\": test_kld,\n",
    "        \"lr\": early_stopping.print_learning_rate(trainer.optimizer)\n",
    "    }\n",
    "    metrics_logger.update(result)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_video(f\"{path_save}\", \"generated_hologram.avi\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
