{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import copy\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import *\n",
    "\n",
    "import nltk\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from holodecml.torch.utils import *\n",
    "from holodecml.torch.losses import *\n",
    "from holodecml.torch.visual import *\n",
    "from holodecml.torch.models import *\n",
    "from holodecml.torch.trainers import *\n",
    "from holodecml.torch.transforms import *\n",
    "from holodecml.torch.optimizers import *\n",
    "from holodecml.torch.data_loader import *\n",
    "from holodecml.torch.beam_search import *\n",
    "\n",
    "from aimlutils.hyper_opt.base_objective import *\n",
    "from aimlutils.torch.checkpoint import *\n",
    "#from aimlutils.torch.losses import *\n",
    "from aimlutils.utils.tqdm import *\n",
    "\n",
    "from typing import List, Callable, Tuple, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "# Stream output to stdout\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "root.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.yml\") as config_file:\n",
    "    conf = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.transforms:Loaded RandomVerticalFlip transformation with probability 0.5\n",
      "INFO:holodecml.torch.transforms:Loaded RandomHorizontalFlip transformation with probability 0.5\n",
      "INFO:holodecml.torch.transforms:Loaded Normalize transformation that normalizes data in the range 0 to 1\n",
      "INFO:holodecml.torch.transforms:Loaded ToTensor transformation, putting tensors on device cuda:0\n",
      "INFO:holodecml.torch.transforms:Loaded Normalize transformation that normalizes data in the range 0 to 1\n",
      "INFO:holodecml.torch.transforms:Loaded ToTensor transformation, putting tensors on device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the image/(x,y,z,d) transformations\n",
    "train_transform = LoadTransformations(conf[\"train_transforms\"], device = device)\n",
    "valid_transform = LoadTransformations(conf[\"validation_transforms\"], device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the readers\n",
    "scaler_path = os.path.join(conf[\"trainer\"][\"path_save\"], \"scalers.save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.data_loader:Loading reader-type multi\n",
      "INFO:holodecml.torch.data_loader:Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n",
      "INFO:holodecml.torch.data_loader:Loaded ['/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_multiparticle_training.nc', '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_12-25particle_gamma_600x400_training.nc'] hologram data containing 130000 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "train_gen = LoadReader( \n",
    "    transform = train_transform, \n",
    "    scaler = joblib.load(scaler_path) if os.path.isfile(scaler_path) else True,\n",
    "    config = conf[\"train_data\"]\n",
    ")\n",
    "\n",
    "if not os.path.isfile(scaler_path):\n",
    "    joblib.dump(train_gen.scaler, scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.data_loader:Loading reader-type multi\n",
      "INFO:holodecml.torch.data_loader:Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n",
      "INFO:holodecml.torch.data_loader:Loaded ['/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_multiparticle_validation.nc', '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_12-25particle_gamma_600x400_validation.nc'] hologram data containing 20000 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "valid_gen = LoadReader(\n",
    "    transform = valid_transform, \n",
    "    scaler = train_gen.scaler,\n",
    "    config = conf[\"validation_data\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Torch's iterator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data iterators from pytorch\n",
    "train_dataloader = DataLoader(\n",
    "    train_gen,\n",
    "    **conf[\"train_iterator\"]\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_gen,\n",
    "    **conf[\"valid_iterator\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.trainers:Loading trainer-type decoder-vae\n",
      "INFO:holodecml.torch.models:Loading model-type att-vae with settings\n",
      "INFO:holodecml.torch.models:weights: False\n",
      "INFO:holodecml.torch.models:image_channels: 1\n",
      "INFO:holodecml.torch.models:out_image_channels: 2\n",
      "INFO:holodecml.torch.models:hidden_dims: [50, 73, 96, 304, 906, 1820]\n",
      "INFO:holodecml.torch.models:z_dim: 1146\n",
      "INFO:holodecml.torch.models.cnn:Loaded a self-attentive encoder-decoder VAE model\n",
      "INFO:holodecml.torch.models.cnn:The model contains 264609625 trainable parameters\n",
      "INFO:holodecml.torch.models.cnn:Setting tunable parameter weights according to Xavier's uniform initialization\n",
      "INFO:holodecml.torch.trainers:Updating the output size of the RNN decoder to 124\n",
      "INFO:holodecml.torch.models:Loading model-type gru-decoder with settings\n",
      "INFO:holodecml.torch.models:hidden_size: 1146\n",
      "INFO:holodecml.torch.models:output_size: 124\n",
      "INFO:holodecml.torch.models:n_layers: 3\n",
      "INFO:holodecml.torch.models:dropout: 0.20486082287158408\n",
      "INFO:holodecml.torch.models:bidirectional: True\n",
      "INFO:holodecml.torch.models:weights: False\n",
      "INFO:holodecml.torch.models.rnn:The model contains 63506860 trainable parameters\n",
      "INFO:holodecml.torch.models.rnn:Setting tunable parameter weights according to Xavier's uniform initialization\n",
      "INFO:holodecml.torch.models:Loading model-type multi-head-dense with settings\n",
      "INFO:holodecml.torch.models:tasks: ['x', 'y', 'z', 'd']\n",
      "INFO:holodecml.torch.models:hidden_dims: [6308, 492, 492, 492]\n",
      "INFO:holodecml.torch.models:dropouts: [0.5, 0.3, 0.3, 0.3]\n",
      "INFO:holodecml.torch.models:weights: False\n",
      "INFO:holodecml.torch.models.dense:The model contains 103755508 trainable parameters\n",
      "INFO:holodecml.torch.models.dense:Setting tunable parameter weights according to Xavier's uniform initialization\n"
     ]
    }
   ],
   "source": [
    "trainer = LoadTrainer(\n",
    "    train_gen, \n",
    "    valid_gen, \n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    device, \n",
    "    conf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metrics and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aimlutils.torch.checkpoint.checkpointer:Loaded EarlyStopping checkpointer with patience 5\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Loaded EarlyStopping checkpointer with patience 3\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Loaded a metrics logger /glade/work/schreck/repos/holodec-ml/scripts/schreck/decoder/results/multi_particle/training_log.csv to track the training results\n"
     ]
    }
   ],
   "source": [
    "# Initialize LR annealing scheduler \n",
    "if \"ReduceLROnPlateau\" in conf[\"callbacks\"]:\n",
    "    if \"decoder\" in conf[\"callbacks\"][\"ReduceLROnPlateau\"]:\n",
    "        schedule_config1 = conf[\"callbacks\"][\"ReduceLROnPlateau\"][\"decoder\"]\n",
    "        scheduler_rnn = ReduceLROnPlateau(trainer.rnn_optimizer, **schedule_config1)\n",
    "    if \"regressor\" in conf[\"callbacks\"][\"ReduceLROnPlateau\"]:\n",
    "        schedule_config2 = conf[\"callbacks\"][\"ReduceLROnPlateau\"][\"regressor\"]\n",
    "        scheduler_linear = ReduceLROnPlateau(trainer.particle_optimizer, **schedule_config2)\n",
    "\n",
    "if \"ExponentialLR\" in conf[\"callbacks\"]:\n",
    "    if \"decoder\" in conf[\"callbacks\"][\"ExponentialLR\"]:\n",
    "        schedule_config1 = conf[\"callbacks\"][\"ExponentialLR\"][\"decoder\"]\n",
    "        scheduler_rnn = ExponentialLR(trainer.rnn_optimizer, **schedule_config1)\n",
    "    if \"regressor\" in conf[\"callbacks\"][\"ExponentialLR\"]:\n",
    "        schedule_config2 = conf[\"callbacks\"][\"ExponentialLR\"][\"regressor\"]\n",
    "        scheduler_linear = ExponentialLR(trainer.particle_optimizer, **schedule_config2)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_rnn = EarlyStopping(**conf[\"callbacks\"][\"EarlyStopping\"][\"decoder\"]) \n",
    "early_stopping_linear = EarlyStopping(**conf[\"callbacks\"][\"EarlyStopping\"][\"regressor\"])\n",
    "\n",
    "# Write metrics to csv each epoch\n",
    "metrics_logger = MetricsLogger(**conf[\"callbacks\"][\"MetricsLogger\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_bce: 0.577 train_mse: 0.877 train_acc: 0.003 train_stop_acc: 0.675 train_seq_acc: 0.014:   3%|â–Ž         | 73/2724 [00:41<23:34,  1.87it/s] "
     ]
    }
   ],
   "source": [
    "results = trainer.train(scheduler_rnn, scheduler_linear, early_stopping_rnn, early_stopping_linear, metrics_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
