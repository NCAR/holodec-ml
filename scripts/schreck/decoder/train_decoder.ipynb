{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools \n",
    "\n",
    "import copy\n",
    "import yaml\n",
    "import torch\n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import *\n",
    "\n",
    "import nltk\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from holodecml.torch.utils import *\n",
    "from holodecml.torch.losses import *\n",
    "from holodecml.torch.visual import *\n",
    "from holodecml.torch.models import *\n",
    "from holodecml.torch.trainers import *\n",
    "from holodecml.torch.transforms import *\n",
    "from holodecml.torch.optimizers import *\n",
    "from holodecml.torch.data_loader import *\n",
    "from holodecml.torch.beam_search import *\n",
    "\n",
    "from aimlutils.echo.src.base_objective import *\n",
    "from aimlutils.torch.checkpoint import *\n",
    "#from aimlutils.torch.losses import *\n",
    "from aimlutils.utils.tqdm import *\n",
    "\n",
    "from typing import List, Callable, Tuple, Dict, Union\n",
    "\n",
    "sys.path.append(\"/glade/work/schreck/repos/ALT/pytorch-seq2seq\")\n",
    "from seq2seq.models.DecoderRNN import DecoderRNN\n",
    "from seq2seq.models.TopKDecoder import TopKDecoder\n",
    "from seq2seq.loss import *\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "# Stream output to stdout\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "root.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transfer/2_10_nshot/10/model.yml\") as config_file:\n",
    "    conf = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.transforms:Loaded RandomVerticalFlip transformation with probability 0.5\n",
      "INFO:holodecml.torch.transforms:Loaded RandomHorizontalFlip transformation with probability 0.5\n",
      "INFO:holodecml.torch.transforms:Loaded Normalize transformation that normalizes data in the range 0 to 1\n",
      "INFO:holodecml.torch.transforms:Loaded ToTensor transformation, putting tensors on device cuda:0\n",
      "INFO:holodecml.torch.transforms:Loaded Normalize transformation that normalizes data in the range 0 to 1\n",
      "INFO:holodecml.torch.transforms:Loaded ToTensor transformation, putting tensors on device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the image/(x,y,z,d) transformations\n",
    "train_transform = LoadTransformations(conf[\"train_transforms\"], device = device)\n",
    "valid_transform = LoadTransformations(conf[\"validation_transforms\"], device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the readers\n",
    "scaler_path = os.path.join(conf[\"trainer\"][\"path_save\"], \"scalers.save\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.data_loader:Loading reader-type nshot_kways\n",
      "INFO:holodecml.torch.data_loader:Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n",
      "INFO:holodecml.torch.data_loader:Loaded {'/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_10particle_gamma_600x400_training.nc': 50000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_1particle_gamma_600x400_training.nc': 50000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_2particle_gamma_600x400_training.nc': 50000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_3particle_gamma_600x400_training.nc': 50000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_4particle_gamma_600x400_training.nc': 50000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_5particle_gamma_600x400_training.nc': 50000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_6particle_gamma_600x400_training.nc': 50000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_7particle_gamma_600x400_training.nc': 50000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_8particle_gamma_600x400_training.nc': 50000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_9particle_gamma_600x400_training.nc': 50000} hologram data containing 500000 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "train_gen = LoadReader( \n",
    "    transform = train_transform, \n",
    "    scaler = joblib.load(scaler_path) if os.path.isfile(scaler_path) else True,\n",
    "    config = conf[\"train_data\"]\n",
    ")\n",
    "\n",
    "if not os.path.isfile(scaler_path):\n",
    "    joblib.dump(train_gen.scaler, scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.data_loader:Loading reader-type multi\n",
      "INFO:holodecml.torch.data_loader:Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n",
      "INFO:holodecml.torch.data_loader:Loaded {'/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_10particle_gamma_600x400_validation.nc': 1000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_1particle_gamma_600x400_validation.nc': 1000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_2particle_gamma_600x400_validation.nc': 1000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_3particle_gamma_600x400_validation.nc': 1000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_4particle_gamma_600x400_validation.nc': 1000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_5particle_gamma_600x400_validation.nc': 1000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_6particle_gamma_600x400_validation.nc': 1000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_7particle_gamma_600x400_validation.nc': 1000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_8particle_gamma_600x400_validation.nc': 1000, '/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_9particle_gamma_600x400_validation.nc': 1000} hologram data containing 10000 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "valid_gen = LoadReader(\n",
    "    transform = valid_transform, \n",
    "    scaler = train_gen.scaler,\n",
    "    config = conf[\"validation_data\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Torch's iterator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data iterators from pytorch\n",
    "train_dataloader = DataLoader(\n",
    "    train_gen,\n",
    "    **conf[\"train_iterator\"]\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_gen,\n",
    "    **conf[\"valid_iterator\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.trainers:Loading trainer-type decoder-vae\n",
      "INFO:holodecml.torch.models:Loading model-type att-vae with settings\n",
      "INFO:holodecml.torch.models:hidden_dims: [114, 59, 34, 627, 544, 2000]\n",
      "INFO:holodecml.torch.models:image_channels: 1\n",
      "INFO:holodecml.torch.models:out_image_channels: 2\n",
      "INFO:holodecml.torch.models:weights: /glade/work/schreck/repos/holodec-ml/scripts/schreck/vae/results/double_channel_0203/best.pt\n",
      "INFO:holodecml.torch.models:z_dim: 1014\n",
      "INFO:holodecml.torch.models.cnn:Loaded a self-attentive encoder-decoder VAE model\n",
      "INFO:holodecml.torch.models.cnn:The model contains 198732538 trainable parameters\n",
      "INFO:holodecml.torch.models.cnn:Loading weights from /glade/work/schreck/repos/holodec-ml/scripts/schreck/vae/results/double_channel_0203/best.pt\n",
      "INFO:holodecml.torch.trainers:Updating the output size of the RNN decoder to 654\n",
      "INFO:holodecml.torch.models:Loading model-type gru-decoder with settings\n",
      "INFO:holodecml.torch.models.rnn:Using a standard softmax output activation\n",
      "INFO:holodecml.torch.models:bidirectional: True\n",
      "INFO:holodecml.torch.models:dropout: 0.20486082287158408\n",
      "INFO:holodecml.torch.models:hidden_size: 1014\n",
      "INFO:holodecml.torch.models:n_layers: 3\n",
      "INFO:holodecml.torch.models:output_size: 654\n",
      "INFO:holodecml.torch.models:weights: False\n",
      "INFO:holodecml.torch.models.rnn:The model contains 51380034 trainable parameters\n",
      "INFO:holodecml.torch.models.rnn:Setting tunable parameter weights according to Xavier's uniform initialization\n",
      "INFO:holodecml.torch.models:Loading model-type multi-head-dense with settings\n",
      "INFO:holodecml.torch.models:dropouts: [0.5, 0.3, 0.3, 0.3]\n",
      "INFO:holodecml.torch.models:hidden_dims: [6308, 492, 492, 492]\n",
      "INFO:holodecml.torch.models:tasks: ['x', 'y', 'z', 'd']\n",
      "INFO:holodecml.torch.models:weights: False\n",
      "INFO:holodecml.torch.models.dense:The model contains 122679508 trainable parameters\n",
      "INFO:holodecml.torch.models.dense:Setting tunable parameter weights according to Xavier's uniform initialization\n",
      "INFO:holodecml.torch.optimizers:Loaded the sgd optimizer with learning rate 0.001 and L2 penalty 0.0005\n",
      "INFO:holodecml.torch.optimizers:Loaded the sgd optimizer with learning rate 0.001 and L2 penalty 0.0005\n"
     ]
    }
   ],
   "source": [
    "trainer = LoadTrainer(\n",
    "    train_gen, \n",
    "    valid_gen, \n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    device, \n",
    "    conf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RMSLELoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.mse = nn.MSELoss()\n",
    "        \n",
    "#     def forward(self, pred, actual):\n",
    "#         return torch.sqrt(self.mse(torch.log(torch.abs(pred) + 1), torch.log(torch.abs(actual) + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metrics and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aimlutils.torch.checkpoint.checkpointer:Loaded EarlyStopping checkpointer with patience 10\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Loaded EarlyStopping checkpointer with patience 10\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Loaded a metrics logger /glade/work/schreck/repos/holodec-ml/scripts/schreck/decoder/transfer/2_10_nshot/10/training_log.csv to track the training results\n"
     ]
    }
   ],
   "source": [
    "# Initialize LR annealing scheduler \n",
    "if \"ReduceLROnPlateau\" in conf[\"callbacks\"]:\n",
    "    if \"decoder\" in conf[\"callbacks\"][\"ReduceLROnPlateau\"]:\n",
    "        schedule_config1 = conf[\"callbacks\"][\"ReduceLROnPlateau\"][\"decoder\"]\n",
    "        scheduler_rnn = ReduceLROnPlateau(trainer.rnn_optimizer, **schedule_config1)\n",
    "    if \"regressor\" in conf[\"callbacks\"][\"ReduceLROnPlateau\"]:\n",
    "        schedule_config2 = conf[\"callbacks\"][\"ReduceLROnPlateau\"][\"regressor\"]\n",
    "        scheduler_linear = ReduceLROnPlateau(trainer.particle_optimizer, **schedule_config2)\n",
    "\n",
    "if \"ExponentialLR\" in conf[\"callbacks\"]:\n",
    "    if \"decoder\" in conf[\"callbacks\"][\"ExponentialLR\"]:\n",
    "        schedule_config1 = conf[\"callbacks\"][\"ExponentialLR\"][\"decoder\"]\n",
    "        scheduler_rnn = ExponentialLR(trainer.rnn_optimizer, **schedule_config1)\n",
    "    if \"regressor\" in conf[\"callbacks\"][\"ExponentialLR\"]:\n",
    "        schedule_config2 = conf[\"callbacks\"][\"ExponentialLR\"][\"regressor\"]\n",
    "        scheduler_linear = ExponentialLR(trainer.particle_optimizer, **schedule_config2)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_rnn = EarlyStopping(**conf[\"callbacks\"][\"EarlyStopping\"][\"decoder\"])\n",
    "early_stopping_linear = EarlyStopping(**conf[\"callbacks\"][\"EarlyStopping\"][\"regressor\"])\n",
    "\n",
    "# Write metrics to csv each epoch\n",
    "metrics_logger = MetricsLogger(**conf[\"callbacks\"][\"MetricsLogger\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_bce: 4.489 train_mse: 0.488 train_acc: 0.001 train_stop_acc: 0.727 train_seq_acc: 0.003: 100%|██████████| 1000/1000 [02:23<00:00,  6.95it/s]\n",
      "Epoch 0 val_bce: 0.231 val_mae: 0.837 val_acc: 0.000 val_stop_acc: 0.340 val_seq_acc: 0.001: 100%|██████████| 157/157 [01:59<00:00,  1.31it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 0 (inf --> 0.999253).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 0 (inf --> 0.837095).  Saving model.\n",
      "Epoch 1 train_bce: 3.991 train_mse: 0.469 train_acc: 0.000 train_stop_acc: 0.998 train_seq_acc: 0.000: 100%|██████████| 1000/1000 [02:10<00:00,  7.64it/s]\n",
      "Epoch 1 val_bce: 0.182 val_mae: 0.832 val_acc: 0.000 val_stop_acc: 0.268 val_seq_acc: 0.000: 100%|██████████| 157/157 [01:51<00:00,  1.41it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 1 out of 10\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 1 (0.837095 --> 0.831994).  Saving model.\n",
      "Epoch 2 train_bce: 4.006 train_mse: 0.455 train_acc: 0.000 train_stop_acc: 1.000 train_seq_acc: 0.001: 100%|██████████| 1000/1000 [02:15<00:00,  7.37it/s]\n",
      "Epoch 2 val_bce: 0.176 val_mae: 0.830 val_acc: 0.000 val_stop_acc: 0.299 val_seq_acc: 0.001: 100%|██████████| 157/157 [01:51<00:00,  1.40it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 2 out of 10\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 2 (0.831994 --> 0.830324).  Saving model.\n",
      "Epoch 3 train_bce: 3.903 train_mse: 0.452 train_acc: 0.000 train_stop_acc: 1.000 train_seq_acc: 0.001: 100%|██████████| 1000/1000 [02:19<00:00,  7.16it/s]\n",
      "Epoch 3 val_bce: 0.172 val_mae: 0.827 val_acc: 0.000 val_stop_acc: 0.318 val_seq_acc: 0.001: 100%|██████████| 157/157 [01:53<00:00,  1.38it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 3 (0.999253 --> 0.999206).  Saving model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 3 (0.830324 --> 0.827350).  Saving model.\n",
      "Epoch 4 train_bce: 3.965 train_mse: 0.445 train_acc: 0.001 train_stop_acc: 1.000 train_seq_acc: 0.000: 100%|██████████| 1000/1000 [02:16<00:00,  7.32it/s]\n",
      "Epoch 4 val_bce: 0.172 val_mae: 0.826 val_acc: 0.000 val_stop_acc: 0.324 val_seq_acc: 0.001: 100%|██████████| 157/157 [01:53<00:00,  1.38it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 4 (0.999206 --> 0.999089).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 4 (0.827350 --> 0.825546).  Saving model.\n",
      "Epoch 5 train_bce: 3.914 train_mse: 0.441 train_acc: 0.000 train_stop_acc: 1.000 train_seq_acc: 0.001: 100%|██████████| 1000/1000 [02:22<00:00,  7.01it/s]\n",
      "Epoch 5 val_bce: 0.172 val_mae: 0.824 val_acc: 0.000 val_stop_acc: 0.329 val_seq_acc: 0.001: 100%|██████████| 157/157 [01:56<00:00,  1.35it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 1 out of 10\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 5 (0.825546 --> 0.823914).  Saving model.\n",
      "Epoch 6 train_bce: 3.930 train_mse: 0.434 train_acc: 0.001 train_stop_acc: 1.000 train_seq_acc: 0.001: 100%|██████████| 1000/1000 [02:17<00:00,  7.30it/s]\n",
      "Epoch 6 val_bce: 0.166 val_mae: 0.822 val_acc: 0.000 val_stop_acc: 0.338 val_seq_acc: 0.001:  47%|████▋     | 74/157 [00:55<00:57,  1.44it/s]"
     ]
    }
   ],
   "source": [
    "results = trainer.train(\n",
    "    scheduler_rnn,\n",
    "    scheduler_linear,\n",
    "    early_stopping_rnn,\n",
    "    early_stopping_linear,\n",
    "    metrics_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DTrainer:\n",
    "    \n",
    "#     def __init__(self, \n",
    "#                  train_gen,\n",
    "#                  valid_gen,\n",
    "#                  dataloader,\n",
    "#                  valid_dataloader,\n",
    "#                  vae_conf,\n",
    "#                  decoder_conf,\n",
    "#                  regressor_conf,\n",
    "#                  decoder_optimizer_conf,\n",
    "#                  regressor_optimizer_conf,\n",
    "#                  regressor_loss = \"mae\",\n",
    "#                  start_epoch=0,\n",
    "#                  epochs=100,\n",
    "#                  batches_per_epoch=100000000,\n",
    "#                  device=\"cpu\",\n",
    "#                  clip=2.0,\n",
    "#                  max_grad_norm=2.0,\n",
    "#                  alpha=1.0,\n",
    "#                  beta=1.0,\n",
    "#                  path_save=\"./\",\n",
    "#                  forcing = 0.0,\n",
    "#                  label_smoothing = 0.0,\n",
    "#                  focal_gamma = 0.0,\n",
    "#                  beam_size = 10, \n",
    "#                  PAD_token = 0,\n",
    "#                  SOS_token = 1,\n",
    "#                  EOS_token = 2):\n",
    "        \n",
    "#         self.train_gen = train_gen\n",
    "#         self.valid_gen = valid_gen\n",
    "#         self.dataloader = dataloader\n",
    "#         self.valid_dataloader = valid_dataloader\n",
    "        \n",
    "#         vae_conf = copy.deepcopy(vae_conf)\n",
    "#         decoder_conf = copy.deepcopy(decoder_conf)\n",
    "#         regressor_conf = copy.deepcopy(regressor_conf)\n",
    "        \n",
    "#         # Build vae\n",
    "#         vae = LoadModel(vae_conf)\n",
    "#         vae.build()\n",
    "#         self.vae = vae.to(device)\n",
    "        \n",
    "#         # Build decoder\n",
    "#         decoder_conf[\"output_size\"] = len(train_gen.token_lookup) + 3\n",
    "#         logger.info(\n",
    "#             f\"Updating the output size of the RNN decoder to {decoder_conf['output_size']}\"\n",
    "#         )\n",
    "#         #self.decoder = LoadModel(decoder_conf).to(device)\n",
    "#         #self.decoder.build()\n",
    "        \n",
    "#         self.decoder = DecoderRNN(\n",
    "#             vocab_size = decoder_conf[\"output_size\"],\n",
    "#             max_len = 3, \n",
    "#             hidden_size = 100, \n",
    "#             sos_id = SOS_token, \n",
    "#             eos_id = EOS_token, \n",
    "#             n_layers = 5, \n",
    "#             rnn_cell = 'gru',\n",
    "#             bidirectional = True, \n",
    "#             input_dropout_p = 0.2, \n",
    "#             dropout_p = 0.2,\n",
    "#             use_attention = False,\n",
    "#             input_size = conf[\"decoder\"][\"hidden_size\"]\n",
    "#         ).to(device)\n",
    "        \n",
    "#         self.decoder.n_layers = 5\n",
    "#         self.decoder.bidirectional = True\n",
    "        \n",
    "#         self.topk = TopKDecoder(self.decoder, beam_size).to(device)\n",
    "\n",
    "        \n",
    "#         # Build regressor\n",
    "#         self.regressor = LoadModel(regressor_conf)\n",
    "#         #self.regressor.build(vae_conf[\"z_dim\"] + decoder_conf[\"hidden_size\"] + 1250)\n",
    "#         self.regressor.build(100 + 100 + 1250)\n",
    "#         self.regressor = self.regressor.to(device)\n",
    "#         self.tasks = self.regressor.tasks\n",
    "        \n",
    "#         # Load RNN optimizer\n",
    "#         self.rnn_optimizer = LoadOptimizer(\n",
    "#             decoder_optimizer_conf,\n",
    "#             self.decoder.parameters()\n",
    "#         )\n",
    "#         self.particle_optimizer = LoadOptimizer(\n",
    "#             regressor_optimizer_conf,\n",
    "#             self.regressor.parameters()\n",
    "#         )\n",
    "        \n",
    "#         # Load other attributes\n",
    "#         self.batch_size = dataloader.batch_size\n",
    "#         self.batches_per_epoch = batches_per_epoch \n",
    "#         self.path_save = path_save\n",
    "#         self.device = device\n",
    "\n",
    "#         self.start_epoch = start_epoch\n",
    "#         self.epochs = epochs\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    "        \n",
    "#         self.forcing = forcing\n",
    "#         self.label_smoothing = label_smoothing\n",
    "#         self.focal_gamma = focal_gamma\n",
    "        \n",
    "#         # Tokenization, beam search and bleu\n",
    "#         self.PAD_token = PAD_token\n",
    "#         self.SOS_token = SOS_token\n",
    "#         self.EOS_token = EOS_token\n",
    "        \n",
    "#         max_steps = self.valid_gen.maxnum_particles\n",
    "#         self.beam_search = BeamSearch(\n",
    "#             end_index = EOS_token, \n",
    "#             max_steps = max_steps, \n",
    "#             beam_size = beam_size\n",
    "#         )\n",
    "        \n",
    "#         self.max_grad_norm = max_grad_norm        \n",
    "#         self.train_rnn = True\n",
    "\n",
    "#     def train_one_epoch(self, epoch, use_teacher_forcing):\n",
    "\n",
    "#         self.vae.eval()\n",
    "#         self.decoder.train()\n",
    "#         self.regressor.train()\n",
    "\n",
    "#         batch_size = self.dataloader.batch_size\n",
    "#         batches_per_epoch = int(np.ceil(self.train_gen.__len__() / batch_size))\n",
    "\n",
    "#         if self.batches_per_epoch < batches_per_epoch:\n",
    "#                 batches_per_epoch = self.batches_per_epoch\n",
    "\n",
    "#         batch_group_generator = tqdm(\n",
    "#             enumerate(self.dataloader), \n",
    "#             total=batches_per_epoch, \n",
    "#             leave=True\n",
    "#         )\n",
    "        \n",
    "#         criterion = WeightedCrossEntropyLoss(\n",
    "#             label_smoothing = self.label_smoothing,\n",
    "#             gamma = self.focal_gamma\n",
    "#         )\n",
    "\n",
    "#         epoch_losses = {\"mse\": [], \"bce\": [], \"accuracy\": [], \n",
    "#                         \"stop_accuracy\": [], \"frac\": [], \"seq_acc\": []}\n",
    "        \n",
    "#         for idx, (images, y_out, w_out) in batch_group_generator:\n",
    "\n",
    "#             images = images.to(self.device)\n",
    "#             y_out = {task: value.to(self.device) for task, value in y_out.items()}\n",
    "#             w_out = w_out.to(self.device)\n",
    "            \n",
    "#             if hasattr(self.train_gen, 'n_shot'): # Support for n-shot, k-ways\n",
    "#                 images = images.transpose(1, 0)\n",
    "#                 y_out = {task: value.squeeze(0) for task, value in y_out.items()}\n",
    "#                 w_out = w_out.squeeze(0)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 # 1. Predict the latent vector and image reconstruction\n",
    "#                 z, mu, logvar, encoder_att = self.vae.encode(images)\n",
    "#                 image_pred, decoder_att = self.vae.decode(z)\n",
    "\n",
    "#                 combined_att = torch.cat([\n",
    "#                     encoder_att[2].flatten(start_dim = 1),\n",
    "#                     decoder_att[0].flatten(start_dim = 1)\n",
    "#                 ], 1)\n",
    "#                 combined_att = combined_att.clone()\n",
    "\n",
    "#                 if self.vae.out_image_channels > 1:\n",
    "#                     z_real = np.sqrt(0.5) * image_pred[:,0,:,:]\n",
    "#                     z_imag = image_pred[:,1,:,:]\n",
    "#                     image_pred = torch.square(z_real) + torch.square(z_imag)\n",
    "#                     image_pred = torch.unsqueeze(image_pred, 1)\n",
    "\n",
    "#             # 2. Predict the number of particles\n",
    "#             decoder_input = torch.LongTensor([self.SOS_token] * w_out.shape[0]).to(self.device)\n",
    "#             encoded_image = z.to(self.device)\n",
    "            \n",
    "#             encoded_image = self.decoder.resize_hidden(encoded_image)\n",
    "            \n",
    "#             decoder_hidden = encoded_image.clone().reshape((1, w_out.shape[0], encoded_image.shape[-1]))\n",
    "            \n",
    "#             n_dims = 2 if self.decoder.bidirectional else 1\n",
    "#             n_dims *= self.decoder.n_layers\n",
    "#             if n_dims > 1:\n",
    "#                 decoder_hidden = torch.cat([decoder_hidden for k in range(n_dims)])\n",
    "                \n",
    "#             target_tensor = w_out.long()\n",
    "#             target_length = w_out.shape[1]\n",
    "#             seq_lens = w_out.max(axis = 1)[0] + 1\n",
    "#             class_weights = torch.ones(w_out.shape).to(self.device)\n",
    "            \n",
    "#             decoder_outputs, decoder_hidden, metadata = self.topk(\n",
    "#                 inputs = target_tensor, \n",
    "#                 encoder_hidden = decoder_hidden[:self.decoder.n_layers],\n",
    "#                 teacher_forcing_ratio = use_teacher_forcing\n",
    "#             )\n",
    "                        \n",
    "#             loss = Perplexity()\n",
    "#             acc, stop_acc, hidden_vectors = [], [], []\n",
    "#             seq_acc = defaultdict(list)\n",
    "#             for step, step_output in enumerate(decoder_outputs):\n",
    "#                 batch_size = target_tensor.size(0)\n",
    "#                 loss.eval_batch(step_output.contiguous().view(batch_size, -1), target_tensor[:, step])\n",
    "#                 topv, topi = step_output.contiguous().view(batch_size, -1).topk(1)\n",
    "#                 topi = topi.squeeze(1)\n",
    "#                 hvs = []\n",
    "#                 pred, true, real_particles = [], [], []\n",
    "#                 for i, (t, p) in enumerate(zip(target_tensor[:, step], topi)):\n",
    "#                     val = int(t.item() == p.item()) \n",
    "#                     if t.item() > 2:\n",
    "#                         acc.append(val)\n",
    "#                         hvs.append([t.item()]) # Use the true index\n",
    "#                         real_particles.append(i)\n",
    "                        \n",
    "#                     if t.item() == 2:\n",
    "#                         stop_acc.append(val)\n",
    "                        \n",
    "#                     pred.append(p.item())\n",
    "#                     true.append(t.item())\n",
    "\n",
    "#                 seq_acc[\"pred\"].append(pred)\n",
    "#                 seq_acc[\"true\"].append(true)\n",
    "                \n",
    "#                 if len(hvs) > 0:\n",
    "#                     hvs = torch.LongTensor(hvs).to(target_tensor.device)\n",
    "#                     embedding = self.decoder.embedding(hvs).squeeze(1)\n",
    "#                     hidden_vectors.append([real_particles, embedding])\n",
    "                    \n",
    "#             if self.max_grad_norm is not None and self.max_grad_norm > 0.0:\n",
    "#                 params = itertools.chain.from_iterable([group['params'] for group in self.rnn_optimizer.param_groups])\n",
    "#                 torch.nn.utils.clip_grad_norm_(params, self.max_grad_norm)\n",
    "                \n",
    "#             self.rnn_optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             self.rnn_optimizer.step()\n",
    "\n",
    "#             epoch_losses[\"bce\"].append(loss.get_loss())            \n",
    "#             epoch_losses[\"accuracy\"] += acc\n",
    "#             epoch_losses[\"stop_accuracy\"] += stop_acc\n",
    "\n",
    "\n",
    "#             if len(hidden_vectors) == 0:\n",
    "#                 continue\n",
    "\n",
    "#             # 3. Use particle embeddings to predict (x,y,z,d)\n",
    "#             regressor_loss = []\n",
    "#             true_part, pred_part = [], []\n",
    "#             for di in range(len(hidden_vectors)):\n",
    "#                 real_particles, h_vecs = hidden_vectors[di]   \n",
    "#                 catt = combined_att[real_particles]\n",
    "#                 x_input = torch.cat([h_vecs.detach(), encoded_image[real_particles].detach(), catt], axis = 1)     \n",
    "#                 particle_attributes = self.regressor(x_input)\n",
    "#                 loss = []\n",
    "#                 for task in self.tasks:\n",
    "#                     _loss = nn.L1Loss()(\n",
    "#                          particle_attributes[task].squeeze(1),\n",
    "#                          y_out[task][:, di][real_particles].float()\n",
    "#                     ) # XSigmoidLoss()\n",
    "#                     loss.append(_loss)\n",
    "#                 regressor_loss.append(torch.mean(torch.stack(loss)))\n",
    "#             regressor_loss = torch.mean(torch.stack(regressor_loss))\n",
    "            \n",
    "#             # Compute \"order-less\" accuracy \n",
    "#             _seq_acc = []\n",
    "#             _true = np.swapaxes(np.array(seq_acc[\"true\"]), 1, 0)\n",
    "#             _pred = np.swapaxes(np.array(seq_acc[\"pred\"]), 1, 0)\n",
    "#             for (true, pred) in zip(_true, _pred):\n",
    "#                 cond = (true > 2)\n",
    "#                 frac = (len(set(true[cond]) & set(pred[cond]))) / len(true[cond])\n",
    "#                 _seq_acc.append(frac)\n",
    "#             seq_acc = np.mean(_seq_acc)\n",
    "\n",
    "#             epoch_losses[\"mse\"].append(regressor_loss.item())\n",
    "#             epoch_losses[\"seq_acc\"].append(seq_acc)\n",
    "            \n",
    "#             # Normalize the accumulated gradient\n",
    "#             if self.max_grad_norm is not None and self.max_grad_norm > 0.0:\n",
    "#                 params = itertools.chain.from_iterable([group['params'] for group in self.particle_optimizer.param_groups])\n",
    "#                 torch.nn.utils.clip_grad_norm_(params, self.max_grad_norm)\n",
    "\n",
    "#             # Backprop on the regressor model\n",
    "#             self.particle_optimizer.zero_grad()\n",
    "#             regressor_loss.backward()\n",
    "#             self.particle_optimizer.step()\n",
    "\n",
    "#             to_print = \"Epoch {} train_bce: {:.3f} train_mse: {:.3f} train_acc: {:.3f} train_stop_acc: {:.3f} train_seq_acc: {:.3f}\".format(\n",
    "#                 epoch, \n",
    "#                 np.mean(epoch_losses[\"bce\"]), \n",
    "#                 np.mean(epoch_losses[\"mse\"]), \n",
    "#                 np.mean(epoch_losses[\"accuracy\"]), \n",
    "#                 np.mean(epoch_losses[\"stop_accuracy\"]),\n",
    "#                 np.mean(epoch_losses[\"seq_acc\"])\n",
    "#             )\n",
    "#             batch_group_generator.set_description(to_print)\n",
    "#             batch_group_generator.update()\n",
    "\n",
    "#             if idx % batches_per_epoch == 0 and idx > 0:\n",
    "#                 break\n",
    "\n",
    "#         return epoch_losses\n",
    "    \n",
    "    \n",
    "#     def test(self, epoch):\n",
    "    \n",
    "#         self.vae.eval()\n",
    "#         self.decoder.eval()\n",
    "#         self.regressor.eval()\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             batch_size = self.valid_dataloader.batch_size\n",
    "#             batches_per_epoch = int(np.ceil(self.valid_gen.__len__() / batch_size))\n",
    "\n",
    "#             batch_group_generator = tqdm(\n",
    "#                 enumerate(self.valid_dataloader), \n",
    "#                 total=batches_per_epoch, \n",
    "#                 leave=True\n",
    "#             )\n",
    "            \n",
    "#             criterion = WeightedCrossEntropyLoss()\n",
    "            \n",
    "#             epoch_losses = {\"mse\": [], \"bce\": [], \"frac\": [], \n",
    "#                             \"accuracy\": [], \"stop_accuracy\": [], \"seq_acc\": []}\n",
    "            \n",
    "#             for idx, (images, y_out, w_out) in batch_group_generator:\n",
    "#                 images = images.to(self.device)\n",
    "#                 y_out = {task: value.to(self.device) for task, value in y_out.items()}\n",
    "#                 w_out = w_out.to(self.device)\n",
    "                \n",
    "#                 if hasattr(self.valid_gen, 'n_shot'): # Support for n-shot, k-ways\n",
    "#                     images = images.transpose(1, 0)\n",
    "#                     y_out = {task: value.squeeze(0) for task, value in y_out.items()}\n",
    "#                     w_out = w_out.squeeze(0)\n",
    "\n",
    "#                 # 1. Predict the latent vector and image reconstruction\n",
    "#                 z, mu, logvar, encoder_att = self.vae.encode(images)\n",
    "#                 image_pred, decoder_att = self.vae.decode(z)\n",
    "                \n",
    "#                 combined_att = torch.cat([\n",
    "#                     encoder_att[2].flatten(start_dim = 1),\n",
    "#                     decoder_att[0].flatten(start_dim = 1)\n",
    "#                 ], 1)\n",
    "#                 combined_att = combined_att.clone()\n",
    "\n",
    "#                 if self.vae.out_image_channels > 1:\n",
    "#                     z_real = np.sqrt(0.5) * image_pred[:,0,:,:]\n",
    "#                     z_imag = image_pred[:,1,:,:]\n",
    "#                     image_pred = torch.square(z_real) + torch.square(z_imag)\n",
    "#                     image_pred = torch.unsqueeze(image_pred, 1)\n",
    "\n",
    "#                 # 2. Predict the number of particles\n",
    "#                 decoder_input = torch.LongTensor([self.SOS_token] * w_out.shape[0]).to(self.device)\n",
    "#                 encoded_image = z.to(self.device)\n",
    "                \n",
    "#                 encoded_image = self.decoder.resize_hidden(encoded_image)\n",
    "                \n",
    "#                 decoder_hidden = encoded_image.clone().reshape((1, w_out.shape[0], encoded_image.shape[-1]))\n",
    "                \n",
    "#                 n_dims = 2 if self.decoder.bidirectional else 1\n",
    "#                 n_dims *= self.decoder.n_layers\n",
    "#                 if n_dims > 1:\n",
    "#                     decoder_hidden = torch.cat([decoder_hidden for k in range(n_dims)])\n",
    "\n",
    "#                 target_tensor = w_out.long()\n",
    "#                 target_length = w_out.shape[1]\n",
    "#                 seq_lens = w_out.max(axis = 1)[0] + 1\n",
    "#                 class_weights = torch.ones(w_out.shape).to(self.device)\n",
    "                \n",
    "#                 decoder_outputs, decoder_hidden, metadata = self.topk(\n",
    "#                     inputs = target_tensor, \n",
    "#                     encoder_hidden = decoder_hidden[:self.decoder.n_layers]\n",
    "#                 )\n",
    "                \n",
    "#                 loss = NLLLoss()    \n",
    "#                 acc, stop_acc, hidden_vectors = [], [], []\n",
    "#                 seq_acc = defaultdict(list)\n",
    "#                 for step, step_output in enumerate(decoder_outputs):\n",
    "#                     batch_size = target_tensor.size(0)\n",
    "#                     loss.eval_batch(step_output.contiguous().view(batch_size, -1), target_tensor[:, step])\n",
    "#                     topv, topi = step_output.contiguous().view(batch_size, -1).topk(1)\n",
    "#                     topi = topi.squeeze(1)\n",
    "#                     hvs = []\n",
    "#                     pred, true, real_particles = [], [], []\n",
    "#                     for i, (t, p) in enumerate(zip(target_tensor[:, step], topi)):\n",
    "#                         val = int(t.item() == p.item()) \n",
    "#                         if t.item() > 2:\n",
    "#                             acc.append(val)\n",
    "#                             hvs.append([p.item()]) # Use the predicted index\n",
    "#                             real_particles.append(i)\n",
    "#                         if t.item() == 2:\n",
    "#                             stop_acc.append(val)\n",
    "#                         pred.append(p.item())\n",
    "#                         true.append(t.item())\n",
    "#                     seq_acc[\"pred\"].append(pred)\n",
    "#                     seq_acc[\"true\"].append(true)\n",
    "\n",
    "#                     if len(hvs) > 0:\n",
    "#                         hvs = torch.LongTensor(hvs).to(target_tensor.device)\n",
    "#                         embedding = self.decoder.embedding(hvs).squeeze(1)\n",
    "#                         hidden_vectors.append([real_particles, embedding])\n",
    "\n",
    "#                 loss = loss.get_loss()\n",
    "#                 epoch_losses[\"bce\"].append(loss)                \n",
    "#                 epoch_losses[\"accuracy\"] += acc\n",
    "#                 epoch_losses[\"stop_accuracy\"] += stop_acc\n",
    "                \n",
    "#                 if len(hidden_vectors) == 0:\n",
    "#                     continue\n",
    "\n",
    "#                 #3. Use particle embeddings to predict (x,y,z,d)\n",
    "#                 regressor_loss = []\n",
    "#                 true_part, pred_part, real_part = [], [], []\n",
    "                \n",
    "#                 for di in range(len(hidden_vectors)):\n",
    "#                     real_particles, h_vecs = hidden_vectors[di]    \n",
    "#                     catt = combined_att[real_particles]\n",
    "#                     x_input = torch.cat([h_vecs.detach(), encoded_image[real_particles].detach(), catt], axis = 1)     \n",
    "#                     particle_attributes = self.regressor(x_input)\n",
    "#                     batch_true, batch_pred = [], []\n",
    "#                     for task in self.tasks:\n",
    "#                         batch_true.append(y_out[task][:, di][real_particles].float())\n",
    "#                         batch_pred.append(particle_attributes[task].squeeze(1))\n",
    "#                     true_part.append(batch_true)\n",
    "#                     pred_part.append(batch_pred)\n",
    "#                     real_part.append(torch.LongTensor(real_particles).to(self.device))\n",
    "#                 regressor_loss = distance_sorted_loss(true_part, pred_part, real_part)\n",
    "                \n",
    "#                 # Compute \"order-less\" accuracy \n",
    "#                 _seq_acc = []\n",
    "#                 _true = np.swapaxes(np.array(seq_acc[\"true\"]), 1, 0)\n",
    "#                 _pred = np.swapaxes(np.array(seq_acc[\"pred\"]), 1, 0)\n",
    "#                 for (true, pred) in zip(_true, _pred):\n",
    "#                     cond = (true > 2)\n",
    "#                     frac = (len(set(true[cond]) & set(pred[cond]))) / len(true[cond])\n",
    "#                     _seq_acc.append(frac)\n",
    "#                 seq_acc = np.mean(_seq_acc)\n",
    "\n",
    "#                 epoch_losses[\"mse\"].append(regressor_loss.item())\n",
    "#                 epoch_losses[\"seq_acc\"].append(seq_acc)\n",
    "\n",
    "#                 to_print = \"Epoch {} val_bce: {:.3f} val_mae: {:.3f} val_acc: {:.3f} val_stop_acc: {:.3f} val_seq_acc: {:.3f}\".format(\n",
    "#                     epoch, \n",
    "#                     np.mean(epoch_losses[\"bce\"]), \n",
    "#                     np.mean(epoch_losses[\"mse\"]), \n",
    "#                     np.mean(epoch_losses[\"accuracy\"]),\n",
    "#                     np.mean(epoch_losses[\"stop_accuracy\"]),\n",
    "#                     np.mean(epoch_losses[\"seq_acc\"])\n",
    "#                 )\n",
    "\n",
    "#                 batch_group_generator.set_description(to_print)\n",
    "#                 batch_group_generator.update()\n",
    "\n",
    "#         return epoch_losses\n",
    "    \n",
    "#     def train(self, \n",
    "#               scheduler_rnn, \n",
    "#               scheduler_linear, \n",
    "#               early_stopping_rnn, \n",
    "#               early_stopping_linear, \n",
    "#               metrics_logger):\n",
    "\n",
    "#         flag_rnn = isinstance(scheduler_rnn, torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "#         flag_linear = isinstance(scheduler_linear, torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "\n",
    "#         for epoch in range(self.start_epoch, self.epochs):    \n",
    "#             tf = 1.0 * (self.forcing) ** epoch\n",
    "            \n",
    "#             train_losses = self.train_one_epoch(epoch, tf)\n",
    "#             test_losses = self.test(epoch)\n",
    "#             #test_losses = train_losses\n",
    "\n",
    "#             # Write results to the callback logger \n",
    "#             result = {\n",
    "#                 \"epoch\": epoch,\n",
    "#                 \"train_bce\": np.mean(train_losses[\"bce\"]),\n",
    "#                 \"valid_bce\": np.mean(test_losses[\"bce\"]),\n",
    "#                 \"train_mse\": np.mean(train_losses[\"mse\"]),\n",
    "#                 \"valid_mae\": np.mean(test_losses[\"mse\"]),\n",
    "#                 \"train_acc\": np.mean(train_losses[\"accuracy\"]),\n",
    "#                 \"valid_acc\": np.mean(test_losses[\"accuracy\"]),\n",
    "#                 \"train_stop_acc\": np.mean(train_losses[\"stop_accuracy\"]),\n",
    "#                 \"valid_stop_acc\": np.mean(test_losses[\"stop_accuracy\"]),                \n",
    "#                 \"train_seq_acc\": np.mean(train_losses[\"seq_acc\"]),\n",
    "#                 \"valid_seq_acc\": np.mean(test_losses[\"seq_acc\"]),\n",
    "#                 \"train_frac_overlap\": np.mean(train_losses[\"frac\"]),\n",
    "#                 \"valid_frac_overlap\": np.mean(test_losses[\"frac\"]),\n",
    "#                 \"lr_rnn\": early_stopping_rnn.print_learning_rate(self.rnn_optimizer),\n",
    "#                 \"lr_linear\": early_stopping_linear.print_learning_rate(self.particle_optimizer),\n",
    "#                 \"forcing_value\": tf\n",
    "#             }\n",
    "#             metrics_logger.update(result)\n",
    "\n",
    "#             if early_stopping_rnn.early_stop:# and early_stopping_linear.early_stop:\n",
    "#                 self.train_rnn = False\n",
    "                \n",
    "#             if early_stopping_linear.early_stop:\n",
    "#                 logger.info(\"Early stopping\")\n",
    "#                 break\n",
    "\n",
    "#             scheduler_rnn.step(1.0-result[\"valid_seq_acc\"] if flag_rnn else (1 + epoch))\n",
    "#             scheduler_linear.step(result[\"valid_mae\"] if flag_linear else (1 + epoch))\n",
    "            \n",
    "#             early_stopping_rnn(epoch, 1.0-result[\"valid_seq_acc\"], self.decoder, self.rnn_optimizer)\n",
    "#             early_stopping_linear(epoch, result[\"valid_mae\"], self.regressor, self.particle_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"type\" in conf[\"trainer\"]:\n",
    "#     conf[\"trainer\"].pop(\"type\")\n",
    "    \n",
    "# trainer = DTrainer(\n",
    "#     train_gen=train_gen,\n",
    "#     valid_gen=valid_gen,\n",
    "#     dataloader=train_dataloader,\n",
    "#     valid_dataloader=valid_dataloader,\n",
    "#     vae_conf=conf[\"vae\"],\n",
    "#     decoder_conf=conf[\"decoder\"],\n",
    "#     regressor_conf=conf[\"regressor\"],\n",
    "#     decoder_optimizer_conf=conf[\"rnn_optimizer\"],\n",
    "#     regressor_optimizer_conf=conf[\"particle_optimizer\"],\n",
    "#     device=device,\n",
    "#     **conf[\"trainer\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderTrainer:\n",
    "        \n",
    "#     def __init__(self, \n",
    "#                  train_gen,\n",
    "#                  valid_gen,\n",
    "#                  dataloader,\n",
    "#                  valid_dataloader,\n",
    "#                  vae_conf,\n",
    "#                  decoder_conf,\n",
    "#                  regressor_conf,\n",
    "#                  decoder_optimizer_conf,\n",
    "#                  regressor_optimizer_conf,\n",
    "#                  start_epoch=0,\n",
    "#                  epochs=100,\n",
    "#                  batches_per_epoch=100000000,\n",
    "#                  device=\"cpu\",\n",
    "#                  regressor_loss=\"mae\",\n",
    "#                  clip=2.0,\n",
    "#                  max_grad_norm=2.0,\n",
    "#                  alpha=1.0,\n",
    "#                  beta=1.0,\n",
    "#                  path_save=\"./\",\n",
    "#                  forcing = 0.0,\n",
    "#                  label_smoothing = 0.0,\n",
    "#                  focal_gamma = 0.0,\n",
    "#                  beam_size = 10, \n",
    "#                  PAD_token = 0,\n",
    "#                  SOS_token = 1,\n",
    "#                  EOS_token = 2):\n",
    "        \n",
    "#         self.train_gen = train_gen\n",
    "#         self.valid_gen = valid_gen\n",
    "#         self.dataloader = dataloader\n",
    "#         self.valid_dataloader = valid_dataloader\n",
    "        \n",
    "#         vae_conf = copy.deepcopy(vae_conf)\n",
    "#         decoder_conf = copy.deepcopy(decoder_conf)\n",
    "#         regressor_conf = copy.deepcopy(regressor_conf)\n",
    "        \n",
    "#         # Build vae        \n",
    "#         vae = LoadModel(vae_conf)\n",
    "#         vae.build()\n",
    "#         self.vae = vae.to(device)\n",
    "        \n",
    "#         # Build decoder\n",
    "#         decoder_conf[\"output_size\"] = len(train_gen.token_lookup) + 3\n",
    "#         logger.info(\n",
    "#             f\"Updating the output size of the RNN decoder to {decoder_conf['output_size']}\"\n",
    "#         )\n",
    "#         self.decoder = LoadModel(decoder_conf).to(device)\n",
    "#         self.decoder.build()\n",
    "        \n",
    "#         # Build regressor\n",
    "#         self.regressor = LoadModel(regressor_conf)\n",
    "#         self.regressor.build(vae_conf[\"z_dim\"] + 2 * decoder_conf[\"hidden_size\"] + 1250)\n",
    "#         self.regressor = self.regressor.to(device)\n",
    "#         self.tasks = self.regressor.tasks\n",
    "        \n",
    "#         # Load regressor loss \n",
    "#         self.regressor_loss = LoadLoss(regressor_loss)\n",
    "        \n",
    "#         # Load RNN optimizer\n",
    "#         self.rnn_optimizer = LoadOptimizer(\n",
    "#             decoder_optimizer_conf,\n",
    "#             self.decoder.parameters()\n",
    "#         )\n",
    "#         self.particle_optimizer = LoadOptimizer(\n",
    "#             regressor_optimizer_conf,\n",
    "#             self.regressor.parameters()\n",
    "#         )\n",
    "                \n",
    "#         # Load other attributes\n",
    "#         self.batch_size = dataloader.batch_size\n",
    "#         self.batches_per_epoch = batches_per_epoch \n",
    "#         self.path_save = path_save\n",
    "#         self.device = device\n",
    "\n",
    "#         self.start_epoch = start_epoch\n",
    "#         self.epochs = epochs\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    "        \n",
    "#         self.forcing = forcing\n",
    "#         self.label_smoothing = label_smoothing\n",
    "#         self.focal_gamma = focal_gamma\n",
    "        \n",
    "#         # Tokenization, beam search and bleu\n",
    "#         self.PAD_token = PAD_token\n",
    "#         self.SOS_token = SOS_token\n",
    "#         self.EOS_token = EOS_token\n",
    "        \n",
    "#         max_steps = self.valid_gen.maxnum_particles\n",
    "#         self.beam_search = BeamSearch(\n",
    "#             end_index = EOS_token, \n",
    "#             max_steps = max_steps, \n",
    "#             beam_size = beam_size\n",
    "#         )\n",
    "#         #self._bleu = BLEU(exclude_indices={PAD_token, EOS_token, SOS_token})\n",
    "        \n",
    "#         self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "#         self.train_rnn = True\n",
    "\n",
    "#     def train_one_epoch(self, epoch, use_teacher_forcing):\n",
    "\n",
    "#         self.vae.eval()\n",
    "#         self.decoder.train()\n",
    "#         self.regressor.train()\n",
    "\n",
    "#         batch_size = self.dataloader.batch_size\n",
    "#         batches_per_epoch = int(np.ceil(self.train_gen.__len__() / batch_size))\n",
    "\n",
    "#         if self.batches_per_epoch < batches_per_epoch:\n",
    "#                 batches_per_epoch = self.batches_per_epoch\n",
    "\n",
    "#         batch_group_generator = tqdm(\n",
    "#             enumerate(self.dataloader), \n",
    "#             total=batches_per_epoch, \n",
    "#             leave=True\n",
    "#         )\n",
    "        \n",
    "#         criterion = WeightedCrossEntropyLoss(\n",
    "#             label_smoothing = self.label_smoothing,\n",
    "#             gamma = self.focal_gamma\n",
    "#         )\n",
    "        \n",
    "#         diameter_loss = RMSLELoss()\n",
    "\n",
    "#         epoch_losses = {\"mse\": [], \"bce\": [], \"accuracy\": [], \n",
    "#                         \"stop_accuracy\": [], \"frac\": [], \"seq_acc\": []}\n",
    "        \n",
    "#         for idx, (images, y_out, w_out) in batch_group_generator:\n",
    "\n",
    "#             images = images.to(self.device)\n",
    "#             y_out = {task: value.to(self.device) for task, value in y_out.items()}\n",
    "#             w_out = w_out.to(self.device)\n",
    "            \n",
    "#             if hasattr(self.train_gen, 'n_shot'): # Support for n-shot, k-ways\n",
    "#                 images = images.transpose(1, 0)\n",
    "#                 y_out = {task: value.squeeze(0) for task, value in y_out.items()}\n",
    "#                 w_out = w_out.squeeze(0)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 # 1. Predict the latent vector and image reconstruction\n",
    "#                 z, mu, logvar, encoder_att = self.vae.encode(images)\n",
    "#                 image_pred, decoder_att = self.vae.decode(z)\n",
    "\n",
    "#                 combined_att = torch.cat([\n",
    "#                     encoder_att[2].flatten(start_dim = 1),\n",
    "#                     decoder_att[0].flatten(start_dim = 1)\n",
    "#                 ], 1)\n",
    "#                 combined_att = combined_att.clone()\n",
    "\n",
    "#                 if self.vae.out_image_channels > 1:\n",
    "#                     z_real = np.sqrt(0.5) * image_pred[:,0,:,:]\n",
    "#                     z_imag = image_pred[:,1,:,:]\n",
    "#                     image_pred = torch.square(z_real) + torch.square(z_imag)\n",
    "#                     image_pred = torch.unsqueeze(image_pred, 1)\n",
    "\n",
    "#             # 2. Predict the number of particles\n",
    "#             decoder_input = torch.LongTensor([self.SOS_token] * w_out.shape[0]).to(self.device)\n",
    "#             encoded_image = z.to(self.device)\n",
    "#             decoder_hidden = encoded_image.clone().reshape((1, w_out.shape[0], encoded_image.shape[-1]))\n",
    "            \n",
    "#             n_dims = 2 if self.decoder.bidirectional else 1\n",
    "#             n_dims *= self.decoder.n_layers\n",
    "#             if n_dims > 1:\n",
    "#                 decoder_hidden = torch.cat([decoder_hidden for k in range(n_dims)])\n",
    "\n",
    "#             target_tensor = w_out.long()\n",
    "#             target_length = w_out.shape[1]\n",
    "#             seq_lens = w_out.max(axis = 1)[0] + 1\n",
    "#             class_weights = torch.ones(w_out.shape).to(self.device)\n",
    "            \n",
    "#             # Use beam search to get predictions\n",
    "#             predictions, probabilities = self.beam_search.search(\n",
    "#                 decoder_input, decoder_hidden, self.decoder\n",
    "#             )\n",
    "\n",
    "#             # Validate on top-1 most likely sequence\n",
    "#             top_preds = predictions[:, 0, :]\n",
    "\n",
    "#             # Compute bleu metric for each sequence in the batch\n",
    "#             for pred, true in zip(top_preds, target_tensor):\n",
    "#                 epoch_losses[\"frac\"].append(frac_overlap(pred, true))\n",
    "\n",
    "#             # Reshape the predicted tensor to match with the target_tensor\n",
    "#             ## This will work only if limit the beam search = target size\n",
    "#             B, T = target_tensor.size()\n",
    "#             _, t = top_preds.size()\n",
    "#             if t < T:\n",
    "#                 reshaped_preds = torch.zeros(B, T)\n",
    "#                 reshaped_preds[:, :t] = top_preds\n",
    "#                 reshaped_preds = reshaped_preds.long().to(self.device)\n",
    "#             else:\n",
    "#                 reshaped_preds = top_preds\n",
    "                \n",
    "            \n",
    "#             # Decode again but force answers from the beam search\n",
    "#             hidden_vectors = []\n",
    "#             accuracy, stop_accuracy, rnn_loss = [], [], []\n",
    "            \n",
    "#             for di in range(target_length + 1):    \n",
    "#                 decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, seq_lens)\n",
    "#                 topv, topi = decoder_output.topk(1)\n",
    "                \n",
    "#                 force = (random.uniform(0, 1) < use_teacher_forcing)\n",
    "#                 if force:\n",
    "#                     decoder_input = target_tensor[:, di]\n",
    "#                 else:\n",
    "#                     decoder_input = reshaped_preds[:, di].detach()\n",
    "                    \n",
    "#                 c1 = (target_tensor[:, di] != self.PAD_token)\n",
    "#                 c2 = (target_tensor[:, di] != self.EOS_token)\n",
    "#                 condition = c1 & c2\n",
    "#                 real_plus_stop = torch.where(c1)\n",
    "#                 real_particles = torch.where(condition)\n",
    "#                 stop_token = torch.where(~c2)\n",
    "\n",
    "#                 if real_plus_stop[0].size(0) == 0:\n",
    "#                     break\n",
    "                    \n",
    "#                 rnn_loss.append(\n",
    "#                     criterion(\n",
    "#                         decoder_output[real_plus_stop], \n",
    "#                         target_tensor[:, di][real_plus_stop],\n",
    "#                         class_weights[:, di][real_plus_stop]\n",
    "#                     )\n",
    "#                 )\n",
    "                \n",
    "#                 accuracy += [\n",
    "#                     int(i.item()==j.item())\n",
    "#                     for i, j in zip(topi[real_particles], target_tensor[:, di][real_particles])\n",
    "#                 ]\n",
    "\n",
    "#                 if stop_token[0].size(0) > 0:\n",
    "#                     stop_accuracy += [\n",
    "#                         int(i.item()==j.item()) \n",
    "#                         for i, j in zip(topi[stop_token], target_tensor[:, di][stop_token])\n",
    "#                     ]\n",
    "                    \n",
    "#                 if real_particles[0].size(0) > 0:\n",
    "#                     token_input = target_tensor[:, di].squeeze() # topi.squeeze()\n",
    "#                     embedding = self.decoder.embed(token_input).squeeze(0)\n",
    "                    \n",
    "#                     embedding = torch.cat([embedding, torch.mean(decoder_hidden, dim=0)], -1)\n",
    "                    \n",
    "#                     hidden_vectors.append([real_particles, embedding])\n",
    "\n",
    "#             # Compute error and accuracy after finding closest particles \n",
    "#             accuracy = np.mean(accuracy)\n",
    "#             epoch_losses[\"accuracy\"].append(accuracy)\n",
    "#             epoch_losses[\"stop_accuracy\"].append(np.mean(stop_accuracy))\n",
    "\n",
    "#             rnn_loss = torch.mean(torch.stack(rnn_loss))\n",
    "#             epoch_losses[\"bce\"].append(rnn_loss.item())    \n",
    "\n",
    "#             if self.train_rnn:\n",
    "                \n",
    "#                 # Normalize the accumulated gradient\n",
    "#                 if self.max_grad_norm is not None and self.max_grad_norm > 0.0:\n",
    "#                     torch.nn.utils.clip_grad_norm_(\n",
    "#                         self.decoder.parameters(), \n",
    "#                         self.max_grad_norm\n",
    "#                     )\n",
    "                \n",
    "#                 self.rnn_optimizer.zero_grad()\n",
    "#                 rnn_loss.backward()\n",
    "#                 self.rnn_optimizer.step()\n",
    "\n",
    "#             if len(hidden_vectors) == 0:\n",
    "#                 continue\n",
    "\n",
    "#             # 3. Use particle embeddings to predict (x,y,z,d)\n",
    "#             regressor_loss = []\n",
    "#             true_part, pred_part = [], []\n",
    "#             for di in range(len(hidden_vectors)):\n",
    "#                 real_particles, h_vecs = hidden_vectors[di]    \n",
    "#                 x_input = torch.cat([h_vecs.detach(), encoded_image, combined_att], axis = 1)            \n",
    "#                 particle_attributes = self.regressor(x_input[real_particles])\n",
    "#                 loss = []\n",
    "#                 for task in self.tasks:\n",
    "#                     _loss = self.regressor_loss(\n",
    "#                          particle_attributes[task].squeeze(1),\n",
    "#                          y_out[task][:, di][real_particles].float()\n",
    "#                     )\n",
    "#                     loss.append(_loss)\n",
    "#                 regressor_loss.append(torch.mean(torch.stack(loss)))\n",
    "#             regressor_loss = torch.mean(torch.stack(regressor_loss))\n",
    "            \n",
    "#             # Compute \"order-less\" accuracy \n",
    "#             seq_acc = []\n",
    "#             for (true, pred) in zip(target_tensor, reshaped_preds):\n",
    "#                 cond = (true > 2)\n",
    "#                 frac = orderless_acc(true[cond], pred[cond])\n",
    "#                 seq_acc.append(frac)\n",
    "#             seq_acc = np.mean(seq_acc)\n",
    "            \n",
    "#             epoch_losses[\"mse\"].append(regressor_loss.item())\n",
    "#             epoch_losses[\"seq_acc\"].append(seq_acc)\n",
    "            \n",
    "            \n",
    "#             # Normalize the accumulated gradient\n",
    "#             if self.max_grad_norm is not None and self.max_grad_norm > 0.0:\n",
    "#                 torch.nn.utils.clip_grad_norm_(\n",
    "#                     self.regressor.parameters(), \n",
    "#                     self.max_grad_norm\n",
    "#                 )\n",
    "\n",
    "#             # Backprop on the regressor model\n",
    "#             self.particle_optimizer.zero_grad()\n",
    "#             regressor_loss.backward()\n",
    "#             self.particle_optimizer.step()\n",
    "\n",
    "#             to_print = \"Epoch {} train_bce: {:.3f} train_mse: {:.3f} train_acc: {:.3f} train_stop_acc: {:.3f} train_seq_acc: {:.3f}\".format(\n",
    "#                 epoch, \n",
    "#                 np.mean(epoch_losses[\"bce\"]), \n",
    "#                 np.mean(epoch_losses[\"mse\"]), \n",
    "#                 np.mean(epoch_losses[\"accuracy\"]), \n",
    "#                 np.mean(epoch_losses[\"stop_accuracy\"]),\n",
    "#                 np.mean(epoch_losses[\"seq_acc\"])\n",
    "#             )\n",
    "#             batch_group_generator.set_description(to_print)\n",
    "#             batch_group_generator.update()\n",
    "\n",
    "#             if idx % batches_per_epoch == 0 and idx > 0:\n",
    "#                 break\n",
    "\n",
    "#         return epoch_losses\n",
    "    \n",
    "    \n",
    "#     def test(self, epoch):\n",
    "    \n",
    "#         self.vae.eval()\n",
    "#         self.decoder.eval()\n",
    "#         self.regressor.eval()\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             batch_size = self.valid_dataloader.batch_size\n",
    "#             batches_per_epoch = int(np.ceil(self.valid_gen.__len__() / batch_size))\n",
    "\n",
    "#             batch_group_generator = tqdm(\n",
    "#                 enumerate(self.valid_dataloader), \n",
    "#                 total=batches_per_epoch, \n",
    "#                 leave=True\n",
    "#             )\n",
    "            \n",
    "#             criterion = WeightedCrossEntropyLoss()\n",
    "            \n",
    "#             epoch_losses = {\"mse\": [], \"bce\": [], \"frac\": [], \n",
    "#                             \"accuracy\": [], \"stop_accuracy\": [], \"seq_acc\": []}\n",
    "            \n",
    "#             for idx, (images, y_out, w_out) in batch_group_generator:\n",
    "#                 images = images.to(self.device)\n",
    "#                 y_out = {task: value.to(self.device) for task, value in y_out.items()}\n",
    "#                 w_out = w_out.to(self.device)\n",
    "                \n",
    "#                 if hasattr(self.valid_gen, 'n_shot'): # Support for n-shot, k-ways\n",
    "#                     images = images.transpose(1, 0)\n",
    "#                     y_out = {task: value.squeeze(0) for task, value in y_out.items()}\n",
    "#                     w_out = w_out.squeeze(0)\n",
    "\n",
    "#                 # 1. Predict the latent vector and image reconstruction\n",
    "#                 z, mu, logvar, encoder_att = self.vae.encode(images)\n",
    "#                 image_pred, decoder_att = self.vae.decode(z)\n",
    "                \n",
    "#                 combined_att = torch.cat([\n",
    "#                     encoder_att[2].flatten(start_dim = 1),\n",
    "#                     decoder_att[0].flatten(start_dim = 1)\n",
    "#                 ], 1)\n",
    "#                 combined_att = combined_att.clone()\n",
    "\n",
    "#                 if self.vae.out_image_channels > 1:\n",
    "#                     z_real = np.sqrt(0.5) * image_pred[:,0,:,:]\n",
    "#                     z_imag = image_pred[:,1,:,:]\n",
    "#                     image_pred = torch.square(z_real) + torch.square(z_imag)\n",
    "#                     image_pred = torch.unsqueeze(image_pred, 1)\n",
    "\n",
    "#                 # 2. Predict the number of particles\n",
    "#                 decoder_input = torch.LongTensor([self.SOS_token] * w_out.shape[0]).to(self.device)\n",
    "#                 encoded_image = z.to(self.device)\n",
    "#                 decoder_hidden = encoded_image.clone().reshape((1, w_out.shape[0], encoded_image.shape[-1]))\n",
    "                \n",
    "#                 n_dims = 2 if self.decoder.bidirectional else 1\n",
    "#                 n_dims *= self.decoder.n_layers\n",
    "#                 if n_dims > 1:\n",
    "#                     decoder_hidden = torch.cat([decoder_hidden for k in range(n_dims)])\n",
    "\n",
    "#                 target_tensor = w_out.long()\n",
    "#                 target_length = w_out.shape[1]\n",
    "#                 seq_lens = w_out.max(axis = 1)[0] + 1\n",
    "#                 class_weights = torch.ones(w_out.shape).to(self.device)\n",
    "\n",
    "#                 #\n",
    "#                 hidden_vectors = []\n",
    "#                 bleu, accuracy, stop_accuracy, rnn_loss = [], [], [], []\n",
    "                \n",
    "#                 # Use beam search to get predictions\n",
    "#                 predictions, probabilities = self.beam_search.search(\n",
    "#                     decoder_input, decoder_hidden, self.decoder\n",
    "#                 )\n",
    "\n",
    "#                 # Validate on top-1 most likely sequence\n",
    "#                 top_preds = predictions[:, 0, :]\n",
    "                \n",
    "#                 # Compute bleu metric for each sequence in the batch\n",
    "#                 for pred, true in zip(top_preds, target_tensor):\n",
    "#                     epoch_losses[\"frac\"].append(frac_overlap(pred, true))\n",
    "                \n",
    "#                 # Reshape the predicted tensor to match with the target_tensor\n",
    "#                 ## This will work only if limit the beam search = target size\n",
    "#                 B, T = target_tensor.size()\n",
    "#                 _, t = top_preds.size()\n",
    "#                 if t < T:\n",
    "#                     reshaped_preds = torch.zeros(B, T)\n",
    "#                     reshaped_preds[:, :t] = top_preds\n",
    "#                     reshaped_preds = reshaped_preds.long().to(self.device)\n",
    "#                 else:\n",
    "#                     reshaped_preds = top_preds\n",
    "    \n",
    "#                 # Use greedy evaluation to get the loss\n",
    "#                 for di in range(target_length + 1):\n",
    "#                     decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "#                     topv, topi = decoder_output.topk(1)\n",
    "#                     decoder_input = reshaped_preds[:, di].detach()\n",
    "#                     c1 = (target_tensor[:, di] != self.PAD_token)\n",
    "#                     c2 = (target_tensor[:, di] != self.EOS_token)\n",
    "#                     condition = c1 & c2\n",
    "#                     real_plus_stop = torch.where(c1)\n",
    "#                     real_particles = torch.where(condition)\n",
    "#                     stop_token = torch.where(~c2)\n",
    "\n",
    "#                     if real_plus_stop[0].size(0) == 0:\n",
    "#                         break\n",
    "                                          \n",
    "#                     rnn_loss.append(\n",
    "#                         criterion(\n",
    "#                             decoder_output[real_plus_stop], \n",
    "#                             target_tensor[:, di][real_plus_stop],\n",
    "#                             class_weights[:, di][real_plus_stop]\n",
    "#                         )\n",
    "#                     )\n",
    "#                     accuracy += [\n",
    "#                         int(i.item()==j.item())\n",
    "#                         for i, j in zip(reshaped_preds[:, di][real_particles], \n",
    "#                                         target_tensor[:, di][real_particles])\n",
    "#                     ]\n",
    "\n",
    "#                     if stop_token[0].size(0) > 0:\n",
    "#                         stop_accuracy += [\n",
    "#                             int(i.item()==j.item()) \n",
    "#                             for i, j in zip(reshaped_preds[:, di][stop_token], \n",
    "#                                             target_tensor[:, di][stop_token])\n",
    "#                         ]\n",
    "\n",
    "#                     if real_particles[0].size(0) > 0:\n",
    "#                         token_input = reshaped_preds[:, di] # topi.squeeze()\n",
    "#                         embedding = self.decoder.embed(token_input).squeeze(0)\n",
    "                        \n",
    "#                         embedding = torch.cat([embedding, torch.mean(decoder_hidden, dim=0)], -1)\n",
    "                        \n",
    "#                         hidden_vectors.append([real_particles, embedding])\n",
    "                        \n",
    "#                 epoch_losses[\"accuracy\"].append(np.mean(accuracy))\n",
    "#                 epoch_losses[\"stop_accuracy\"].append(np.mean(stop_accuracy))\n",
    "#                 rnn_loss = torch.mean(torch.stack(rnn_loss))\n",
    "#                 epoch_losses[\"bce\"].append(rnn_loss.item())\n",
    "\n",
    "#                 if len(hidden_vectors) == 0:\n",
    "#                     continue\n",
    "\n",
    "#                 #3. Use particle embeddings to predict (x,y,z,d)\n",
    "#                 regressor_loss = []\n",
    "#                 true_part, pred_part, real_part = [], [], []\n",
    "#                 for di in range(len(hidden_vectors)):\n",
    "#                     real_particles, h_vecs = hidden_vectors[di]\n",
    "#                     x_input = torch.cat([h_vecs.detach(), encoded_image, combined_att], axis = 1)\n",
    "#                     particle_attributes = self.regressor(x_input[real_particles])\n",
    "#                     batch_true, batch_pred = [], []\n",
    "#                     for task in self.tasks:\n",
    "#                         batch_true.append(y_out[task][:, di][real_particles].float())\n",
    "#                         batch_pred.append(particle_attributes[task].squeeze(1))\n",
    "#                     true_part.append(batch_true)\n",
    "#                     pred_part.append(batch_pred)\n",
    "#                     real_part.append(real_particles[0].cpu().numpy())\n",
    "#                 regressor_loss = distance_sorted_loss(true_part, pred_part, real_part)\n",
    "                \n",
    "#                 seq_acc = []\n",
    "#                 for (true, pred) in zip(target_tensor, reshaped_preds):\n",
    "#                     cond = (true > 2)\n",
    "#                     frac = orderless_acc(true[cond], pred[cond])\n",
    "#                     seq_acc.append(frac)\n",
    "#                 seq_acc = np.mean(seq_acc)\n",
    "\n",
    "#                 epoch_losses[\"mse\"].append(regressor_loss.item())\n",
    "#                 epoch_losses[\"seq_acc\"].append(seq_acc)\n",
    "\n",
    "#                 to_print = \"Epoch {} val_bce: {:.3f} val_mae: {:.3f} val_acc: {:.3f} val_stop_acc: {:.3f} val_seq_acc: {:.3f}\".format(\n",
    "#                     epoch, \n",
    "#                     np.mean(epoch_losses[\"bce\"]), \n",
    "#                     np.mean(epoch_losses[\"mse\"]), \n",
    "#                     np.mean(epoch_losses[\"accuracy\"]),\n",
    "#                     np.mean(epoch_losses[\"stop_accuracy\"]),\n",
    "#                     np.mean(epoch_losses[\"seq_acc\"])\n",
    "#                 )\n",
    "\n",
    "#                 batch_group_generator.set_description(to_print)\n",
    "#                 batch_group_generator.update()\n",
    "\n",
    "#         return epoch_losses\n",
    "    \n",
    "#     def train(self, \n",
    "#               scheduler_rnn, \n",
    "#               scheduler_linear, \n",
    "#               early_stopping_rnn\n",
    ", \n",
    "#               early_stopping_linear, \n",
    "#               metrics_logger):\n",
    "\n",
    "#         flag_rnn = isinstance(scheduler_rnn, torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "#         flag_linear = isinstance(scheduler_linear, torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "\n",
    "#         for epoch in range(self.start_epoch, self.epochs):    \n",
    "#             tf = 1.0 * (self.forcing) ** epoch\n",
    "            \n",
    "#             train_losses = self.train_one_epoch(epoch, tf)\n",
    "#             test_losses = self.test(epoch)\n",
    "\n",
    "#             # Write results to the callback logger \n",
    "#             result = {\n",
    "#                 \"epoch\": epoch,\n",
    "#                 \"train_bce\": np.mean(train_losses[\"bce\"]),\n",
    "#                 \"valid_bce\": np.mean(test_losses[\"bce\"]),\n",
    "#                 \"train_mse\": np.mean(train_losses[\"mse\"]),\n",
    "#                 \"valid_mae\": np.mean(test_losses[\"mse\"]),\n",
    "#                 \"train_acc\": np.mean(train_losses[\"accuracy\"]),\n",
    "#                 \"valid_acc\": np.mean(test_losses[\"accuracy\"]),\n",
    "#                 \"train_stop_acc\": np.mean(train_losses[\"stop_accuracy\"]),\n",
    "#                 \"valid_stop_acc\": np.mean(test_losses[\"stop_accuracy\"]),                \n",
    "#                 \"train_seq_acc\": np.mean(train_losses[\"seq_acc\"]),\n",
    "#                 \"valid_seq_acc\": np.mean(test_losses[\"seq_acc\"]),\n",
    "#                 \"train_frac_overlap\": np.mean(train_losses[\"frac\"]),\n",
    "#                 \"valid_frac_overlap\": np.mean(test_losses[\"frac\"]),\n",
    "#                 \"lr_rnn\": early_stopping_rnn.print_learning_rate(self.rnn_optimizer),\n",
    "#                 \"lr_linear\": early_stopping_linear.print_learning_rate(self.particle_optimizer),\n",
    "#                 \"forcing_value\": tf\n",
    "#             }\n",
    "#             metrics_logger.update(result)\n",
    "\n",
    "#             if early_stopping_rnn.early_stop:# and early_stopping_linear.early_stop:\n",
    "#                 self.train_rnn = False\n",
    "#             if early_stopping_linear.early_stop:\n",
    "#                 logger.info(\"Early stopping\")\n",
    "#                 break\n",
    "\n",
    "#             scheduler_rnn.step(1.0-result[\"valid_seq_acc\"] if flag_rnn else (1 + epoch))\n",
    "#             scheduler_linear.step(result[\"valid_mae\"] if flag_linear else (1 + epoch))\n",
    "            \n",
    "#             early_stopping_rnn(epoch, 1.0-result[\"valid_seq_acc\"], self.decoder, self.rnn_optimizer)\n",
    "#             early_stopping_linear(epoch, result[\"valid_mae\"], self.regressor, self.particle_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"type\" in conf[\"trainer\"]:\n",
    "#     conf[\"trainer\"].pop(\"type\")\n",
    "    \n",
    "# trainer = DecoderTrainer(\n",
    "#     train_gen=train_gen,\n",
    "#     valid_gen=valid_gen,\n",
    "#     dataloader=train_dataloader,\n",
    "#     valid_dataloader=valid_dataloader,\n",
    "#     vae_conf=conf[\"vae\"],\n",
    "#     decoder_conf=conf[\"decoder\"],\n",
    "#     regressor_conf=conf[\"regressor\"],\n",
    "#     decoder_optimizer_conf=conf[\"rnn_optimizer\"],\n",
    "#     regressor_optimizer_conf=conf[\"particle_optimizer\"],\n",
    "#     device=device,\n",
    "#     **conf[\"trainer\"]\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
