{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import copy\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import *\n",
    "\n",
    "import nltk\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from holodecml.torch.utils import *\n",
    "from holodecml.torch.losses import *\n",
    "from holodecml.torch.visual import *\n",
    "from holodecml.torch.models import *\n",
    "from holodecml.torch.trainers import *\n",
    "from holodecml.torch.transforms import *\n",
    "from holodecml.torch.optimizers import *\n",
    "from holodecml.torch.data_loader import *\n",
    "from holodecml.torch.beam_search import *\n",
    "\n",
    "from aimlutils.hyper_opt.base_objective import *\n",
    "from aimlutils.torch.checkpoint import *\n",
    "#from aimlutils.torch.losses import *\n",
    "from aimlutils.utils.tqdm import *\n",
    "\n",
    "from typing import List, Callable, Tuple, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "# Stream output to stdout\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "root.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/test/model.yml\") as config_file:\n",
    "    conf = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image/(x,y,z,d) transformations\n",
    "train_transform = LoadTransformations(conf[\"train_transforms\"], device = device)\n",
    "valid_transform = LoadTransformations(conf[\"validation_transforms\"], device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the readers\n",
    "scaler_path = os.path.join(conf[\"trainer\"][\"path_save\"], \"scalers.save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.transforms:Loaded RandomVerticalFlip transformation with probability 0.5\n",
      "INFO:holodecml.torch.transforms:Loaded RandomHorizontalFlip transformation with probability 0.5\n",
      "INFO:holodecml.torch.transforms:Loaded Normalize transformation that normalizes data in the range 0 to 1\n",
      "INFO:holodecml.torch.transforms:Loaded ToTensor transformation, putting tensors on device cuda:0\n",
      "INFO:holodecml.torch.data_loader:Loading reader-type multi\n",
      "INFO:holodecml.torch.data_loader:Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n",
      "INFO:holodecml.torch.data_loader:Loaded ['/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_3particle_training.nc'] hologram data containing 50000 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "train_gen = LoadReader( \n",
    "    transform = train_transform, \n",
    "    scaler = joblib.load(scaler_path) if os.path.isfile(scaler_path) else True,\n",
    "    config = conf[\"train_data\"]\n",
    ")\n",
    "\n",
    "if not os.path.isfile(scaler_path):\n",
    "    joblib.dump(train_gen.scaler, scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.transforms:Loaded Normalize transformation that normalizes data in the range 0 to 1\n",
      "INFO:holodecml.torch.transforms:Loaded ToTensor transformation, putting tensors on device cuda:0\n",
      "INFO:holodecml.torch.data_loader:Loading reader-type multi\n",
      "INFO:holodecml.torch.data_loader:Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n",
      "INFO:holodecml.torch.data_loader:Loaded ['/glade/p/cisl/aiml/ai4ess_hackathon/holodec/synthetic_holograms_3particle_validation.nc'] hologram data containing 10000 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data scaler transformation {'x': StandardScaler(copy=True, with_mean=True, with_std=True), 'y': StandardScaler(copy=True, with_mean=True, with_std=True), 'z': StandardScaler(copy=True, with_mean=True, with_std=True), 'd': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "valid_gen = LoadReader(\n",
    "    transform = valid_transform, \n",
    "    scaler = train_gen.scaler,\n",
    "    config = conf[\"validation_data\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Torch's iterator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data iterators from pytorch\n",
    "train_dataloader = DataLoader(\n",
    "    train_gen,\n",
    "    **conf[\"train_iterator\"]\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_gen,\n",
    "    **conf[\"valid_iterator\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:holodecml.torch.trainers:Loading trainer-type decoder-vae\n",
      "INFO:holodecml.torch.models:Loading model-type att-vae with settings\n",
      "INFO:holodecml.torch.models:weights: /glade/work/schreck/repos/holodec-ml/scripts/schreck/vae/results/double_channel_1221/best.pt\n",
      "INFO:holodecml.torch.models:image_channels: 1\n",
      "INFO:holodecml.torch.models:out_image_channels: 2\n",
      "INFO:holodecml.torch.models:hidden_dims: [50, 73, 96, 304, 906, 1820]\n",
      "INFO:holodecml.torch.models:z_dim: 1146\n",
      "INFO:holodecml.torch.models.cnn:Loaded a self-attentive encoder-decoder VAE model\n",
      "INFO:holodecml.torch.models.cnn:The model contains 264609625 trainable parameters\n",
      "INFO:holodecml.torch.models.cnn:Loading weights from /glade/work/schreck/repos/holodec-ml/scripts/schreck/vae/results/double_channel_1221/best.pt\n",
      "INFO:holodecml.torch.trainers:Updating the output size of the RNN decoder to 124\n",
      "INFO:holodecml.torch.models:Loading model-type gru-decoder with settings\n",
      "INFO:holodecml.torch.models:hidden_size: 1146\n",
      "INFO:holodecml.torch.models:output_size: 124\n",
      "INFO:holodecml.torch.models:n_layers: 3\n",
      "INFO:holodecml.torch.models:dropout: 0.20486082287158408\n",
      "INFO:holodecml.torch.models:bidirectional: True\n",
      "INFO:holodecml.torch.models:weights: False\n",
      "INFO:holodecml.torch.models.rnn:The model contains 63506860 trainable parameters\n",
      "INFO:holodecml.torch.models.rnn:Setting tunable parameter weights according to Xavier's uniform initialization\n",
      "INFO:holodecml.torch.models:Loading model-type multi-head-dense with settings\n",
      "INFO:holodecml.torch.models:tasks: ['x', 'y', 'z', 'd']\n",
      "INFO:holodecml.torch.models:hidden_dims: [6308, 492, 492, 492]\n",
      "INFO:holodecml.torch.models:dropouts: [0.5, 0.3, 0.3, 0.3]\n",
      "INFO:holodecml.torch.models:weights: False\n",
      "INFO:holodecml.torch.models.dense:The model contains 103755508 trainable parameters\n",
      "INFO:holodecml.torch.models.dense:Setting tunable parameter weights according to Xavier's uniform initialization\n"
     ]
    }
   ],
   "source": [
    "trainer = LoadTrainer(\n",
    "    train_gen, \n",
    "    valid_gen, \n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    device, \n",
    "    conf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metrics and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aimlutils.torch.checkpoint.checkpointer:Loaded EarlyStopping checkpointer with patience 5\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Loaded EarlyStopping checkpointer with patience 3\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Loaded a metrics logger /glade/work/schreck/repos/holodec-ml/scripts/schreck/decoder/results/test/training_log.csv to track the training results\n"
     ]
    }
   ],
   "source": [
    "# Initialize LR annealing scheduler \n",
    "if \"ReduceLROnPlateau\" in conf[\"callbacks\"]:\n",
    "    if \"decoder\" in conf[\"callbacks\"][\"ReduceLROnPlateau\"]:\n",
    "        schedule_config1 = conf[\"callbacks\"][\"ReduceLROnPlateau\"][\"decoder\"]\n",
    "        scheduler_rnn = ReduceLROnPlateau(trainer.rnn_optimizer, **schedule_config1)\n",
    "    if \"regressor\" in conf[\"callbacks\"][\"ReduceLROnPlateau\"]:\n",
    "        schedule_config2 = conf[\"callbacks\"][\"ReduceLROnPlateau\"][\"regressor\"]\n",
    "        scheduler_linear = ReduceLROnPlateau(trainer.particle_optimizer, **schedule_config2)\n",
    "\n",
    "if \"ExponentialLR\" in conf[\"callbacks\"]:\n",
    "    if \"decoder\" in conf[\"callbacks\"][\"ExponentialLR\"]:\n",
    "        schedule_config1 = conf[\"callbacks\"][\"ExponentialLR\"][\"decoder\"]\n",
    "        scheduler_rnn = ExponentialLR(trainer.rnn_optimizer, **schedule_config1)\n",
    "    if \"regressor\" in conf[\"callbacks\"][\"ExponentialLR\"]:\n",
    "        schedule_config2 = conf[\"callbacks\"][\"ExponentialLR\"][\"regressor\"]\n",
    "        scheduler_linear = ExponentialLR(trainer.particle_optimizer, **schedule_config2)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_rnn = EarlyStopping(**conf[\"callbacks\"][\"EarlyStopping\"][\"decoder\"]) \n",
    "early_stopping_linear = EarlyStopping(**conf[\"callbacks\"][\"EarlyStopping\"][\"regressor\"])\n",
    "\n",
    "# Write metrics to csv each epoch\n",
    "metrics_logger = MetricsLogger(**conf[\"callbacks\"][\"MetricsLogger\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_bce: 0.071 train_mse: 0.458 train_acc: 0.439 train_stop_acc: 0.996 train_seq_acc: 0.537: 100%|██████████| 2084/2084 [08:33<00:00,  4.05it/s]\n",
      "Epoch 0 val_bce: 0.052 val_mae: 0.407 val_acc: 0.486 val_stop_acc: 1.000 val_seq_acc: 0.692: 100%|██████████| 313/313 [01:51<00:00,  2.80it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 0 (inf --> 0.307941).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 0 (inf --> 0.406501).  Saving model.\n",
      "Epoch 1 train_bce: 0.050 train_mse: 0.292 train_acc: 0.587 train_stop_acc: 0.999 train_seq_acc: 0.718: 100%|██████████| 2084/2084 [08:33<00:00,  4.06it/s]\n",
      "Epoch 1 val_bce: 0.044 val_mae: 0.304 val_acc: 0.516 val_stop_acc: 1.000 val_seq_acc: 0.707: 100%|██████████| 313/313 [01:50<00:00,  2.84it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 1 (0.307941 --> 0.292998).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 1 (0.406501 --> 0.303826).  Saving model.\n",
      "Epoch 2 train_bce: 0.047 train_mse: 0.253 train_acc: 0.607 train_stop_acc: 0.999 train_seq_acc: 0.744: 100%|██████████| 2084/2084 [08:33<00:00,  4.06it/s]\n",
      "Epoch 2 val_bce: 0.039 val_mae: 0.288 val_acc: 0.542 val_stop_acc: 1.000 val_seq_acc: 0.730: 100%|██████████| 313/313 [01:48<00:00,  2.87it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 2 (0.292998 --> 0.270001).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 2 (0.303826 --> 0.287712).  Saving model.\n",
      "Epoch 3 train_bce: 0.044 train_mse: 0.234 train_acc: 0.632 train_stop_acc: 1.000 train_seq_acc: 0.772: 100%|██████████| 2084/2084 [08:30<00:00,  4.08it/s]\n",
      "Epoch 3 val_bce: 0.036 val_mae: 0.285 val_acc: 0.548 val_stop_acc: 1.000 val_seq_acc: 0.734: 100%|██████████| 313/313 [01:48<00:00,  2.87it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 3 (0.270001 --> 0.265608).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 3 (0.287712 --> 0.285294).  Saving model.\n",
      "Epoch 4 train_bce: 0.040 train_mse: 0.221 train_acc: 0.655 train_stop_acc: 1.000 train_seq_acc: 0.795: 100%|██████████| 2084/2084 [08:29<00:00,  4.09it/s]\n",
      "Epoch 4 val_bce: 0.034 val_mae: 0.251 val_acc: 0.580 val_stop_acc: 1.000 val_seq_acc: 0.760: 100%|██████████| 313/313 [01:48<00:00,  2.88it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 4 (0.265608 --> 0.240349).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 4 (0.285294 --> 0.250531).  Saving model.\n",
      "Epoch 5 train_bce: 0.037 train_mse: 0.211 train_acc: 0.685 train_stop_acc: 1.000 train_seq_acc: 0.819: 100%|██████████| 2084/2084 [08:32<00:00,  4.07it/s]\n",
      "Epoch 5 val_bce: 0.032 val_mae: 0.235 val_acc: 0.595 val_stop_acc: 1.000 val_seq_acc: 0.769: 100%|██████████| 313/313 [01:49<00:00,  2.85it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 5 (0.240349 --> 0.231230).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 5 (0.250531 --> 0.234775).  Saving model.\n",
      "Epoch 6 train_bce: 0.033 train_mse: 0.204 train_acc: 0.712 train_stop_acc: 1.000 train_seq_acc: 0.839: 100%|██████████| 2084/2084 [08:34<00:00,  4.05it/s]\n",
      "Epoch 6 val_bce: 0.031 val_mae: 0.231 val_acc: 0.602 val_stop_acc: 1.000 val_seq_acc: 0.772: 100%|██████████| 313/313 [01:49<00:00,  2.86it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 6 (0.231230 --> 0.228268).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 6 (0.234775 --> 0.230963).  Saving model.\n",
      "Epoch 7 train_bce: 0.030 train_mse: 0.197 train_acc: 0.738 train_stop_acc: 1.000 train_seq_acc: 0.856: 100%|██████████| 2084/2084 [08:45<00:00,  3.96it/s]\n",
      "Epoch 7 val_bce: 0.030 val_mae: 0.236 val_acc: 0.606 val_stop_acc: 1.000 val_seq_acc: 0.771: 100%|██████████| 313/313 [01:50<00:00,  2.83it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 1 out of 5\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 1 out of 3\n",
      "Epoch 8 train_bce: 0.028 train_mse: 0.192 train_acc: 0.763 train_stop_acc: 1.000 train_seq_acc: 0.870: 100%|██████████| 2084/2084 [08:41<00:00,  4.00it/s]\n",
      "Epoch 8 val_bce: 0.029 val_mae: 0.218 val_acc: 0.615 val_stop_acc: 1.000 val_seq_acc: 0.779: 100%|██████████| 313/313 [01:50<00:00,  2.84it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 8 (0.228268 --> 0.220647).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 8 (0.230963 --> 0.217923).  Saving model.\n",
      "Epoch 9 train_bce: 0.025 train_mse: 0.188 train_acc: 0.786 train_stop_acc: 1.000 train_seq_acc: 0.884: 100%|██████████| 2084/2084 [08:42<00:00,  3.99it/s]\n",
      "Epoch 9 val_bce: 0.029 val_mae: 0.219 val_acc: 0.620 val_stop_acc: 1.000 val_seq_acc: 0.784: 100%|██████████| 313/313 [01:50<00:00,  2.83it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 9 (0.220647 --> 0.216454).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 1 out of 3\n",
      "Epoch 10 train_bce: 0.023 train_mse: 0.184 train_acc: 0.806 train_stop_acc: 1.000 train_seq_acc: 0.896: 100%|██████████| 2084/2084 [08:40<00:00,  4.00it/s]\n",
      "Epoch 10 val_bce: 0.029 val_mae: 0.214 val_acc: 0.623 val_stop_acc: 1.000 val_seq_acc: 0.783: 100%|██████████| 313/313 [01:51<00:00,  2.81it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 1 out of 5\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 10 (0.217923 --> 0.213940).  Saving model.\n",
      "Epoch 11 train_bce: 0.021 train_mse: 0.180 train_acc: 0.825 train_stop_acc: 1.000 train_seq_acc: 0.906: 100%|██████████| 2084/2084 [08:37<00:00,  4.02it/s]\n",
      "Epoch 11 val_bce: 0.029 val_mae: 0.203 val_acc: 0.629 val_stop_acc: 1.000 val_seq_acc: 0.788: 100%|██████████| 313/313 [01:49<00:00,  2.86it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 11 (0.216454 --> 0.211961).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 11 (0.213940 --> 0.203470).  Saving model.\n",
      "Epoch 12 train_bce: 0.019 train_mse: 0.179 train_acc: 0.840 train_stop_acc: 1.000 train_seq_acc: 0.914: 100%|██████████| 2084/2084 [08:31<00:00,  4.07it/s]\n",
      "Epoch 12 val_bce: 0.029 val_mae: 0.215 val_acc: 0.623 val_stop_acc: 1.000 val_seq_acc: 0.780: 100%|██████████| 313/313 [01:49<00:00,  2.85it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 1 out of 5\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 1 out of 3\n",
      "Epoch 13 train_bce: 0.018 train_mse: 0.174 train_acc: 0.856 train_stop_acc: 1.000 train_seq_acc: 0.922: 100%|██████████| 2084/2084 [08:33<00:00,  4.06it/s]\n",
      "Epoch 13 val_bce: 0.029 val_mae: 0.205 val_acc: 0.632 val_stop_acc: 1.000 val_seq_acc: 0.789: 100%|██████████| 313/313 [01:48<00:00,  2.88it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 13 (0.211961 --> 0.210796).  Saving model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    14: reducing learning rate of group 0 to 2.4009e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 2 out of 3\n",
      "Epoch 14 train_bce: 0.016 train_mse: 0.159 train_acc: 0.869 train_stop_acc: 1.000 train_seq_acc: 0.929: 100%|██████████| 2084/2084 [08:31<00:00,  4.08it/s]\n",
      "Epoch 14 val_bce: 0.030 val_mae: 0.184 val_acc: 0.633 val_stop_acc: 1.000 val_seq_acc: 0.793: 100%|██████████| 313/313 [01:49<00:00,  2.87it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 14 (0.210796 --> 0.206836).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 14 (0.203470 --> 0.183844).  Saving model.\n",
      "Epoch 15 train_bce: 0.015 train_mse: 0.152 train_acc: 0.878 train_stop_acc: 1.000 train_seq_acc: 0.933: 100%|██████████| 2084/2084 [08:33<00:00,  4.06it/s]\n",
      "Epoch 15 val_bce: 0.030 val_mae: 0.185 val_acc: 0.634 val_stop_acc: 1.000 val_seq_acc: 0.793: 100%|██████████| 313/313 [01:48<00:00,  2.89it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 15 (0.206836 --> 0.206802).  Saving model.\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 1 out of 3\n",
      "Epoch 16 train_bce: 0.014 train_mse: 0.149 train_acc: 0.888 train_stop_acc: 1.000 train_seq_acc: 0.938: 100%|██████████| 2084/2084 [08:36<00:00,  4.03it/s]\n",
      "Epoch 16 val_bce: 0.030 val_mae: 0.187 val_acc: 0.636 val_stop_acc: 1.000 val_seq_acc: 0.792: 100%|██████████| 313/313 [01:49<00:00,  2.86it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 1 out of 5\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    17: reducing learning rate of group 0 to 4.8017e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 train_bce: 0.013 train_mse: 0.146 train_acc: 0.898 train_stop_acc: 1.000 train_seq_acc: 0.943: 100%|██████████| 2084/2084 [08:31<00:00,  4.07it/s]\n",
      "Epoch 17 val_bce: 0.030 val_mae: 0.183 val_acc: 0.636 val_stop_acc: 1.000 val_seq_acc: 0.793: 100%|██████████| 313/313 [01:48<00:00,  2.89it/s]\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:EarlyStopping counter: 2 out of 5\n",
      "INFO:aimlutils.torch.checkpoint.checkpointer:Validation loss decreased on epoch 17 (0.183844 --> 0.183050).  Saving model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    18: reducing learning rate of group 0 to 1.1578e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 train_bce: 0.010 train_mse: 0.146 train_acc: 0.918 train_stop_acc: 1.000 train_seq_acc: 0.950:   9%|▉         | 186/2084 [00:46<07:58,  3.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-deaa268f8aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/holodecml/torch/trainers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, scheduler_rnn, scheduler_linear, early_stopping_rnn, early_stopping_linear, metrics_logger)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;31m# Write results to the callback logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m             result = {\n\u001b[0m\u001b[1;32m   1139\u001b[0m                 \u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0;34m\"train_bce\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bce\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/holodecml/torch/trainers.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, epoch, use_teacher_forcing)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 combined_att = torch.cat([\n\u001b[1;32m    767\u001b[0m                     \u001b[0mencoder_att\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                     \u001b[0mdecoder_att\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m                 ], 1)\n\u001b[1;32m    770\u001b[0m                 \u001b[0mcombined_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_att\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/holodecml/torch/models/cnn.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_block5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m#z, att_map5 = self.decoder_atten5(z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_block6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0matt_map1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_map2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_map3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/holodecml/torch/spectral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_u_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/holodecml/torch/spectral.py\u001b[0m in \u001b[0;36m_update_u_v\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = trainer.train(scheduler_rnn, scheduler_linear, early_stopping_rnn, early_stopping_linear, metrics_logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
