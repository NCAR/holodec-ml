{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.disable_eager_execution()\n",
    "\n",
    "import sys \n",
    "import yaml\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy.fft as FFT\n",
    "from typing import List, Dict\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from tensorflow.keras.layers import (Input, Conv2D, Dense, Flatten, \n",
    "                                     MaxPool2D, RepeatVector, Lambda,\n",
    "                                     LeakyReLU, Dropout, add, Activation, AveragePooling2D)\n",
    "from tensorflow.keras.models import Model, save_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from keras_radam import RAdam\n",
    "from keras_radam.training import RAdamOptimizer\n",
    "\n",
    "from holodecml.library.losses import SymmetricCrossEntropy\n",
    "from holodecml.library.callbacks import get_callbacks\n",
    "from holodecml.library.FourierOpticsLib import OpticsFFT, OpticsIFFT\n",
    "\n",
    "from multiprocessing import cpu_count, Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file) as config_file:\n",
    "    config = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(config[\"path_save\"])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "# Stream output to stdout\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "root.addHandler(ch)\n",
    "#########\n",
    "\n",
    "# Save the log file\n",
    "fp = os.path.join(f'{config[\"path_save\"]}/log.txt')\n",
    "fh = logging.FileHandler(fp,\n",
    "                         mode='w',\n",
    "                         encoding='utf-8')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "root.addHandler(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up some globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = config[\"path_data\"]\n",
    "num_particles = config[\"num_particles\"]\n",
    "split = 'train'\n",
    "subset = False\n",
    "output_cols = [\"x\", \"y\", \"z\", \"d\", \"hid\"]\n",
    "\n",
    "batch_size = config[\"conv2d_network\"][\"batch_size\"]\n",
    "\n",
    "input_shape = (600, 400, 1)\n",
    "\n",
    "n_particles = config[\"num_particles\"]\n",
    "output_channels = len(output_cols) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles_dict = {\n",
    "    1: '1particle',\n",
    "    3: '3particle',\n",
    "    'multi': 'multiparticle',\n",
    "    '50-100': '50-100'}\n",
    "\n",
    "split_dict = {\n",
    "    'train' : 'training',\n",
    "    'test'   : 'test',\n",
    "    'valid': 'validation'}\n",
    "\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(\n",
    "        \n",
    "        self, \n",
    "        path_data: str, \n",
    "        num_particles: int, \n",
    "        split: str, \n",
    "        subset: bool, \n",
    "        output_cols: List[str], \n",
    "        batch_size: int, \n",
    "        shuffle: bool = True,\n",
    "        maxnum_particles: int = False,\n",
    "        scaler: Dict[str, str] = False) -> None:\n",
    "        \n",
    "        'Initialization'\n",
    "        self.ds = self.open_dataset(path_data, num_particles, split)\n",
    "        self.batch_size = batch_size\n",
    "        self.output_cols = [x for x in output_cols if x != 'hid']        \n",
    "        self.subset = subset\n",
    "        self.hologram_numbers = self.ds.hologram_number.values\n",
    "        if shuffle:\n",
    "            random.shuffle(self.hologram_numbers)\n",
    "        self.num_particles = num_particles\n",
    "        self.xsize = len(self.ds.xsize.values)\n",
    "        self.ysize = len(self.ds.ysize.values)\n",
    "        self.shuffle = shuffle\n",
    "        self.maxnum_particles = maxnum_particles\n",
    "                \n",
    "        if not scaler:\n",
    "            self.scaler = {col: StandardScaler() for col in output_cols}\n",
    "            for col in output_cols:\n",
    "                scale = self.ds[col].values\n",
    "                self.scaler[col].fit(scale.reshape(scale.shape[-1], -1))\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "        \n",
    "    def get_transform(self):\n",
    "        return self.scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return math.ceil(len(self.hologram_numbers) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        'Generate one batch of data'\n",
    "        holograms = self.hologram_numbers[\n",
    "            idx * self.batch_size: (idx + 1) * self.batch_size\n",
    "        ]\n",
    "        x_out, y_out, w_out = self._batch(holograms)\n",
    "        return x_out, y_out, w_out\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            random.shuffle(self.hologram_numbers)\n",
    "            \n",
    "    def _batch(self, holograms: List[int]):\n",
    "        'Create a batch of data'\n",
    "        try:\n",
    "        \n",
    "            x_out = np.zeros((\n",
    "                len(holograms), self.xsize, self.ysize\n",
    "            ))\n",
    "            y_out = np.zeros((\n",
    "                len(holograms), \n",
    "                self.maxnum_particles if self.maxnum_particles else self.num_particles, \n",
    "                len(self.output_cols)\n",
    "            ))\n",
    "            # Move the scaler.transform to here\n",
    "            \n",
    "            a = time.time()\n",
    "            for k, hologram in enumerate(holograms):\n",
    "                im = self.ds[\"image\"][hologram].values\n",
    "                x_out[k] = (im-np.mean(im)) / (np.std(im))\n",
    "                #A = np.log(np.abs(OpticsFFT(A)))                    \n",
    "                particles = np.where(self.ds[\"hid\"] == hologram + 1)[0]  \n",
    "                for l, p in enumerate(particles):\n",
    "                    for m, col in enumerate(self.output_cols):\n",
    "                        val = self.ds[col][p].values\n",
    "                        y_out[k, l, m] = self.scaler[col].transform(\n",
    "                            val.reshape(1, -1)\n",
    "                        )\n",
    "                if self.maxnum_particles and len(particles) < self.maxnum_particles:\n",
    "                    for l in range(len(particles), self.maxnum_particles):\n",
    "                        for m, col in enumerate(self.output_cols):\n",
    "                            val = y_out[k, l, m]\n",
    "                            y_out[k, l, m] = self.scaler[col].transform(\n",
    "                                val.reshape(1, -1)\n",
    "                            )\n",
    "            #\n",
    "            # convert y_out to sparse if we are using padding\n",
    "#             if self.maxnum_particles:\n",
    "#                 y_out = sparse_vstack([\n",
    "#                     csr_matrix(y_out[i]) for i in y_out.shape[0]\n",
    "#                 ])\n",
    "            \n",
    "            x_out = np.expand_dims(x_out, axis=-1)\n",
    "            return x_out, y_out, [None] #class weights option\n",
    "        \n",
    "        except:\n",
    "            print(traceback.print_exc())\n",
    "    \n",
    "    def open_dataset(self, path_data, num_particles, split):\n",
    "        \"\"\"\n",
    "        Opens a HOLODEC file\n",
    "\n",
    "        Args: \n",
    "            path_data: (str) Path to dataset directory\n",
    "            num_particles: (int or str) Number of particles per hologram\n",
    "            split: (str) Dataset split of either 'train', 'valid', or 'test'\n",
    "\n",
    "        Returns:\n",
    "            ds: (xarray Dataset) Opened dataset\n",
    "        \"\"\"\n",
    "        path_data = os.path.join(path_data, self.dataset_name(num_particles, split))\n",
    "\n",
    "        if not os.path.isfile(path_data):\n",
    "            print(f\"Data file does not exist at {path_data}. Exiting.\")\n",
    "            raise \n",
    "\n",
    "        ds = xr.open_dataset(path_data)\n",
    "        return ds\n",
    "    \n",
    "    def dataset_name(self, num_particles, split, file_extension='nc'):\n",
    "        \"\"\"\n",
    "        Return the dataset filename given user inputs\n",
    "\n",
    "        Args: \n",
    "            num_particles: (int or str) Number of particles per hologram\n",
    "            split: (str) Dataset split of either 'train', 'valid', or 'test'\n",
    "            file_extension: (str) Dataset file extension\n",
    "\n",
    "        Returns:\n",
    "            ds_name: (str) Dataset name\n",
    "        \"\"\"\n",
    "\n",
    "        valid = [1,3,'multi','50-100']\n",
    "        if num_particles not in valid:\n",
    "            raise ValueError(\"results: num_particles must be one of %r.\" % valid)\n",
    "        num_particles = num_particles_dict[num_particles]\n",
    "\n",
    "        valid = ['train','test','valid']\n",
    "        if split not in valid:\n",
    "            raise ValueError(\"results: split must be one of %r.\" % valid)\n",
    "        split = split_dict[split]\n",
    "        ds_name = f'synthetic_holograms_{num_particles}_{split}.{file_extension}'\n",
    "\n",
    "        return ds_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(\n",
    "    path_data, num_particles, \"train\", subset, \n",
    "    output_cols, batch_size, maxnum_particles = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = {col: StandardScaler() for col in train_gen.output_cols}\n",
    "# for col in train_gen.output_cols:\n",
    "#     scale = train_gen.ds[col].values\n",
    "#     scaler[col].fit(scale.reshape(scale.shape[-1], -1))\n",
    "#     result = scaler[col].transform(scale.reshape(scale.shape[-1], -1))\n",
    "#     print(col, min(result), max(result), np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scalers = train_gen.get_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = DataGenerator(\n",
    "    path_data, num_particles, \"test\", subset, \n",
    "    output_cols, batch_size, scaler = train_scalers, maxnum_particles = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def R2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return SS_res/(SS_tot + K.epsilon())\n",
    "\n",
    "def keras_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wsme(y_true, y_pred):\n",
    "#     cond1 = K.equal(y_true, 0.0)\n",
    "#     zero = K.switch(cond1,K.square(y_pred - y_true), \n",
    "#                              K.zeros_like(y_true))\n",
    "#     cond2 = K.greater(y_true, 0.0)\n",
    "#     real = K.switch(cond2, \n",
    "#                              K.square(y_pred - y_true), \n",
    "#                              K.zeros_like(y_true))\n",
    "#     w1 = K.sum(K.cast(cond1, \"float\"))\n",
    "#     w2 = K.sum(K.cast(cond2, \"float\"))\n",
    "#     total = w1 + w2\n",
    "#     zero = K.sum(zero) / w1\n",
    "#     real = K.sum(real) / w2\n",
    "#     return (real + zero)\n",
    "\n",
    "def wsme(y_true, y_pred):\n",
    "    \n",
    "    w = K.abs(K.mean(y_true[1]))\n",
    "    w = w / (1 - w)\n",
    "    \n",
    "    # w = K.sum(K.cast(K.greater(y_true[1], 0), \"float\")) # Number actually not zero\n",
    "    \n",
    "    error = K.square(y_true - y_pred)\n",
    "    error = K.switch(K.equal(y_true, 0), w * error, error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 3\n",
    "\n",
    "custom_losses = {\n",
    "    \"sce\": SymmetricCrossEntropy(0.5, 0.5),\n",
    "    \"weighted_mse\": wsme,\n",
    "    \"r2\": R2,\n",
    "    \"rmse\": rmse\n",
    "}\n",
    "\n",
    "class Conv2DNeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    A Conv2D Neural Network Model that can support an arbitrary numbers of\n",
    "    layers.\n",
    "\n",
    "    Attributes:\n",
    "        filters: List of number of filters in each Conv2D layer\n",
    "        kernel_sizes: List of kernel sizes in each Conv2D layer\n",
    "        conv2d_activation: Type of activation function for conv2d layers\n",
    "        pool_sizes: List of Max Pool sizes\n",
    "        dense_sizes: Sizes of dense layers\n",
    "        dense_activation: Type of activation function for dense layers\n",
    "        output_activation: Type of activation function for output layer\n",
    "        lr: Optimizer learning rate\n",
    "        optimizer: Name of optimizer or optimizer object.\n",
    "        adam_beta_1: Exponential decay rate for the first moment estimates\n",
    "        adam_beta_2: Exponential decay rate for the first moment estimates\n",
    "        sgd_momentum: Stochastic Gradient Descent momentum\n",
    "        decay: Optimizer decay\n",
    "        loss: Name of loss function or loss object\n",
    "        batch_size: Number of examples per batch\n",
    "        epochs: Number of epochs to train\n",
    "        verbose: Level of detail to provide during training\n",
    "        model: Keras Model object\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        filters=(8,), \n",
    "        kernel_sizes=(5,),\n",
    "        conv2d_activation=\"relu\", \n",
    "        pool_sizes=(4,), \n",
    "        pool_dropout=0.0,\n",
    "        dense_sizes=(64,),\n",
    "        dense_activation=\"relu\", \n",
    "        dense_dropout = 0.0,\n",
    "        output_activation=\"linear\",\n",
    "        lr=0.001, \n",
    "        optimizer=\"adam\", \n",
    "        adam_beta_1=0.9,\n",
    "        adam_beta_2=0.999, \n",
    "        sgd_momentum=0.9, \n",
    "        decay=0, \n",
    "        loss=\"mse\",\n",
    "        metrics = [], \n",
    "        batch_size=32, \n",
    "        epochs=2, \n",
    "        verbose=0\n",
    "    ):\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_sizes = [tuple((v,v)) for v in kernel_sizes]\n",
    "        self.conv2d_activation = conv2d_activation\n",
    "        self.pool_sizes = [tuple((v,v)) for v in pool_sizes]\n",
    "        self.pool_dropout = pool_dropout\n",
    "        self.dense_sizes = dense_sizes\n",
    "        self.dense_activation = dense_activation\n",
    "        self.dense_dropout = dense_dropout\n",
    "        self.output_activation = output_activation\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_obj = None\n",
    "        self.adam_beta_1 = adam_beta_1\n",
    "        self.adam_beta_2 = adam_beta_2\n",
    "        self.sgd_momentum = sgd_momentum\n",
    "        self.decay = decay\n",
    "        self.loss = custom_losses[loss] if loss in custom_losses else loss\n",
    "        self.metrics = []\n",
    "        for m in metrics:\n",
    "            if m in custom_losses:\n",
    "                self.metrics.append(custom_losses[m])\n",
    "            else:\n",
    "                self.metrics.append(m)\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        \n",
    "        if self.conv2d_activation == \"leakyrelu\":\n",
    "            self.conv2d_activation = LeakyReLU(alpha=0.1)\n",
    "        if self.dense_activation == \"leakyrelu\":\n",
    "            self.dense_activation = LeakyReLU(alpha=0.1)\n",
    "        if self.output_activation == \"leakyrelu\":\n",
    "            self.output_activation = LeakyReLU(alpha=0.1)\n",
    "\n",
    "    def build_neural_network(self, input_shape, n_particles, output_shape):\n",
    "        \"\"\"Create Keras neural network model and compile it.\"\"\"\n",
    "        \n",
    "        # function for creating a vgg block\n",
    "        def vgg_block(layer_in, n_filters, n_conv):\n",
    "            # add convolutional layers\n",
    "            for _ in range(n_conv):\n",
    "                layer_in = Conv2D(n_filters, (3,3), \n",
    "                                  padding='same', activation='relu')(layer_in)\n",
    "            # add max pooling layer\n",
    "            layer_in = MaxPool2D((2,2), strides=(2,2))(layer_in)\n",
    "            return layer_in\n",
    "        \n",
    "        # Input\n",
    "        conv_input = Input(shape=(input_shape), name=\"input\")\n",
    "        \n",
    "        # vgg blocks\n",
    "        nn_model = vgg_block(conv_input, 32, 2)\n",
    "        nn_model = vgg_block(nn_model, 64, 2)\n",
    "        nn_model = vgg_block(nn_model, 128, 4)\n",
    "        nn_model = vgg_block(nn_model, 256, 4)\n",
    "        nn_model = Flatten()(nn_model)\n",
    "        \n",
    "        # Classifier\n",
    "        for h in range(len(self.dense_sizes)):\n",
    "            nn_model = Dense(self.dense_sizes[h],\n",
    "                             activation=self.dense_activation,\n",
    "                             kernel_initializer='he_uniform',\n",
    "                             name=f\"dense_{h:02d}\")(nn_model)\n",
    "            if self.dense_dropout > 0.0:\n",
    "                nn_model = Dropout(self.dense_dropout, \n",
    "                                   name=f\"dense_dr_{h:02d}\")(nn_model)\n",
    "        \n",
    "        nn_model = RepeatVector(n_particles, name = \"repeat\")(nn_model)\n",
    "        nn_model = Dense(output_shape,\n",
    "                         activation=self.output_activation,\n",
    "                         name=f\"dense_output\")(nn_model)\n",
    "        nn_model = Lambda(\n",
    "            self.LastLayer,\n",
    "            input_shape = (n_particles, output_shape),\n",
    "            name=\"coordinate\"\n",
    "        )(nn_model)\n",
    "        \n",
    "        self.model = Model(\n",
    "            inputs = conv_input, \n",
    "            outputs = nn_model\n",
    "        )\n",
    "        \n",
    "        if self.optimizer == \"adam\":\n",
    "            self.optimizer_obj = Adam(lr=self.lr, clipnorm = 1.0)\n",
    "        elif self.optimizer == \"sgd\":\n",
    "            self.optimizer_obj = SGD(lr=self.lr, momentum=self.sgd_momentum,\n",
    "                                     decay=self.decay)\n",
    "            \n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer_obj, \n",
    "            loss=self.loss,\n",
    "            metrics=self.metrics\n",
    "        )\n",
    "        self.model.summary()\n",
    "\n",
    "    def fit(self, x, y, xv=None, yv=None, callbacks=None):\n",
    "        \n",
    "        if len(x.shape[1:])==2:\n",
    "            x = np.expand_dims(x, axis=-1)\n",
    "        if len(y.shape) == 1:\n",
    "            output_shape = 1\n",
    "        else:\n",
    "            output_shape = y.shape[1]\n",
    "        \n",
    "        input_shape = x.shape[1:]\n",
    "        self.build_neural_network(input_shape, output_shape)\n",
    "        self.model.fit(x, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose, validation_data=(xv, yv), callbacks=callbacks)\n",
    "        return self.model.history.history\n",
    "    \n",
    "    def LastLayer(self, x):\n",
    "        return 1.75 * K.tanh(x / 100) \n",
    "\n",
    "    def predict(self, x):\n",
    "        y_out = self.model.predict(np.expand_dims(x, axis=-1),\n",
    "                                   batch_size=self.batch_size)\n",
    "        return y_out\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        y_prob = self.model.predict(x, batch_size=self.batch_size)\n",
    "        return y_prob\n",
    "    \n",
    "    def load_weights(self, weights):\n",
    "        try:\n",
    "            self.model.load_weights(weights)\n",
    "            self.model.compile(\n",
    "                optimizer=self.optimizer, \n",
    "                loss=self.loss, \n",
    "                metrics=self.metrics\n",
    "            )\n",
    "        except:\n",
    "            print(\"You must first call build_neural_network before loading weights. Exiting.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "class Conv2DNeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    A Conv2D Neural Network Model that can support an arbitrary numbers of\n",
    "    layers.\n",
    "\n",
    "    Attributes:\n",
    "        filters: List of number of filters in each Conv2D layer\n",
    "        kernel_sizes: List of kernel sizes in each Conv2D layer\n",
    "        conv2d_activation: Type of activation function for conv2d layers\n",
    "        pool_sizes: List of Max Pool sizes\n",
    "        dense_sizes: Sizes of dense layers\n",
    "        dense_activation: Type of activation function for dense layers\n",
    "        output_activation: Type of activation function for output layer\n",
    "        lr: Optimizer learning rate\n",
    "        optimizer: Name of optimizer or optimizer object.\n",
    "        adam_beta_1: Exponential decay rate for the first moment estimates\n",
    "        adam_beta_2: Exponential decay rate for the first moment estimates\n",
    "        sgd_momentum: Stochastic Gradient Descent momentum\n",
    "        decay: Optimizer decay\n",
    "        loss: Name of loss function or loss object\n",
    "        batch_size: Number of examples per batch\n",
    "        epochs: Number of epochs to train\n",
    "        verbose: Level of detail to provide during training\n",
    "        model: Keras Model object\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        filters=(8,), \n",
    "        kernel_sizes=(5,),\n",
    "        conv2d_activation=\"relu\", \n",
    "        pool_sizes=(4,), \n",
    "        pool_dropout=0.0,\n",
    "        dense_sizes=(64,),\n",
    "        dense_activation=\"relu\", \n",
    "        dense_dropout = 0.0,\n",
    "        output_activation=\"linear\",\n",
    "        lr=0.001, \n",
    "        optimizer=\"adam\", \n",
    "        adam_beta_1=0.9,\n",
    "        adam_beta_2=0.999, \n",
    "        sgd_momentum=0.9, \n",
    "        decay=0, \n",
    "        loss=\"mse\",\n",
    "        metrics = [], \n",
    "        batch_size=32, \n",
    "        epochs=2, \n",
    "        verbose=0\n",
    "    ):\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_sizes = [tuple((v,v)) for v in kernel_sizes]\n",
    "        self.conv2d_activation = conv2d_activation\n",
    "        self.pool_sizes = [tuple((v,v)) for v in pool_sizes]\n",
    "        self.pool_dropout = pool_dropout\n",
    "        self.dense_sizes = dense_sizes\n",
    "        self.dense_activation = dense_activation\n",
    "        self.dense_dropout = dense_dropout\n",
    "        self.output_activation = output_activation\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_obj = None\n",
    "        self.adam_beta_1 = adam_beta_1\n",
    "        self.adam_beta_2 = adam_beta_2\n",
    "        self.sgd_momentum = sgd_momentum\n",
    "        self.decay = decay\n",
    "        self.loss = custom_losses[loss] if loss in custom_losses else loss\n",
    "        self.metrics = []\n",
    "        for m in metrics:\n",
    "            if m in custom_losses:\n",
    "                self.metrics.append(custom_losses[m])\n",
    "            else:\n",
    "                self.metrics.append(m)\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        \n",
    "        if self.conv2d_activation == \"leakyrelu\":\n",
    "            self.conv2d_activation = LeakyReLU(alpha=0.1)\n",
    "        if self.dense_activation == \"leakyrelu\":\n",
    "            self.dense_activation = LeakyReLU(alpha=0.1)\n",
    "        if self.output_activation == \"leakyrelu\":\n",
    "            self.output_activation = LeakyReLU(alpha=0.1)\n",
    "\n",
    "    def build_neural_network(self, input_shape, n_particles, output_shape):\n",
    "        \"\"\"Create Keras neural network model and compile it.\"\"\"\n",
    "        \n",
    "        def vgg_block(layer_in, n_filters, n_conv):\n",
    "            for _ in range(n_conv):\n",
    "                layer_in = Conv2D(\n",
    "                    n_filters,\n",
    "                    (3,3),\n",
    "                    padding='same',\n",
    "                    activation='relu'\n",
    "                )(layer_in)\n",
    "            layer_in = MaxPool2D(\n",
    "                (2,2),\n",
    "                strides=(2,2)\n",
    "            )(layer_in)\n",
    "            return layer_in\n",
    "        \n",
    "        def residual_module(layer_in, n_filters):\n",
    "            merge_input = layer_in\n",
    "            # check if the number of filters needs to be increase, assumes channels last format\n",
    "            if layer_in.shape[-1] != n_filters:\n",
    "                merge_input = Conv2D(\n",
    "                    n_filters, \n",
    "                    (1,1), \n",
    "                    padding='same', \n",
    "                    activation='relu', \n",
    "                    kernel_initializer='he_normal'\n",
    "                )(layer_in)\n",
    "            # conv1\n",
    "            conv1 = Conv2D(\n",
    "                n_filters,\n",
    "                (3,3),\n",
    "                padding='same',\n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal'\n",
    "            )(layer_in)\n",
    "            # conv2\n",
    "            conv2 = Conv2D(\n",
    "                n_filters,\n",
    "                (3,3),\n",
    "                padding='same',\n",
    "                activation='linear',\n",
    "                kernel_initializer='he_normal'\n",
    "            )(conv1)\n",
    "            # add filters, assumes filters/channels last\n",
    "            layer_out = add([conv2, merge_input])\n",
    "            # activation function\n",
    "            layer_out = Activation('relu')(layer_out)\n",
    "            return layer_out\n",
    " \n",
    "        # Input\n",
    "        conv_input = Input(shape=(input_shape), name=\"input\")\n",
    "            \n",
    "        nn_model = vgg_block(conv_input, 32, 2)\n",
    "        nn_model = vgg_block(nn_model, 64, 2)\n",
    "        nn_model = vgg_block(nn_model, 128, 4)\n",
    "        nn_model = vgg_block(nn_model, 256, 4)  \n",
    "        \n",
    "#         nn_model = residual_module(conv_input, 32)\n",
    "#         nn_model = AveragePooling2D(pool_size=(2, 2), padding='same')(nn_model)\n",
    "#         nn_model = residual_module(nn_model, 32) \n",
    "#         nn_model = AveragePooling2D(pool_size=(2, 2), padding='same')(nn_model)\n",
    "#         nn_model = residual_module(nn_model, 32) \n",
    "#         nn_model = AveragePooling2D(pool_size=(2, 2), padding='same')(nn_model)\n",
    "#         nn_model = Flatten()(nn_model)\n",
    "        \n",
    "        # Classifier\n",
    "        for h in range(len(self.dense_sizes)):\n",
    "            nn_model = Dense(self.dense_sizes[h],\n",
    "                             activation=self.dense_activation,\n",
    "                             kernel_initializer='he_uniform',\n",
    "                             name=f\"dense_{h:02d}\")(nn_model)\n",
    "            if self.dense_dropout > 0.0:\n",
    "                nn_model = Dropout(self.dense_dropout, \n",
    "                                   name=f\"dense_dr_{h:02d}\")(nn_model)\n",
    "        \n",
    "        # Output\n",
    "        nn_model = RepeatVector(n_particles, name = \"repeat\")(nn_model)\n",
    "        nn_model = Dense(output_shape,\n",
    "                         activation=self.output_activation,\n",
    "                         name=f\"dense_output\")(nn_model)\n",
    "        nn_model = Lambda(\n",
    "            self.LastLayer,\n",
    "            input_shape = (n_particles, output_shape)\n",
    "        )(nn_model)\n",
    "        \n",
    "        self.model = Model(conv_input, nn_model)\n",
    "        \n",
    "        if self.optimizer == \"adam\":\n",
    "            self.optimizer_obj = Adam(lr=self.lr, clipnorm = 1.0)\n",
    "        elif self.optimizer == \"sgd\":\n",
    "            self.optimizer_obj = SGD(lr=self.lr, momentum=self.sgd_momentum,\n",
    "                                     decay=self.decay)\n",
    "            \n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer_obj, \n",
    "            loss=self.loss,\n",
    "            metrics=self.metrics\n",
    "        )\n",
    "        #self.model.summary()\n",
    "\n",
    "    def fit(self, x, y, xv=None, yv=None, callbacks=None):\n",
    "        \n",
    "        if len(x.shape[1:])==2:\n",
    "            x = np.expand_dims(x, axis=-1)\n",
    "        if len(y.shape) == 1:\n",
    "            output_shape = 1\n",
    "        else:\n",
    "            output_shape = y.shape[1]\n",
    "        \n",
    "        input_shape = x.shape[1:]\n",
    "        self.build_neural_network(input_shape, output_shape)\n",
    "        self.model.fit(x, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose, validation_data=(xv, yv), callbacks=callbacks)\n",
    "        return self.model.history.history\n",
    "    \n",
    "    def LastLayer(self, x):\n",
    "        return 1.75 * K.tanh(x / 100) \n",
    "\n",
    "    def predict(self, x):\n",
    "        y_out = self.model.predict(np.expand_dims(x, axis=-1),\n",
    "                                   batch_size=self.batch_size)\n",
    "        return y_out\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        y_prob = self.model.predict(x, batch_size=self.batch_size)\n",
    "        return y_prob\n",
    "    \n",
    "    def load_weights(self, weights):\n",
    "        try:\n",
    "            self.model.load_weights(weights)\n",
    "            self.model.compile(\n",
    "                optimizer=self.optimizer, \n",
    "                loss=self.loss, \n",
    "                metrics=self.metrics\n",
    "            )\n",
    "        except:\n",
    "            print(\"You must first call build_neural_network before loading weights. Exiting.\")\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.clear_session()\n",
    "#ops.reset_default_graph()\n",
    "mod = Conv2DNeuralNetwork(**config[\"conv2d_network\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.build_neural_network(input_shape, n_particles, output_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 600, 400, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 600, 400, 32) 320         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 600, 400, 32) 9248        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 600, 400, 32) 64          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 600, 400, 32) 0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 600, 400, 32) 0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 300, 200, 32) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 300, 200, 32) 9248        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 300, 200, 32) 9248        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 300, 200, 32) 0           conv2d_4[0][0]                   \n",
      "                                                                 average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300, 200, 32) 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 150, 100, 32) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 150, 100, 32) 9248        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 150, 100, 32) 9248        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 150, 100, 32) 0           conv2d_6[0][0]                   \n",
      "                                                                 average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 150, 100, 32) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 75, 50, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 120000)       0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_00 (Dense)                (None, 64)           7680064     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_dr_00 (Dropout)           (None, 64)           0           dense_00[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_01 (Dense)                (None, 32)           2080        dense_dr_00[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_dr_01 (Dropout)           (None, 32)           0           dense_01[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat (RepeatVector)           (None, 3, 32)        0           dense_dr_01[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_output (Dense)            (None, 3, 4)         132         repeat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 3, 4)         0           dense_output[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,728,900\n",
      "Trainable params: 7,728,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.8280 - mse: 1.3642Epoch 1/100\n",
      " 78/391 [====>.........................] - ETA: 3:38 - loss: 0.6806 - mse: 1.1231Epoch 1/100\n",
      "391/391 [==============================] - 353s 904ms/step - loss: 0.8279 - mse: 1.3639 - val_loss: 0.6802 - val_mse: 1.1230\n",
      "Epoch 2/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7461 - mse: 1.2292Epoch 1/100\n",
      "391/391 [==============================] - 339s 867ms/step - loss: 0.7460 - mse: 1.2292 - val_loss: 0.6556 - val_mse: 1.0826\n",
      "Epoch 3/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7157 - mse: 1.1793Epoch 1/100\n",
      " 78/391 [====>.........................] - ETA: 3:41 - loss: 0.6489 - mse: 1.0708Epoch 1/100\n",
      "391/391 [==============================] - 330s 844ms/step - loss: 0.7157 - mse: 1.1792 - val_loss: 0.6485 - val_mse: 1.0707\n",
      "Epoch 4/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6999 - mse: 1.1531Epoch 1/100\n",
      "391/391 [==============================] - 333s 853ms/step - loss: 0.6999 - mse: 1.1532 - val_loss: 0.6451 - val_mse: 1.0652\n",
      "Epoch 5/100\n",
      "385/391 [============================>.] - ETA: 4s - loss: 0.6880 - mse: 1.1336"
     ]
    }
   ],
   "source": [
    "mod.model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    validation_data=valid_gen,\n",
    "    epochs=config[\"conv2d_network\"][\"epochs\"],\n",
    "    verbose=True,\n",
    "    callbacks=callbacks,\n",
    "    use_multiprocessing=True,\n",
    "    workers=16,\n",
    "    max_queue_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use sklearn to do Bayesian optimzation of hyperparameters\n",
    "\n",
    "##### (Based on https://medium.com/@crawftv/parameter-hyperparameter-tuning-with-bayesian-optimization-7acf42d348e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import skopt\n",
    "# from skopt import gbrt_minimize, gp_minimize\n",
    "# from skopt.utils import use_named_args\n",
    "# from skopt.space import Real, Categorical, Integer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter1 = Integer(low=1, high=64, name='filter1')\n",
    "# filter2 = Integer(low=1, high=64, name='filter2')\n",
    "# filter3 = Integer(low=1, high=64, name='filter3')\n",
    "# kernel_sizes = Integer(low=1, high=10, name='kernel_sizes')\n",
    "# pool_sizes = Integer(low=1, high=50, name='pool_sizes')\n",
    "# dense_1 = Integer(low=10, high=1000, name='dense_1')\n",
    "# dense_2 = Integer(low=10, high=1000, name='dense_2')\n",
    "# learning_rate = Real(low=1e-5, high=1e-2, prior='log-uniform',\n",
    "#                          name='learning_rate')\n",
    "\n",
    "# # To add \n",
    "# ## kernel size dimensions \n",
    "# ## pool size dimensions \n",
    "# ## dropout for pool layers\n",
    "# ## dropout for dense layers\n",
    "# ## Number of dense layers\n",
    "# ## scaling factor I use in Lambda function\n",
    "\n",
    "# dimensions = [\n",
    "#     filter1,\n",
    "#     filter2,\n",
    "#     filter3,\n",
    "#     kernel_sizes,\n",
    "#     pool_sizes,\n",
    "#     dense_1,\n",
    "#     dense_2,\n",
    "#     learning_rate\n",
    "# ]\n",
    "\n",
    "# default_parameters = [52, 64, 41, 2, 29, 175, 912, 0.01]\n",
    "\n",
    "# #[57, 57, 18, 6, 10, 781, 607, 0.005285282058684279]\n",
    "# # [8, 12, 16, 5, 5, 64, 32, 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @use_named_args(dimensions=dimensions)\n",
    "# def fitness(\n",
    "#     filter1, filter2, filter3,\n",
    "#     kernel_sizes, pool_sizes,\n",
    "#     dense_1, dense_2, learning_rate):\n",
    "    \n",
    "    \n",
    "#     a = time.time()\n",
    "    \n",
    "#     # Update the configuration\n",
    "#     c = config[\"conv2d_network\"]\n",
    "#     c[\"filters\"] = [filter1, filter2, filter3]\n",
    "#     c[\"kernel_sizes\"] = [kernel_sizes, kernel_sizes, kernel_sizes]\n",
    "#     c[\"pool_sizes\"] = [pool_sizes, pool_sizes, pool_sizes]\n",
    "#     c[\"dense_sizes\"] = [dense_1, dense_2]\n",
    "#     c[\"lr\"] = learning_rate\n",
    "#     c[\"epochs\"] = 1\n",
    "    \n",
    "#     save_path = os.path.join(config[\"path_save\"], \"log.txt\")\n",
    "#     with open(save_path, \"a+\") as fid:\n",
    "#         fid.write(\"------------------------------------\\n\")\n",
    "#         fid.write(\"Starting run\\n\")\n",
    "#         fid.write(f\"filters: {filter1,filter2,filter3} kernel_size: {kernel_sizes} pool_size: {pool_sizes} dense1: {dense_1} dense2: {dense_2} lr: {learning_rate}\\n\")\n",
    "\n",
    "#     # Load the model\n",
    "#     model = Conv2DNeuralNetwork(**c)\n",
    "#     model.build_neural_network(input_shape, n_particles, output_channels)\n",
    "    \n",
    "#     # Load callbacks, though we prob. wont need them for 1 epoch optimization\n",
    "#     callbacks = get_callbacks(config)\n",
    "    \n",
    "#     # Train a model\n",
    "#     blackbox = model.model.fit(\n",
    "#         train_gen,\n",
    "#         validation_data=valid_gen,\n",
    "#         epochs=config[\"conv2d_network\"][\"epochs\"],\n",
    "#         verbose=True,\n",
    "#         callbacks=callbacks,\n",
    "#         use_multiprocessing=True,\n",
    "#         workers=24,\n",
    "#         max_queue_size=100\n",
    "#     )\n",
    "    \n",
    "#     # Return the validation accuracy for the last epoch.\n",
    "#     objective = blackbox.history['val_R2'][-1]\n",
    "    \n",
    "#     with open(save_path, \"a+\") as fid:\n",
    "#         fid.write(f\"Final result: {objective}\\n\")\n",
    "#         fid.write(f\"This iteration took {time.time() - a} s\\n\")\n",
    "\n",
    "#     # Delete the Keras model with these hyper-parameters from memory.\n",
    "#     del model\n",
    "    \n",
    "#     # Garbage collection\n",
    "#     gc.collect()\n",
    "    \n",
    "#     # Clear the Keras session, otherwise it will keep adding new\n",
    "#     # models to the same TensorFlow graph each time we create\n",
    "#     # a model with a different set of hyper-parameters.\n",
    "#     K.clear_session()\n",
    "#     #tf.compat.v1.reset_default_graph()\n",
    "#     ops.reset_default_graph()\n",
    "    \n",
    "#     # The optimizer aims for the lowest score\n",
    "#     # For categorical problems, return the negative accuracy\n",
    "#     return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multi-GPU multiprocessing potentially \n",
    "# # https://github.com/scikit-optimize/scikit-optimize/issues/737\n",
    "\n",
    "# gp_result = gp_minimize(\n",
    "#     func=fitness,\n",
    "#     dimensions=dimensions,\n",
    "#     n_calls=50,\n",
    "#     noise=0.01,\n",
    "#     n_jobs=-1,\n",
    "#     kappa = 5,\n",
    "#     x0=default_parameters\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
