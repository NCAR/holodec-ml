{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Activation, MaxPool2D, SeparableConv2D, AveragePooling2D, concatenate,add\n",
    "from tensorflow.keras.models import Model, save_model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = \"/scr/sci/mhayman/holodec/holodec-ml-data/\"\n",
    "\n",
    "\n",
    "# ds_file = \"synthetic_holograms_v02.nc\"  # 3 particle data (raw data)\n",
    "ds_file = \"synthetic_holograms_v02_ft_multipart_real_imag_amplitude_float.nc\" # 3 particle data\n",
    "\n",
    "ds = xr.open_dataset(ds_path+ds_file)  # file with mean (DC) value removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = 7000  # number of training+validation points\n",
    "valid_index = 2000  # number of validation points\n",
    "all_labels = ds[[\"d\",\"z\",\"x\",\"y\"]].to_dataframe()\n",
    "train_labels = all_labels.iloc[valid_index:split_index]\n",
    "test_labels = all_labels.iloc[split_index:]\n",
    "val_labels = all_labels.iloc[:valid_index]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train_labels = pd.DataFrame(scaler.fit_transform(train_labels), index=train_labels.index, columns=train_labels.columns)\n",
    "scaled_val_labels = pd.DataFrame(scaler.fit_transform(val_labels), index=val_labels.index, columns=val_labels.columns)\n",
    "scaled_test_labels = pd.DataFrame(scaler.transform(test_labels), index=test_labels.index, columns=test_labels.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipart = np.nonzero(ds[\"hid\"].values==1)[0]\n",
    "ds[[\"d\",\"z\",\"x\",\"y\"]].isel(particle=ipart).to_array()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if channel_name is None:\n",
    "    in_data = ds[\"image_ft\"].transpose(\"hologram_number\", \"ysize\", 'xsize').expand_dims(\"channel\", 3)\n",
    "else:\n",
    "    in_data = ds[\"image_ft\"].transpose(\"hologram_number\", \"ysize\", \"xsize\",channel_name)\n",
    "\n",
    "\n",
    "in_image = ds[\"image\"].transpose(\"hologram_number\", \"ysize\", 'xsize').expand_dims(\"channel\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_in_data = in_data / data_rescale\n",
    "scaled_in_image = in_image / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_input1 = Input(shape=scaled_in_data[...,0:1].shape[1:])\n",
    "filter_input2 = Input(shape=scaled_in_data[...,1:2].shape[1:])\n",
    "filter_input3 = Input(shape=scaled_in_data[...,2:3].shape[1:])\n",
    "conv_input = Input(shape=scaled_in_image.shape[1:])\n",
    "\n",
    "# use average pooling for input\n",
    "pool_1_ch1 = AveragePooling2D(pool_size=(8, 8))(filter_input1)\n",
    "pool_1_ch2 = AveragePooling2D(pool_size=(8, 8))(filter_input2)\n",
    "pool_1_ch3 = AveragePooling2D(pool_size=(8, 8))(filter_input3)\n",
    "\n",
    "dense_1_ch1 = Dense(16, activation=\"relu\")(pool_1_ch1)\n",
    "dense_1_ch2 = Dense(16, activation=\"relu\")(pool_1_ch2)\n",
    "dense_1_ch3 = Dense(16, activation=\"relu\")(pool_1_ch3)\n",
    "\n",
    "flat_1_ch1 = Flatten()(dense_1_ch1)\n",
    "flat_1_ch2 = Flatten()(dense_1_ch2)\n",
    "flat_1_ch3 = Flatten()(dense_1_ch3)\n",
    "\n",
    "conv_1 = Conv2D(8, (5, 5), padding=\"same\")(conv_input)\n",
    "act_1 = Activation(\"relu\")(conv_1)\n",
    "pool_1 = MaxPool2D(pool_size=(4, 4))(act_1)\n",
    "conv_2 = Conv2D(16, (5, 5), padding=\"same\")(pool_1)\n",
    "act_2 = Activation(\"relu\")(conv_2)\n",
    "pool_2 = MaxPool2D(pool_size=(4, 4))(act_2)\n",
    "conv_3 = Conv2D(32, (5, 5), padding=\"same\")(pool_2)\n",
    "act_3 = Activation(\"relu\")(conv_3)\n",
    "pool_3 = MaxPool2D(pool_size=(4, 4))(act_3)\n",
    "flat_3 = Flatten()(pool_3)\n",
    "\n",
    "concat = concatenate([flat_1_ch1,flat_1_ch2,flat_1_ch3,flat_3],axis=-1)\n",
    "flat = Flatten()(concat)\n",
    "\n",
    "dense_2 = Dense(32, activation=\"relu\")(flat)\n",
    "#dense_3 = Dense(16, activation=\"relu\")(dense_2)\n",
    "out = Dense(all_labels.shape[1])(dense_2)  # number of outputs determined by the parameters we are training to\n",
    "mod = Model([filter_input1,filter_input2,filter_input3,conv_input], out)\n",
    "mod.compile(optimizer=\"adam\", loss=\"mae\", metrics=['acc'])\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(mod,show_shapes=True,to_file=\"results/holodec_\"+ds_file.replace(\".nc\",\"_\")+\"\".join(all_labels.columns)+\"_denseNN_MultiIn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_data = [scaled_in_data[valid_index:split_index,...,0:1].values,\n",
    "                     scaled_in_data[valid_index:split_index,...,1:2].values,\n",
    "                     scaled_in_data[valid_index:split_index,...,2:3].values,\n",
    "                     scaled_in_image[valid_index:split_index].values,]\n",
    "\n",
    "scaled_valid_data = [scaled_in_data[:valid_index,...,0:1].values,\n",
    "                     scaled_in_data[:valid_index,...,1:2].values,\n",
    "                     scaled_in_data[:valid_index,...,2:3].values,\n",
    "                     scaled_in_image[:valid_index].values,]\n",
    "\n",
    "scaled_test_data = [scaled_in_data[split_index:,...,0:1].values,\n",
    "                     scaled_in_data[split_index:,...,1:2].values,\n",
    "                     scaled_in_data[split_index:,...,2:3].values,\n",
    "                     scaled_in_image[split_index:].values,]\n",
    "\n",
    "scaled_all_data = [scaled_in_data[...,0:1].values,\n",
    "                   scaled_in_data[...,1:2].values,\n",
    "                   scaled_in_data[...,2:3].values,\n",
    "                   scaled_in_image.values,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = mod.fit(scaled_train_data,\n",
    "                  scaled_train_labels.values, \n",
    "                  batch_size=16, epochs=30, verbose=1,\n",
    "                  validation_data=(scaled_valid_data,scaled_val_labels.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(len(history.history['acc']))+1\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(epochs,history.history['loss'],'bo-',alpha=0.5,label='Training')\n",
    "ax.plot(epochs,history.history['val_loss'],'rs-',alpha=0.5,label='Validation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid(b=True)\n",
    "plt.legend()\n",
    "plt.savefig(\"results/LossHistory_denseNN_MultiIn_\"+ds_file.replace(\".nc\",\"_\")+\"\".join(all_labels.columns)+\".png\", dpi=200, bbox_inches=\"tight\")\n",
    "\n",
    "fig, bx = plt.subplots(1, 1, figsize=(8, 4))\n",
    "bx.plot(epochs,history.history['acc'],'bo-',alpha=0.5,label='Training')\n",
    "bx.plot(epochs,history.history['val_acc'],'rs-',alpha=0.5,label='Validation')\n",
    "bx.set_xlabel('Epoch')\n",
    "bx.set_ylabel('Accuracy')\n",
    "bx.grid(b=True)\n",
    "plt.legend()\n",
    "plt.savefig(\"results/AccuracyHistory_denseNN_MultiIn_\"+ds_file.replace(\".nc\",\"_\")+\"\".join(all_labels.columns)+\".png\", dpi=200, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can skip the training process and just load the CNN model\n",
    "# mod = load_model(\"holodec_ft_dxyz_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_start = datetime.datetime.now()\n",
    "preds_out = mod.predict(scaled_all_data, batch_size=64)\n",
    "cnn_stop = datetime.datetime.now()\n",
    "print(f\"{scaled_in_data.values.shape[0]} samples in {(cnn_stop-cnn_start).total_seconds()} seconds\")\n",
    "print(f\"for {(cnn_stop-cnn_start).total_seconds()/scaled_in_data.values.shape[0]} seconds per hologram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(mod, \"results/holodec_\"+ds_file.replace(\".nc\",\"\")+\"\".join(all_labels.columns)+\"_denseNN_MultiIn.h5\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_original = scaler.inverse_transform(preds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_labels.max() - test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error = np.mean(preds_original[split_index:] - test_labels.values, axis=0)\n",
    "std_error = np.std(preds_original[split_index:] - test_labels.values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = {}\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "for a, clabel in enumerate(all_labels.columns):\n",
    "    ax=axes.ravel()[a]\n",
    "    ax.scatter(test_labels.iloc[:, a], preds_original[split_index:, a], 1, 'k')\n",
    "    diag = np.linspace(test_labels.iloc[:, a].min(), test_labels.iloc[:, a].max(), 10)\n",
    "    ax.plot(diag, diag, 'b--' )\n",
    "    ax.set_title(test_labels.columns[a])\n",
    "    plt.text(0.1,0.9,f\"${mean_error[a]:.1f} \\pm {std_error[a]:.1f} \\mu m$\",ha='left',va='top',transform=ax.transAxes)\n",
    "    validation_data[test_labels.columns[a]] = test_labels.iloc[:, a]\n",
    "    validation_data[test_labels.columns[a]+'_pred'] = preds_original[split_index:, a]\n",
    "plt.savefig(\"results/error_scatter_denseNN_\"+ds_file.replace(\".nc\",\"_\")+\"\".join(all_labels.columns)+\".png\", dpi=200, bbox_inches=\"tight\")\n",
    "validation_data_df = pd.DataFrame(validation_data)\n",
    "validation_data_df.to_csv('results/validation_data_denseNN_MultiIn_'+ds_file.replace(\".nc\",\"_\")+''.join(all_labels.columns)+'.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "for a, clabel in enumerate(all_labels.columns):\n",
    "    ax=axes.ravel()[a]\n",
    "    ax.hist( (preds_original[split_index:, a] - test_labels.iloc[:, a].values) / (test_labels.iloc[:, a].max() - test_labels.iloc[:, a].min()),\n",
    "           bins=20)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Error in \"+test_labels.columns[a])\n",
    "plt.savefig(\"results/relative_error_histogram_denseNN_MultiIn_\"+ds_file.replace(\".nc\",\"_\")+\"\".join(all_labels.columns)+\".png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(preds_original[split_index:] - test_labels.values), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(preds_original[split_index:] - test_labels.values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(preds_original[split_index:] - test_labels.values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(preds_out[split_index:] - scaled_test_labels.values), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"xsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
