{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Activation, MaxPool2D, SeparableConv2D, UpSampling2D, concatenate, Conv2DTranspose, Lambda, Reshape, Layer\n",
    "# from tensorflow.keras.layers import Input, Conv2D\n",
    "from tensorflow.keras.models import Model, save_model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "# set path to local libraries\n",
    "dirP_str = '../../../library'\n",
    "if dirP_str not in sys.path:\n",
    "    sys.path.append(dirP_str)\n",
    "\n",
    "import ml_utils as ml\n",
    "import ml_defs as mldef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths= {'data':'/scr/sci/mhayman/holodec/holodec-ml-data/'}\n",
    "fn = 'synthetic_holograms_v02.nc'\n",
    "\n",
    "run_num = 0\n",
    "num_epochs = 150\n",
    "\n",
    "input_variable = 'image'\n",
    "\n",
    "split_fraction = 0.8\n",
    "valid_fraction = 0.2\n",
    "\n",
    "image_rescale = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Model Definitions\n",
    "settings={\n",
    "        'n_layers':1, # must be greater than 1 because the latent layer counts\n",
    "        'n_filters':2, # number of input convolutional channels\n",
    "        'nConv':4, # convolution kernel size\n",
    "        'nPool':4, # max pool size\n",
    "        'activation':'relu', # convolution activation\n",
    "        'kernel_initializer':\"he_normal\",\n",
    "        'latent_dim':32,\n",
    "        'n_dense_layers':2, # number of dense layers in bottom layer\n",
    "        'loss_fun':'mse',   # training loss function\n",
    "        'out_act':'linear',  # output activation\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(paths['data']+fn,chunks={'hologram_number':1}) as ds:\n",
    "    print(ds.data_vars)\n",
    "#     file_base = 'histogram'+file_use+'data_%dcount'%hologram_count+run_date_str\n",
    "    print('Training dataset attributes')\n",
    "    for att in ds.attrs:\n",
    "        print('  '+att+': '+str(ds.attrs[att]))\n",
    "    \n",
    "    print('   max particle size: %d'%ds['d'].values.max())\n",
    "    print('   min particle size: %d'%ds['d'].values.min())\n",
    "    print()\n",
    "    \n",
    "    # Setup labels\n",
    "    split_index = np.int(split_fraction*ds.sizes['hologram_number'])  # number of training+validation points\n",
    "    valid_index = np.int(valid_fraction*ds.sizes['hologram_number'])  # number of validation points\n",
    "\n",
    "    train_labels = ds[input_variable].isel(hologram_number=slice(valid_index,split_index))\n",
    "    test_labels = ds[input_variable].isel(hologram_number=slice(split_index,None))\n",
    "    val_labels = ds[input_variable].isel(hologram_number=slice(None,valid_index))\n",
    "\n",
    "    scaled_train_labels = train_labels/image_rescale\n",
    "    scaled_val_labels = val_labels/image_rescale\n",
    "    scaled_test_labels = test_labels/image_rescale\n",
    "    \n",
    "#     scaler = ml.MinMaxScalerX(train_labels,dim=('hologram_number','xsize','ysize'))\n",
    "#     scaled_train_labels = scaler.fit_transform(train_labels)\n",
    "#     scaled_val_labels = scaler.fit_transform(val_labels)\n",
    "#     scaled_test_labels = scaler.fit_transform(test_labels)\n",
    "#     scaled_all_labels = scaler.fit_transform(all_labels)\n",
    "\n",
    "    # setup the input to be used\n",
    "    \n",
    "    scaled_in_data = ds[input_variable]/image_rescale\n",
    "    print('\\ninput dimensions:')\n",
    "    print(scaled_in_data.dims)\n",
    "    print(scaled_in_data.shape)\n",
    "    print()\n",
    "    print('split index: %d'%split_index)\n",
    "    print('valid index: %d'%valid_index)\n",
    "    \n",
    "    if not 'channel' in scaled_in_data.dims:\n",
    "        scaled_in_data = scaled_in_data.expand_dims(\"channel\", 3)\n",
    "    scaled_in_train = scaled_in_data.isel(hologram_number=slice(valid_index,split_index))\n",
    "    scaled_in_valid = scaled_in_data.isel(hologram_number=slice(None,valid_index))\n",
    "    scaled_in_test = scaled_in_data.isel(hologram_number=slice(split_index,None))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = settings['loss_fun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = settings['n_filters']\n",
    "nConv = settings['nConv']\n",
    "nPool = settings['nPool']\n",
    "kernel_initializer = settings['kernel_initializer']\n",
    "latent_dim = settings['latent_dim']\n",
    "n_dense_layers = settings['n_dense_layers']\n",
    "\n",
    "# create the model\n",
    "input_node = Input(shape=scaled_in_data.shape[1:])  \n",
    "next_input_node = input_node\n",
    "for _ in range(settings['n_layers']):\n",
    "    # define the down sampling layer\n",
    "    conv_1d = Conv2D(n_filters, (nConv, nConv), padding=\"same\", kernel_initializer = kernel_initializer)(next_input_node)\n",
    "    act_1d = Activation(\"relu\")(conv_1d)\n",
    "    conv_2d = Conv2D(n_filters, (nConv, nConv), padding=\"same\", kernel_initializer = kernel_initializer)(act_1d)\n",
    "    act_2d = Activation(\"relu\")(conv_2d)\n",
    "    next_input_node = MaxPool2D(pool_size=(nPool, nPool))(act_2d)\n",
    "    n_filters = n_filters*2\n",
    "\n",
    "n_filters = n_filters//2\n",
    "\n",
    "input_shape = K.int_shape(next_input_node)\n",
    "zinput = Flatten()(next_input_node)\n",
    "z_mean = Dense(latent_dim,activation='relu')(zinput)\n",
    "z_log_var = Dense(latent_dim,activation='relu')(zinput)\n",
    "for _ in range(np.maximum(n_dense_layers-2,0)):\n",
    "    # represent the mean and variance branches\n",
    "    # with separate dense networks\n",
    "    z_mean = Dense(latent_dim,activation='relu')(z_mean)\n",
    "    z_log_var = Dense(latent_dim,activation='relu')(z_log_var)\n",
    "z_mean = Dense(latent_dim,activation='linear')(z_mean)\n",
    "z_log_var = Dense(latent_dim,activation='linear')(z_log_var)\n",
    "\n",
    "z = Lambda(mldef.vae_sample)([z_mean,z_log_var])\n",
    "\n",
    "x1 = Dense(np.prod(input_shape[1:]),activation='relu')(z)\n",
    "return_node = Reshape(input_shape[1:])(x1)\n",
    "\n",
    "# # add VAE with convolution inputs and outputs\n",
    "# unet_out = mldef.add_unet_vae(cnn_input,settings['n_layers'],settings['n_filters'],nConv=settings['nConv'],\n",
    "#             nPool=settings['nPool'],activation=settings['activation'],\n",
    "#             kernel_initializer = settings['kernel_initializer'],\n",
    "#             latent_dim=settings['latent_dim'],n_dense_layers=settings['n_dense_layers'],)\n",
    "\n",
    "for _ in range(np.maximum(settings['n_layers'],0)):\n",
    "    # define the up sampling and feed foward layer\n",
    "    upsamp_1u = Conv2DTranspose(n_filters, (nConv,nConv), strides=(nPool,nPool),padding=\"same\")(return_node)\n",
    "    conv_1u = Conv2D(n_filters,(nConv,nConv),padding=\"same\",kernel_initializer = kernel_initializer)(upsamp_1u)\n",
    "    act_1u = Activation(\"relu\")(conv_1u)\n",
    "    conv_2u = Conv2D(n_filters,(nConv,nConv),padding=\"same\",kernel_initializer = kernel_initializer)(act_1u)\n",
    "    return_node = Activation(\"relu\")(conv_2u)\n",
    "    n_filters = n_filters // 2\n",
    "\n",
    "# add the output layer\n",
    "z_decoded = Conv2D(1,(1,1),padding=\"same\",activation=settings['out_act'])(return_node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(Layer):\n",
    "    def vae_loss(self,x,z_decoded,z_mean,z_log_var):\n",
    "        x_mse_loss = tensorflow.keras.metrics.mse(x,z_decoded)\n",
    "        x_mse_loss = K.mean(K.square(x-z_decoded),axis=[1,2,3])\n",
    "        beta = 5e-4\n",
    "        kl_loss = -beta*K.mean(1+z_log_var-K.square(z_mean) - K.exp(z_log_var),axis=-1)\n",
    "        return K.mean(x_mse_loss+ kl_loss)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        z_mean = inputs[2]\n",
    "        z_log_var = inputs[3]\n",
    "        loss = self.vae_loss(x,z_decoded,z_mean,z_log_var)\n",
    "        self.add_loss(loss,inputs=inputs)\n",
    "        return z_decoded\n",
    "\n",
    "y = CustomVariationalLayer()([input_node,z_decoded,z_mean,z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and compile the model\n",
    "mod = Model(input_node, y)\n",
    "mod.compile(optimizer=\"adam\", loss=None, metrics=['acc'])\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(mod,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = mod.fit(scaled_in_train.values,y=None,\n",
    "                  batch_size=16, epochs=20, verbose=1,\n",
    "                  validation_data=(scaled_in_valid.values,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(len(history.history['loss']))+1\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(epochs,history.history['loss'],'bo-',alpha=0.5,label='Training')\n",
    "ax.plot(epochs,history.history['val_loss'],'rs-',alpha=0.5,label='Validation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid(b=True)\n",
    "plt.legend()\n",
    "# plt.savefig(\"results/LossHistory_\"+nn_descript+'_'+ds_file.replace(\".nc\",\"\")+f\"_{num_epochs}epochs_run{run_num}\"+\".png\", dpi=200, bbox_inches=\"tight\")\n",
    "\n",
    "# fig, bx = plt.subplots(1, 1, figsize=(8, 4))\n",
    "# bx.plot(epochs,history.history['acc'],'bo-',alpha=0.5,label='Training')\n",
    "# bx.plot(epochs,history.history['val_acc'],'rs-',alpha=0.5,label='Validation')\n",
    "# bx.set_xlabel('Epoch')\n",
    "# bx.set_ylabel('Accuracy')\n",
    "# bx.grid(b=True)\n",
    "# plt.legend()\n",
    "# plt.savefig(\"results/AccuracyHistory_\"+nn_descript+'_'+ds_file.replace(\".nc\",\"\")+f\"_{num_epochs}epochs_run{run_num}\"+\".png\", dpi=200, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_start = datetime.datetime.now()\n",
    "preds_out = mod.predict(scaled_in_test.values, batch_size=64)\n",
    "cnn_stop = datetime.datetime.now()\n",
    "print(f\"{scaled_in_test.values.shape[0]} samples in {(cnn_stop-cnn_start).total_seconds()} seconds\")\n",
    "print(f\"for {(cnn_stop-cnn_start).total_seconds()/scaled_in_test.values.shape[0]} seconds per hologram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_out_da = xr.DataArray(preds_out,dims=('hologram_number','xsize','ysize','channel'),\n",
    "                            coords=scaled_in_test.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for im in [19, 206, 432, 543]:\n",
    "    fig_obj, ax_obj_lst = plt.subplots(1, 3, figsize=(3*7, 4))\n",
    "    ax_obj = ax_obj_lst[0]\n",
    "    im_obj = ax_obj.imshow(scaled_in_test.isel(hologram_number=im,channel=0))\n",
    "    plt.colorbar(im_obj, ax=ax_obj)\n",
    "    ax_obj.set_title('True image')\n",
    "\n",
    "    ax_obj = ax_obj_lst[1]\n",
    "    im_obj = ax_obj.imshow(preds_out_da.isel(hologram_number=im,channel=0))\n",
    "    plt.colorbar(im_obj, ax=ax_obj)\n",
    "    ax_obj.set_title('Reconstructed Image')\n",
    "    \n",
    "    ax_obj = ax_obj_lst[2]\n",
    "    im_obj = ax_obj.imshow(scaled_in_test.isel(hologram_number=im,channel=0)-preds_out_da.isel(hologram_number=im,channel=0))\n",
    "    plt.colorbar(im_obj, ax=ax_obj)\n",
    "    ax_obj.set_title('Difference')\n",
    "#     plt.plot(preds_out_da.isel(hologram_number=im,channel=0,ysize=300))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a UNET for image processing\n",
    "nFilters = 16\n",
    "nPool = 2\n",
    "nConv = 7\n",
    "nLayers = 3\n",
    "loss_fun = \"mse\" #,\"mae\" #\"binary_crossentropy\"\n",
    "out_act = \"linear\" # \"sigmoid\"\n",
    "nn_descript = f'UNET_{nFilters}Filt_{nConv}Conv_{nPool}Pool_{nLayers}Layers_'+loss_fun+'_'+out_act\n",
    "\n",
    "# define the input based on input data dimensions\n",
    "cnn_input = Input(shape=scaled_in_data.shape[1:])  \n",
    "\n",
    "# create the unet\n",
    "unet_out = add_conv_layers(cnn_input,nLayers,nFilters,nConv=nConv,nPool=nPool,activation=\"relu\")\n",
    "\n",
    "# add the output layer\n",
    "out = Conv2D(scaled_train_labels.sizes['type'],(1,1),padding=\"same\",activation=out_act)(unet_out)\n",
    "\n",
    "# build and compile the model\n",
    "mod = Model(cnn_input, unet_out)\n",
    "mod.compile(optimizer=\"adam\", loss=loss_fun,metrics=['acc'])\n",
    "mod.summary()\n",
    "run_num=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(mod,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 16\n",
    "nPool = 4\n",
    "nConv = 5\n",
    "loss_fun = \"mse\" #,\"mae\" #\"binary_crossentropy\"\n",
    "out_act = \"linear\" # \"sigmoid\"\n",
    "nn_descript = f'UNET_{n_filters}Filt_{nConv}Conv_{nPool}Pool_'+loss_fun+'_'+out_act\n",
    "cnn_input2 = Input(shape=scaled_in_data.shape[1:])  # input\n",
    "\n",
    "conv_1a = SeparableConv2D(n_filters*1, (nConv, nConv), padding=\"same\", kernel_initializer = \"he_normal\")(cnn_input)\n",
    "act_1a = Activation(\"relu\")(conv_1a)\n",
    "conv_1b = SeparableConv2D(n_filters*1, (nConv, nConv), padding=\"same\", kernel_initializer = \"he_normal\")(act_1a)\n",
    "act_1b = Activation(\"relu\")(conv_1b)\n",
    "pool_1 = MaxPool2D(pool_size=(nPool, nPool))(act_1b)\n",
    "\n",
    "conv_2a = SeparableConv2D(n_filters*2,(nConv,nConv),padding=\"same\", kernel_initializer = \"he_normal\")(pool_1)\n",
    "act_2a = Activation(\"relu\")(conv_2a)\n",
    "conv_2b = SeparableConv2D(n_filters*2,(nConv,nConv),padding=\"same\", kernel_initializer = \"he_normal\")(act_2a)\n",
    "act_2b = Activation(\"relu\")(conv_2b)\n",
    "pool_2 = MaxPool2D(pool_size=(nPool, nPool))(act_2b)\n",
    "\n",
    "conv_3a = SeparableConv2D(n_filters*4,(nConv,nConv),padding=\"same\", kernel_initializer = \"he_normal\")(pool_2)\n",
    "act_3a = Activation(\"relu\")(conv_3a)\n",
    "conv_3b = SeparableConv2D(n_filters*4,(nConv,nConv),padding=\"same\", kernel_initializer = \"he_normal\")(act_3a)\n",
    "act_3b = Activation(\"relu\")(conv_3b)\n",
    "pool_3 = MaxPool2D(pool_size=(nPool, nPool))(act_3b)\n",
    "\n",
    "conv_4a = SeparableConv2D(n_filters*8,(nConv,nConv),padding=\"same\", kernel_initializer = \"he_normal\")(pool_3)\n",
    "act_4a = Activation(\"relu\")(conv_4a)\n",
    "\n",
    "conv_4b = SeparableConv2D(n_filters*8,(nConv,nConv),padding=\"same\", kernel_initializer = \"he_normal\")(act_4a)\n",
    "act_4b = Activation(\"relu\")(conv_4b)\n",
    "\n",
    "# upsamp_5 = UpSampling2D((4,4))(act_4b)\n",
    "upsamp_5 = Conv2DTranspose(n_filters*4, (nConv,nConv), strides=(nPool,nPool),padding=\"same\")(act_4b)\n",
    "concat_5 = concatenate([upsamp_5,act_3b],axis=3)\n",
    "conv_5a = SeparableConv2D(n_filters*4,(nConv,nConv),padding=\"same\", kernel_initializer = \"he_normal\")(concat_5)\n",
    "act_5a = Activation(\"relu\")(conv_5a)\n",
    "conv_5b = SeparableConv2D(n_filters*4,(nConv,nConv),padding=\"same\", kernel_initializer = \"he_normal\")(act_5a)\n",
    "act_5b = Activation(\"relu\")(conv_5b)\n",
    "\n",
    "\n",
    "# upsamp_6 = UpSampling2D((4,4))(act_5b)\n",
    "upsamp_6 = Conv2DTranspose(n_filters*2, (nConv,nConv), strides=(nPool,nPool),padding=\"same\")(act_5b)\n",
    "concat_6 = concatenate([upsamp_6,act_2b],axis=3)\n",
    "conv_6a = SeparableConv2D(n_filters*2,(nConv,nConv),padding=\"same\",kernel_initializer = \"he_normal\")(concat_6)\n",
    "act_6a = Activation(\"relu\")(conv_6a)\n",
    "conv_6b = SeparableConv2D(n_filters*2,(nConv,nConv),padding=\"same\",kernel_initializer = \"he_normal\")(act_6a)\n",
    "act_6b = Activation(\"relu\")(conv_6b)\n",
    "\n",
    "# upsamp_7 = UpSampling2D((4,4))(act_6b)\n",
    "upsamp_7 = Conv2DTranspose(n_filters, (nConv,nConv), strides=(nPool,nPool),padding=\"same\")(act_6b)\n",
    "concat_7 = concatenate([upsamp_7,act_1b],axis=3)\n",
    "conv_7a = SeparableConv2D(n_filters,(nConv,nConv),padding=\"same\",kernel_initializer = \"he_normal\")(concat_7)\n",
    "act_7a = Activation(\"relu\")(conv_7a)\n",
    "conv_7b = SeparableConv2D(n_filters,(nConv,nConv),padding=\"same\",kernel_initializer = \"he_normal\")(act_7a)\n",
    "act_7b = Activation(\"relu\")(conv_7b)\n",
    "\n",
    "out2 = Conv2D(scaled_train_labels.sizes['type'],(1,1),padding=\"same\",activation=out_act)(act_7b)\n",
    "\n",
    "\n",
    "mod2 = Model(cnn_input2, out2)\n",
    "mod2.compile(optimizer=\"adam\", loss=loss_fun,metrics=['acc'])\n",
    "mod2.summary()\n",
    "run_num=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(mod,show_shapes=True)\n",
    "# plot_model(mod,show_shapes=True,to_file=\"results/holodec_\"+nn_descript+'_'+ds_file.replace(\".nc\",\"\")+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_labels.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = mod.fit(scaled_in_data[valid_index:split_index].values,\n",
    "                  scaled_train_labels.values, \n",
    "                  batch_size=64, epochs=num_epochs, verbose=1,\n",
    "                  validation_data=(scaled_in_data[:valid_index].values,scaled_val_labels.values))\n",
    "run_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(len(history.history['loss']))+1\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(epochs,history.history['loss'],'bo-',alpha=0.5,label='Training')\n",
    "ax.plot(epochs,history.history['val_loss'],'rs-',alpha=0.5,label='Validation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid(b=True)\n",
    "plt.legend()\n",
    "plt.savefig(\"results/LossHistory_\"+nn_descript+'_'+ds_file.replace(\".nc\",\"\")+f\"_{num_epochs}epochs_run{run_num}\"+\".png\", dpi=200, bbox_inches=\"tight\")\n",
    "\n",
    "fig, bx = plt.subplots(1, 1, figsize=(8, 4))\n",
    "bx.plot(epochs,history.history['acc'],'bo-',alpha=0.5,label='Training')\n",
    "bx.plot(epochs,history.history['val_acc'],'rs-',alpha=0.5,label='Validation')\n",
    "bx.set_xlabel('Epoch')\n",
    "bx.set_ylabel('Accuracy')\n",
    "bx.grid(b=True)\n",
    "plt.legend()\n",
    "plt.savefig(\"results/AccuracyHistory_\"+nn_descript+'_'+ds_file.replace(\".nc\",\"\")+f\"_{num_epochs}epochs_run{run_num}\"+\".png\", dpi=200, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(mod, ds_path+\"/models/holodec_\"+nn_descript+'_'+ds_file.replace(\".nc\",\"\")+f\"{num_epochs}epochs_run{run_num}\"+\".h5\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can skip the training process and just load the CNN model\n",
    "# mod = load_model(ds_path+\"/models/holodec_UNET_image_data_256x256_5000count30epochs_run1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_start = datetime.datetime.now()\n",
    "preds_out = mod.predict(scaled_in_data.values, batch_size=64)\n",
    "cnn_stop = datetime.datetime.now()\n",
    "print(f\"{scaled_in_data.values.shape[0]} samples in {(cnn_stop-cnn_start).total_seconds()} seconds\")\n",
    "print(f\"for {(cnn_stop-cnn_start).total_seconds()/scaled_in_data.values.shape[0]} seconds per hologram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_out_da = xr.DataArray(preds_out,dims=('hologram_number','xsize','ysize','type'),\n",
    "                            coords=all_labels.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_original = scaler.inverse_transform(preds_out_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iscatter = np.nonzero(preds_original.sel(type='amplitude').values.flatten() > 0.2)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for a, clabel in enumerate(all_labels.coords['type'].values):\n",
    "    ax=axes.ravel()[a]\n",
    "    ax.scatter(all_labels.sel(type=clabel).values.flatten()[iscatter], preds_original.sel(type=clabel).values.flatten()[iscatter], 1, 'k')\n",
    "    diag = np.linspace(all_labels.sel(type=clabel).min(), all_labels.sel(type=clabel).max(), 10)\n",
    "    ax.plot(diag, diag, 'b--' )\n",
    "    ax.set_title(clabel)\n",
    "    plt.savefig(\"results/\"+nn_descript+f\"_ScatterPlot\"+f\"_{num_epochs}epochs_run{run_num}_\"+ds_file.replace(\".nc\",\"\")+\".png\",dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error = (preds_original[split_index:] - test_labels).mean(dim=('hologram_number','xsize','ysize'))\n",
    "std_error = (preds_original[split_index:] - test_labels).std(dim=('hologram_number','xsize','ysize'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [18,2854,1247,858,3143,832,4021,3921,222,2431,321]\n",
    "\n",
    "diff_cmap = plt.get_cmap('seismic')\n",
    "diff_cmap.set_bad(color='black')\n",
    "\n",
    "for ind in index_list:\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    ax = ax.ravel()\n",
    "    inan_mask = np.nonzero((preds_original.sel(type='amplitude',hologram_number=ind).values < 0.1)* \\\n",
    "        (all_labels.sel(type='amplitude',hologram_number=ind).values < 0.1))\n",
    "    nan_mask = np.ones(preds_original.sel(type='amplitude',hologram_number=ind).values.shape)\n",
    "    nan_mask[inan_mask] = np.nan\n",
    "    ax[0].imshow(preds_original.sel(type='amplitude',hologram_number=ind).values,vmin=0,vmax=1)\n",
    "    ax[1].imshow(all_labels.sel(type='amplitude',hologram_number=ind).values,vmin=0,vmax=1)\n",
    "    ax[2].imshow(preds_original.sel(type='amplitude',hologram_number=ind).values-all_labels.sel(type='amplitude',hologram_number=ind).values,vmin=-1,vmax=1,cmap=diff_cmap)\n",
    "    ax[3].imshow(preds_original.sel(type='z',hologram_number=ind).values*nan_mask,vmin=ds.attrs['zmin'],vmax=ds.attrs['zmax'])\n",
    "    ax[4].imshow(all_labels.sel(type='z',hologram_number=ind).values*nan_mask,vmin=ds.attrs['zmin'],ds.attrs['zmax'])\n",
    "    ax[5].imshow((preds_original.sel(type='z',hologram_number=ind).values-all_labels.sel(type='z',hologram_number=ind).values)*nan_mask,vmin=-1e-2,vmax=1e-2,cmap=diff_cmap)\n",
    "    plt.savefig(\"results/\"+nn_descript+f\"_ExampleImage{ind}\"+f\"_{num_epochs}epochs_run{run_num}_\"+ds_file.replace(\".nc\",\"\")+\".png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_number = in_data.sizes['channel']\n",
    "# index_list = [18]\n",
    "for ind in index_list:\n",
    "    fig, ax = plt.subplots(2, channel_number//2, figsize=(np.minimum(channel_number*3,12), 8))\n",
    "    for ai in range(channel_number):\n",
    "        axind = ai//2+np.mod(ai,2)*channel_number//2\n",
    "        ax[np.mod(ai,2),ai//2].imshow(scaled_in_data.isel(channel=ai,hologram_number=ind),vmin=-2,vmax=2)\n",
    "    plt.savefig(\"results/\"+nn_descript+f\"_ExampleInput{ind}\"+f\"_{num_epochs}epochs_run{run_num}_\"+ds_file.replace(\".nc\",\"\")+\".png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "xg,yg = np.meshgrid(preds_original['xsize'].values,preds_original['ysize'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in index_list:\n",
    "    ipart = np.nonzero(preds_original.sel(type='amplitude',hologram_number=ind).values > 0.2)\n",
    "    amp_p = preds_original.sel(type='amplitude',hologram_number=ind).values[ipart]\n",
    "    z_p = preds_original.sel(type='z',hologram_number=ind).values[ipart]\n",
    "    x_p = xg[ipart]\n",
    "    y_p = yg[ipart]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(z_p,x_p,y_p,c=amp_p,vmin=0,vmax=1)\n",
    "    ax.set_xlim([ds.attrs['zmin'],ds.attrs['zmax']])\n",
    "    ax.set_ylim([preds_original['xsize'].values[0],preds_original['xsize'].values[1]])\n",
    "    ax.set_zlim([preds_original['ysize'].values[0],preds_original['ysize'].values[1]])\n",
    "    ax.set_xlabel('z')\n",
    "    ax.set_ylabel('x')\n",
    "    ax.set_zlabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
