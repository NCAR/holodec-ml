{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "elder-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from holodecml.data import num_particles_dict, load_raw_datasets, scale_images, unet_bin_xy\n",
    "from holodecml.losses import unet_loss, unet_loss_xy\n",
    "from holodecml.models import custom_unet, custom_jnet, custom_jnet_full\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xarray as xr\n",
    "import os\n",
    "from os.path import join\n",
    "import yaml\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "appointed-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"/glade/p/cisl/aiml/ai4ess_hackathon/holodec/\"\n",
    "num_particles = [1,2,3,4,5,6,7,8,9,10,\"12-25\"]\n",
    "output_cols = [\"x\", \"y\", \"hid\"]\n",
    "subset = [500,500,500,500,500,500,500,500,500,500,500*13]\n",
    "scaler_out = MinMaxScaler()\n",
    "bin_factor = 10\n",
    "h = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incredible-integer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 500\n",
      "num_particle: 1\n",
      "train shapes: input - (500, 600, 400)\t output - (500, 3)\n",
      "valid shapes: input - (50, 600, 400)\t output - (50, 3)\n",
      "2 500\n",
      "num_particle: 2\n",
      "train shapes: input - (500, 600, 400)\t output - (1000, 3)\n",
      "valid shapes: input - (50, 600, 400)\t output - (100, 3)\n",
      "3 500\n",
      "num_particle: 3\n",
      "train shapes: input - (500, 600, 400)\t output - (1500, 3)\n",
      "valid shapes: input - (50, 600, 400)\t output - (150, 3)\n",
      "4 500\n",
      "num_particle: 4\n",
      "train shapes: input - (500, 600, 400)\t output - (2000, 3)\n",
      "valid shapes: input - (50, 600, 400)\t output - (200, 3)\n",
      "5 500\n",
      "num_particle: 5\n",
      "train shapes: input - (500, 600, 400)\t output - (2500, 3)\n",
      "valid shapes: input - (50, 600, 400)\t output - (250, 3)\n",
      "6 500\n",
      "num_particle: 6\n",
      "train shapes: input - (500, 600, 400)\t output - (3000, 3)\n",
      "valid shapes: input - (50, 600, 400)\t output - (300, 3)\n",
      "7 500\n",
      "num_particle: 7\n",
      "train shapes: input - (500, 600, 400)\t output - (3500, 3)\n",
      "valid shapes: input - (50, 600, 400)\t output - (350, 3)\n",
      "8 500\n",
      "num_particle: 8\n",
      "train shapes: input - (500, 600, 400)\t output - (4000, 3)\n",
      "valid shapes: input - (50, 600, 400)\t output - (400, 3)\n",
      "9 500\n",
      "num_particle: 9\n",
      "train shapes: input - (500, 600, 400)\t output - (4500, 3)\n",
      "valid shapes: input - (50, 600, 400)\t output - (450, 3)\n",
      "10 500\n",
      "num_particle: 10\n",
      "train shapes: input - (500, 600, 400)\t output - (5000, 3)\n",
      "valid shapes: input - (50, 600, 400)\t output - (500, 3)\n",
      "12-25 6500\n",
      "num_particle: 12-25\n",
      "train shapes: input - (6500, 600, 400)\t output - (120095, 3)\n",
      "valid shapes: input - (650, 600, 400)\t output - (11922, 3)\n"
     ]
    }
   ],
   "source": [
    "train_inputs_list = []\n",
    "train_outputs_list = []\n",
    "valid_inputs_list = []\n",
    "valid_outputs_list = []\n",
    "for num,sub in zip(num_particles, subset):\n",
    "    print(num,sub)\n",
    "    train_inputs,\\\n",
    "    train_outputs = load_raw_datasets(path_data, num, 'train',\n",
    "                                      output_cols, sub)\n",
    "    valid_inputs,\\\n",
    "    valid_outputs = load_raw_datasets(path_data, num, 'valid',\n",
    "                                      output_cols, sub//10)\n",
    "    print(f\"num_particle: {num}\")\n",
    "    print(f\"train shapes: input - {train_inputs.shape}\\t output - {train_outputs.shape}\")\n",
    "    print(f\"valid shapes: input - {valid_inputs.shape}\\t output - {valid_outputs.shape}\")\n",
    "    \n",
    "    train_inputs, scaler_in = scale_images(train_inputs)\n",
    "    valid_inputs, _ = scale_images(valid_inputs, scaler_in)\n",
    "\n",
    "    train_outputs = unet_bin_xy(train_inputs, train_outputs, bin_factor)\n",
    "    valid_outputs = unet_bin_xy(valid_inputs, valid_outputs, bin_factor)\n",
    "        \n",
    "    train_inputs_list.append(train_inputs)\n",
    "    train_outputs_list.append(train_outputs)\n",
    "    valid_inputs_list.append(valid_inputs)\n",
    "    valid_outputs_list.append(valid_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "equivalent-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = np.vstack(train_inputs_list)\n",
    "train_outputs = np.vstack(train_outputs_list)\n",
    "valid_inputs = np.vstack(valid_inputs_list)\n",
    "valid_outputs = np.vstack(valid_outputs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "premier-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jnet_datasets_xy_1to25(path_data, num_particles, output_cols,\n",
    "                                subset=False, bin_factor=False, input_col=\"image\"):\n",
    "\n",
    "    train_inputs_list = []\n",
    "    train_outputs_list = []\n",
    "    valid_inputs_list = []\n",
    "    valid_outputs_list = []\n",
    "    for num,sub in zip(num_particles, subset):\n",
    "        train_inputs,\\\n",
    "        train_outputs = load_raw_datasets(path_data, num, 'train',\n",
    "                                          output_cols, sub)\n",
    "        valid_inputs,\\\n",
    "        valid_outputs = load_raw_datasets(path_data, num, 'valid',\n",
    "                                          output_cols, sub//10)\n",
    "        train_inputs_list.append(train_inputs)\n",
    "        train_outputs_list.append(train_outputs)\n",
    "        valid_inputs_list.append(valid_inputs)\n",
    "        valid_outputs_list.append(valid_outputs)\n",
    "\n",
    "    train_inputs = np.vstack(train_inputs_list)\n",
    "    train_outputs = np.vstack(train_outputs_list)\n",
    "    valid_inputs = np.vstack(valid_inputs_list)\n",
    "    valid_outputs = np.vstack(valid_outputs_list)\n",
    "    \n",
    "    return train_inputs, train_outputs, valid_inputs, valid_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-holocaust",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
