{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holodecml.data import load_raw_datasets, load_unet_datasets\n",
    "from holodecml.losses import unet_loss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"/glade/p/cisl/aiml/ai4ess_hackathon/holodec/\"\n",
    "num_particles = \"medium\"\n",
    "output_cols = [\"x\", \"y\", \"z\", \"d\", \"hid\"]\n",
    "subset = False\n",
    "scaler_out = MinMaxScaler()\n",
    "num_bins = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_outputs[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_raw, train_outputs_raw = load_raw_datasets(path_data, num_particles,\n",
    "                                                'train', output_cols, subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_inputs_raw, valid_outputs_raw = load_raw_datasets(path_data, num_particles,\n",
    "                                                'valid', output_cols, subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid = 0\n",
    "print(len(np.where(train_outputs_raw[\"hid\"] == hid + 1)[0]))\n",
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "ax.imshow(train_inputs_raw[hid].T, cmap='gray', vmin=0, vmax=255)\n",
    "# ax.title.set_text(f'')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hologram(h, inputs, outputs):\n",
    "    \"\"\"\n",
    "    Given a hologram number, plot hologram and particle point\n",
    "    \n",
    "    Args: \n",
    "        h: (int) hologram index\n",
    "        inputs: (pd df) input images\n",
    "        outputs: (pd df) output x, y, z, and d values by hid\n",
    "    \n",
    "    Returns:\n",
    "        print of pseudocolor plot of hologram and hologram particles\n",
    "    \"\"\"    \n",
    "    x_vals = np.linspace(-888*2, 888*2, inputs[h, :, :].shape[0])\n",
    "    y_vals = np.linspace(-592*2, 592*2, inputs[h, :, :].shape[1])\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.pcolormesh(x_vals, y_vals, inputs[h, :, :].T, cmap=\"RdBu_r\")\n",
    "    h_particles = np.where(outputs[\"hid\"] == h +1)[0]\n",
    "    for h_particle in h_particles:\n",
    "        plt.scatter(outputs.loc[h_particle, \"x\"],\n",
    "                    outputs.loc[h_particle, \"y\"],\n",
    "                    outputs.loc[h_particle, \"d\"],\n",
    "                    outputs.loc[h_particle, \"z\"],\n",
    "                    vmin=outputs[\"z\"].min(),\n",
    "                    vmax=outputs[\"z\"].max(),\n",
    "                    cmap=\"cool\")\n",
    "        plt.annotate(f\"d: {outputs.loc[h_particle,'d']:.1f} µm\",\n",
    "                     (outputs.loc[h_particle, \"x\"], outputs.loc[h_particle, \"y\"]))\n",
    "    plt.xlabel(\"horizontal particle position (µm)\", fontsize=16)\n",
    "    plt.ylabel(\"vertical particle position (µm)\", fontsize=16)\n",
    "    plt.title(f\"Hologram and particle positions plotted in four dimensions: {h_particles.shape[0]} particles\", fontsize=20, pad=20)\n",
    "    plt.colorbar().set_label(label=\"z-axis particle position (µm)\", size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1\n",
    "plot_hologram(h, train_inputs_raw, train_outputs_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy inputs/outputs\n",
    "xs = np.array([-10.5, 1.0, -9.5, -10.0, -9.5, 0.5, 1.5, -3.5, 6.5, 9.1, 10, 10.5])\n",
    "ys = np.array([-3.5, 2.0, -0.5, -1.5, -1.0, -4.0, 2.5, 0.5, -2.0, 3.0, 5, 4.5])\n",
    "zs = np.array([90, -100, 35, -65, -40, 0, -20, 80, 100, -11, 23, 88])\n",
    "ds = np.array([1.1, 0.5, 2.0, 3.5, 0.4, 0.01, 3.9, 2.3, 1.16, 1.95, 0.75, 0.25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_coords = []\n",
    "train_outputs = []\n",
    "for hid in train_outputs_raw[\"hid\"].unique()[:2]:\n",
    "    outputs_hid = train_outputs_raw.loc[train_outputs_raw['hid'] == hid]\n",
    "    xs_hid = np.digitize(outputs_hid['x'], np.linspace(-888*2, 888*2, train_inputs_raw.shape[0]))\n",
    "    ys_hid = np.digitize(outputs_hid['y'], np.linspace(-592*2, 592*2, train_inputs_raw.shape[1]))\n",
    "    zs_hid = outputs_hid['z'].values\n",
    "    ds_hid = outputs_hid['d'].values\n",
    "    coords = np.array(list(zip(xs_hid,ys_hid)))\n",
    "    coords = coords[np.lexsort((coords[:,1], coords[:,0]))]\n",
    "    unique, unique_idx, unique_counts = np.unique(coords, return_index=True, return_counts=True, axis=0)\n",
    "    for i in np.argwhere(unique_counts > 1):\n",
    "        # find indices in original coordinates where there are multiple particles\n",
    "        idx_equal = np.where((coords == unique[i][0]).all(axis=1))[0]\n",
    "        # find the index of the particle with the z that is closest to the camera\n",
    "        idx_max = np.argmin(zs_hid[np.min(idx_equal):np.max(idx_equal)+1])\n",
    "        unique_idx[i] = idx_equal[idx_max]\n",
    "    zs_hid = zs_hid[unique_idx]\n",
    "    ds_hid = ds_hid[unique_idx]\n",
    "    train_coords.append(coords)\n",
    "    train_outputs.append(np.stack((zs_hid, ds_hid), axis=1))\n",
    "# train_outputs_unet = np.stack(train_outputs_unet, axis=-1)\n",
    "# train_outputs_unet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.zeros((train_inputs_raw[0].shape[1], train_inputs_raw[0].shape[0]))\n",
    "image[train_coords[1][:, 0], train_coords[1][:, 1]] = zs_hid\n",
    "plt.imshow(image, interpolation='bilinear', cmap=plt.cm.gray, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_coords[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[3:6, 842:847]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How sparse is this image... very \n",
    "fig=plt.figure(figsize=(12, 8))\n",
    "image = np.zeros((train_inputs_raw[0].shape[1], train_inputs_raw[0].shape[0]))\n",
    "image[train_coords[0][:, 0], train_coords[0][:, 1]] = 1\n",
    "plt.imshow(image, interpolation='bilinear', cmap=plt.cm.gray, aspect='auto', vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_inputs, train_outputs, valid_inputs, valid_outputs = load_unet_datasets(path_data, num_particles, output_cols,\n",
    "                                                                              scaler_out, subset, num_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binned_hologram(h, inputs, coords, outputs):\n",
    "    \"\"\"\n",
    "    Given a hologram number, plot hologram and particle point\n",
    "    \n",
    "    Args: \n",
    "        h: (int) hologram index\n",
    "        inputs: (pd df) input images\n",
    "        outputs: (pd df) output x, y, z, and d values by hid\n",
    "    \n",
    "    Returns:\n",
    "        print of pseudocolor plot of hologram and hologram particles\n",
    "    \"\"\"    \n",
    "    x_vals = np.linspace(-888*2, 888*2, inputs[h, :, :].shape[0])\n",
    "    y_vals = np.linspace(-592*2, 592*2, inputs[h, :, :].shape[1])\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.pcolormesh(x_vals, y_vals, inputs[h, :, :].T, cmap=\"RdBu_r\")\n",
    "    for h_particle in range(outputs[h].shape[0]):\n",
    "        plt.scatter(np.linspace(-888*2, 888*2, train_inputs_raw.shape[0])[coords[h][h_particle][0]],\n",
    "                    np.linspace(-592*2, 592*2, train_inputs_raw.shape[1])[coords[h][h_particle][1]],\n",
    "                    outputs[h][h_particle][1],\n",
    "                    outputs[h][h_particle][0],\n",
    "                    vmin=outputs[h][:,1].min(),\n",
    "                    vmax=outputs[h][:,1].max(),\n",
    "                    cmap=\"cool\")\n",
    "        plt.annotate(f\"d: {outputs[h][h_particle][1]:.1f} µm\",\n",
    "                     (np.linspace(-888*2, 888*2, train_inputs_raw.shape[0])[coords[h][h_particle][0]],\n",
    "                      np.linspace(-592*2, 592*2, train_inputs_raw.shape[1])[coords[h][h_particle][1]]))\n",
    "    plt.xlabel(\"horizontal particle position (µm)\", fontsize=16)\n",
    "    plt.ylabel(\"vertical particle position (µm)\", fontsize=16)\n",
    "    plt.title(f\"Hologram and particle positions plotted in four dimensions: {outputs[h].shape[0]} particles\", fontsize=20, pad=20)\n",
    "    plt.colorbar().set_label(label=\"z-axis particle position (µm)\", size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1\n",
    "plot_binned_hologram(h, train_inputs_raw, train_coords, train_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Conv2DTranspose,\n",
    "    MaxPooling2D,\n",
    "    Dropout,\n",
    "    SpatialDropout2D,\n",
    "    UpSampling2D,\n",
    "    Input,\n",
    "    concatenate,\n",
    "    multiply,\n",
    "    add,\n",
    "    Activation,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_conv(filters, kernel_size, strides, padding):\n",
    "    return Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n",
    "\n",
    "\n",
    "def upsample_simple(filters, kernel_size, strides, padding):\n",
    "    return UpSampling2D(strides)\n",
    "\n",
    "\n",
    "def attention_gate(inp_1, inp_2, n_intermediate_filters):\n",
    "    \"\"\"Attention gate. Compresses both inputs to n_intermediate_filters filters before processing.\n",
    "       Implemented as proposed by Oktay et al. in their Attention U-net, see: https://arxiv.org/abs/1804.03999.\n",
    "    \"\"\"\n",
    "    inp_1_conv = Conv2D(\n",
    "        n_intermediate_filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(inp_1)\n",
    "    inp_2_conv = Conv2D(\n",
    "        n_intermediate_filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(inp_2)\n",
    "\n",
    "    f = Activation(\"relu\")(add([inp_1_conv, inp_2_conv]))\n",
    "    g = Conv2D(\n",
    "        filters=1,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(f)\n",
    "    h = Activation(\"sigmoid\")(g)\n",
    "    return multiply([inp_1, h])\n",
    "\n",
    "\n",
    "def attention_concat(conv_below, skip_connection):\n",
    "    \"\"\"Performs concatenation of upsampled conv_below with attention gated version of skip-connection\n",
    "    \"\"\"\n",
    "    below_filters = conv_below.get_shape().as_list()[-1]\n",
    "    attention_across = attention_gate(skip_connection, conv_below, below_filters)\n",
    "    return concatenate([conv_below, attention_across])\n",
    "\n",
    "\n",
    "def conv2d_block(\n",
    "    inputs,\n",
    "    use_batch_norm=True,\n",
    "    dropout=0.3,\n",
    "    dropout_type=\"spatial\",\n",
    "    filters=16,\n",
    "    kernel_size=(3, 3),\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    padding=\"same\",\n",
    "):\n",
    "\n",
    "    if dropout_type == \"spatial\":\n",
    "        DO = SpatialDropout2D\n",
    "    elif dropout_type == \"standard\":\n",
    "        DO = Dropout\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"dropout_type must be one of ['spatial', 'standard'], got {dropout_type}\"\n",
    "        )\n",
    "\n",
    "    c = Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        activation=activation,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=not use_batch_norm,\n",
    "    )(inputs)\n",
    "    if use_batch_norm:\n",
    "        c = BatchNormalization()(c)\n",
    "    if dropout > 0.0:\n",
    "        c = DO(dropout)(c)\n",
    "    c = Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        activation=activation,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=not use_batch_norm,\n",
    "    )(c)\n",
    "    if use_batch_norm:\n",
    "        c = BatchNormalization()(c)\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_unet(\n",
    "    input_shape,\n",
    "    num_classes=1,\n",
    "    activation=\"relu\",\n",
    "    use_batch_norm=True,\n",
    "    upsample_mode=\"deconv\",  # 'deconv' or 'simple'\n",
    "    dropout=0.3,\n",
    "    dropout_change_per_layer=0.0,\n",
    "    dropout_type=\"spatial\",\n",
    "    use_dropout_on_upsampling=False,\n",
    "    use_attention=False,\n",
    "    filters=16,\n",
    "    num_layers=3,\n",
    "    output_activation=\"sigmoid\",\n",
    "):  # 'sigmoid' or 'softmax'\n",
    "\n",
    "    \"\"\"\n",
    "    Customisable UNet architecture (Ronneberger et al. 2015 [1]).\n",
    "    Arguments:\n",
    "    input_shape: 3D Tensor of shape (x, y, num_channels)\n",
    "    num_classes (int): Unique classes in the output mask. Should be set to 1 for binary segmentation\n",
    "    activation (str): A keras.activations.Activation to use. ReLu by default.\n",
    "    use_batch_norm (bool): Whether to use Batch Normalisation across the channel axis between convolutional layers\n",
    "    upsample_mode (one of \"deconv\" or \"simple\"): Whether to use transposed convolutions or simple upsampling in the decoder part\n",
    "    dropout (float between 0. and 1.): Amount of dropout after the initial convolutional block. Set to 0. to turn Dropout off\n",
    "    dropout_change_per_layer (float between 0. and 1.): Factor to add to the Dropout after each convolutional block\n",
    "    dropout_type (one of \"spatial\" or \"standard\"): Type of Dropout to apply. Spatial is recommended for CNNs [2]\n",
    "    use_dropout_on_upsampling (bool): Whether to use dropout in the decoder part of the network\n",
    "    use_attention (bool): Whether to use an attention dynamic when concatenating with the skip-connection, implemented as proposed by Oktay et al. [3]\n",
    "    filters (int): Convolutional filters in the initial convolutional block. Will be doubled every block\n",
    "    num_layers (int): Number of total layers in the encoder not including the bottleneck layer\n",
    "    output_activation (str): A keras.activations.Activation to use. Sigmoid by default for binary segmentation\n",
    "    Returns:\n",
    "    model (keras.models.Model): The built U-Net\n",
    "    Raises:\n",
    "    ValueError: If dropout_type is not one of \"spatial\" or \"standard\"\n",
    "    \"\"\"\n",
    "\n",
    "    if upsample_mode == \"deconv\":\n",
    "        upsample = upsample_conv\n",
    "    else:\n",
    "        upsample = upsample_simple\n",
    "\n",
    "    # Build U-Net model\n",
    "    inputs = Input(input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    down_layers = []\n",
    "    for l in range(num_layers):\n",
    "        x = conv2d_block(\n",
    "            inputs=x,\n",
    "            filters=filters,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            dropout=dropout,\n",
    "            dropout_type=dropout_type,\n",
    "            activation=activation,\n",
    "        )\n",
    "        down_layers.append(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        dropout += dropout_change_per_layer\n",
    "        filters = filters * 2  # double the number of filters with each layer\n",
    "\n",
    "    x = conv2d_block(\n",
    "        inputs=x,\n",
    "        filters=filters,\n",
    "        use_batch_norm=use_batch_norm,\n",
    "        dropout=dropout,\n",
    "        dropout_type=dropout_type,\n",
    "        activation=activation,\n",
    "    )\n",
    "\n",
    "    if not use_dropout_on_upsampling:\n",
    "        dropout = 0.0\n",
    "        dropout_change_per_layer = 0.0\n",
    "\n",
    "    for conv in reversed(down_layers):\n",
    "        filters //= 2  # decreasing number of filters with each layer\n",
    "        dropout -= dropout_change_per_layer\n",
    "        x = upsample(filters, (2, 2), strides=(2, 2), padding=\"same\")(x)\n",
    "        if use_attention:\n",
    "            x = attention_concat(conv_below=x, skip_connection=conv)\n",
    "        else:\n",
    "            x = concatenate([x, conv])\n",
    "        x = conv2d_block(\n",
    "            inputs=x,\n",
    "            filters=filters,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            dropout=dropout,\n",
    "            dropout_type=dropout_type,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation=output_activation)(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = np.expand_dims(train_inputs, axis=-1).shape[1:]\n",
    "model = custom_unet(\n",
    "    input_shape,\n",
    "    use_batch_norm=True,\n",
    "    num_classes=3,\n",
    "    filters=64,\n",
    "    dropout=0.2,\n",
    "    output_activation='sigmoid',\n",
    "    use_attention=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(lr=0.0001),\n",
    "    loss=unet_loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 600, 400, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 600, 400, 64) 576         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 600, 400, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d (SpatialDropo (None, 600, 400, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 600, 400, 64) 36864       spatial_dropout2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 600, 400, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 300, 200, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 300, 200, 128 73728       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 300, 200, 128 512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_1 (SpatialDro (None, 300, 200, 128 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 300, 200, 128 147456      spatial_dropout2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 300, 200, 128 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 150, 100, 128 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 150, 100, 256 294912      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 150, 100, 256 1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_2 (SpatialDro (None, 150, 100, 256 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 150, 100, 256 589824      spatial_dropout2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 150, 100, 256 1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 75, 50, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 75, 50, 512)  1179648     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 75, 50, 512)  2048        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_3 (SpatialDro (None, 75, 50, 512)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 75, 50, 512)  2359296     spatial_dropout2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 75, 50, 512)  2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 150, 100, 256 524544      batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 150, 100, 256 65792       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 150, 100, 256 65792       conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 150, 100, 256 0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 150, 100, 256 0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 150, 100, 1)  257         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 150, 100, 1)  0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 150, 100, 256 0           batch_normalization_5[0][0]      \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 150, 100, 512 0           conv2d_transpose[0][0]           \n",
      "                                                                 multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 150, 100, 256 1179648     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 150, 100, 256 1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 150, 100, 256 589824      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 150, 100, 256 1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 300, 200, 128 131200      batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 300, 200, 128 16512       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 300, 200, 128 16512       conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 300, 200, 128 0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 300, 200, 128 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 300, 200, 1)  129         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 300, 200, 1)  0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 300, 200, 128 0           batch_normalization_3[0][0]      \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300, 200, 256 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 300, 200, 128 294912      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 300, 200, 128 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 300, 200, 128 147456      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 300, 200, 128 512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 600, 400, 64) 32832       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 600, 400, 64) 4160        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 600, 400, 64) 4160        conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 600, 400, 64) 0           conv2d_18[0][0]                  \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 600, 400, 64) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 600, 400, 1)  65          activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 600, 400, 1)  0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 600, 400, 64) 0           batch_normalization_1[0][0]      \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 600, 400, 128 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 600, 400, 64) 73728       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 600, 400, 64) 256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 600, 400, 64) 36864       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 600, 400, 64) 256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 600, 400, 3)  195         batch_normalization_13[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 7,878,150\n",
      "Trainable params: 7,872,518\n",
      "Non-trainable params: 5,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    /glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /glade/work/ggantos/holodec-ml/holodecml/losses.py:100 unet_loss  *\n        print(\"MAX\", max(y_pred))\n    /glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:503 __iter__  **\n        self._disallow_iteration()\n    /glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:496 _disallow_iteration\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n    /glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:474 _disallow_when_autograph_enabled\n        \" indicate you are trying to use an unsupported feature.\".format(task))\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dc015f3bf99a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    /glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /glade/work/ggantos/holodec-ml/holodecml/losses.py:100 unet_loss  *\n        print(\"MAX\", max(y_pred))\n    /glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:503 __iter__  **\n        self._disallow_iteration()\n    /glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:496 _disallow_iteration\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n    /glade/work/ggantos/ncar_20200417/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:474 _disallow_when_autograph_enabled\n        \" indicate you are trying to use an unsupported feature.\".format(task))\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    np.expand_dims(train_inputs, axis=-1),\n",
    "    train_outputs,\n",
    "    batch_size=8,\n",
    "    epochs=10,\n",
    "    validation_data=(np.expand_dims(valid_inputs, axis=-1), valid_outputs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_outputs_pred = model.predict(np.expand_dims(valid_inputs, axis=-1),\n",
    "                                     batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "valid_outputs_pred_da = xr.DataArray(valid_outputs_pred,\n",
    "                                     coords={\"hid\": np.arange(valid_outputs_pred.shape[0]),\n",
    "                                             \"x\": np.arange(valid_outputs_pred.shape[1]),\n",
    "                                             \"y\": np.arange(valid_outputs_pred.shape[2]),\n",
    "                                             \"output\": [\"p\", \"z\", \"d\"]},\n",
    "                                     dims=(\"hid\", \"x\", \"y\", \"output\"),\n",
    "                                     name=\"valid_pred_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "path_save = \"/glade/p/cisl/aiml/ggantos/holodec/unet/test_jupyter_fixed/\"\n",
    "model.save(path_save, save_format=\"tf\")\n",
    "model.save_weights(path_save + '_weights', save_format='tf')\n",
    "valid_outputs_pred_da.to_netcdf(join(path_save, \"valid_outputs_pred.nc\"))\n",
    "for k in history.history.keys():\n",
    "    np.savetxt(join(path_save, k+\".csv\"), history.history[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_outputs_pred_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_outputs_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_outputs_pred[0, :, :, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_outputs[0, :, :, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 8))\n",
    "plt.imshow(valid_outputs[0, :, :, 0], interpolation='bilinear', cmap=plt.cm.gray, aspect='auto', vmin=0, vmax=1)\n",
    "print(np.sum(valid_outputs[0, :, :, 0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 8))\n",
    "plt.imshow(valid_outputs_pred[0, :, :, 0], interpolation='bilinear', cmap=plt.cm.gray, aspect='auto', vmin=0, vmax=1)\n",
    "print(np.sum(valid_outputs_pred[0, :, :, 0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holodecml.losses import unet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_loss(valid_outputs[0, :, :, 0], valid_outputs_pred[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "bce = valid_outputs[0, :, :, 0] * -tf.math.log(valid_outputs_pred[0, :, :, 0]+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(bce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_2 = (1 - valid_outputs[0, :, :, 0]) * -tf.math.log(1 - valid_outputs_pred[0, :, :, 0]+1e-8)\n",
    "tf.reduce_sum(bce_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 - valid_outputs[0, :, :, 0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((1 - valid_outputs[0, :, :, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "-tf.math.log(np.max(1 - valid_outputs_pred[0, :, :, 0], 1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_outputs_pred[0, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 1.3, 2],\n",
    "             [1, 5.4, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(x, 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x < 1e-8] = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
