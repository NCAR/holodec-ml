{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nervous-study",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/ggantos/miniconda3/envs/micro/lib/python3.6/site-packages/distributed/config.py:63: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config.update(yaml.load(text))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from holodecml.data import load_raw_datasets, load_train_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blocked-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"/glade/p/cisl/aiml/ai4ess_hackathon/holodec/\"\n",
    "num_particles = \"12-25\"\n",
    "output_cols = [\"x\", \"y\", \"z\", \"d\", \"hid\"]\n",
    "subset = False\n",
    "scaler_out = MinMaxScaler()\n",
    "rad = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alike-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_images(images, scaler_in=None):\n",
    "    \"\"\"\n",
    "    Takes in array of images and scales pixel values between 0 and 1\n",
    "    \n",
    "    Args: \n",
    "        images: (np array) Input image data\n",
    "        scaler_in: (dict) Image scaler 'max' and 'min' values\n",
    "        \n",
    "    Returns:\n",
    "        images_scaled: (np array) Input image data scaled between 0 and 1\n",
    "        scaler_in: (dict) Image scaler 'max' and 'min' values\n",
    "    \"\"\"\n",
    "    \n",
    "    if scaler_in is None:\n",
    "        scaler_in = {}\n",
    "        scaler_in[\"min\"] = images.min()\n",
    "        scaler_in[\"max\"] = images.max()\n",
    "    images_scaled = (images.astype(np.float32) - scaler_in[\"min\"])\n",
    "    images_scaled /= (scaler_in[\"max\"] - scaler_in[\"min\"])\n",
    "\n",
    "    return images_scaled, scaler_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demonstrated-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pred_patches(path_preds, path_data, num_particles, output_cols,\n",
    "                       scaler_out=False, subset=False, rad=False, threshold=0.3):\n",
    "    \n",
    "    '''Creates hologram patches centered around true (for train) and predicted (for validation) particles.'''\n",
    "    \n",
    "    # load raw datasets\n",
    "    train_inputs,\\\n",
    "    train_outputs = load_raw_datasets(path_data, num_particles, 'train',\n",
    "                                      output_cols, subset)\n",
    "    valid_inputs,\\\n",
    "    valid_outputs = load_raw_datasets(path_data, num_particles, 'valid',\n",
    "                                      output_cols, subset)\n",
    "    print(\"finished loading raw datasets\")\n",
    "    \n",
    "    # scale images\n",
    "    train_inputs, scaler_in = scale_images(train_inputs)\n",
    "    valid_inputs, _ = scale_images(valid_inputs, scaler_in)\n",
    "    \n",
    "    # use scipy.ndimage.label and scipy.ndimage.center_of_mass to extract predicted particles\n",
    "    preds = xr.open_dataset(path_preds + \"train_outputs_pred.nc\").to_array()[0, :, :, :, 0]\n",
    "    coms = []\n",
    "    for pred in preds:\n",
    "        holo_label, label_count = label(np.where(pred >= threshold, 1, 0))\n",
    "        com = np.array(center_of_mass(pred, holo_label, np.arange(1, label_count+1)))\n",
    "        coms.append(np.round(com).astype(int))\n",
    "    print(\"Finished extracting coms from train preds\")\n",
    "\n",
    "    # calculate bin factor from inputs to the jnet model\n",
    "    x_factor = train_inputs.shape[1] // preds.x.shape[0]\n",
    "    y_factor = train_inputs.shape[2] // preds.y.shape[0]\n",
    "    \n",
    "    # exise patches from zero-padded holograms for validation set\n",
    "    train_patches = []\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    train_hids = []\n",
    "    for hid in train_outputs[\"hid\"].unique().astype(int):\n",
    "        input_hid = np.pad(train_inputs[hid-1], rad)\n",
    "        com = coms[hid-1]\n",
    "        for c in com:\n",
    "            idx_x, idx_y = int(c[0]*x_factor)+rad, int(c[1]*y_factor)+rad            \n",
    "            patch = input_hid[idx_x-rad:idx_x+rad+1, idx_y-rad:idx_y+rad+1]\n",
    "            train_patches.append(patch)\n",
    "            train_x.append(idx_x-rad)\n",
    "            train_y.append(idx_y-rad)\n",
    "            train_hids.append(hid)\n",
    "    train_patches = np.stack(train_patches)\n",
    "    train_x = np.array(train_x).reshape(-1,1)\n",
    "    train_y = np.array(train_y).reshape(-1,1)\n",
    "    train_hids = np.array(train_hids).reshape(-1,1)\n",
    "    train_hids = np.hstack((train_hids, train_x, train_y))\n",
    "    print(\"Finished extracting train patches\")\n",
    "\n",
    "    # use scipy.ndimage.label and scipy.ndimage.center_of_mass to extract predicted particles\n",
    "    preds = xr.open_dataset(path_preds + \"valid_outputs_pred.nc\").to_array()[0, :, :, :, 0]\n",
    "    coms = []\n",
    "    for pred in preds:\n",
    "        holo_label, label_count = label(np.where(pred >= threshold, 1, 0))\n",
    "        com = np.array(center_of_mass(pred, holo_label, np.arange(1, label_count+1)))\n",
    "        coms.append(np.round(com).astype(int))\n",
    "    print(\"Finished extracting coms from valid preds\")\n",
    "\n",
    "    # calculate bin factor from inputs to the jnet model\n",
    "    x_factor = valid_inputs.shape[1] // preds.x.shape[0]\n",
    "    y_factor = valid_inputs.shape[2] // preds.y.shape[0]\n",
    "    \n",
    "    # exise patches from zero-padded holograms for validation set\n",
    "    valid_patches = []\n",
    "    valid_x = []\n",
    "    valid_y = []\n",
    "    valid_hids = []\n",
    "    for hid in valid_outputs[\"hid\"].unique().astype(int):\n",
    "        input_hid = np.pad(valid_inputs[hid-1], rad)\n",
    "        com = coms[hid-1]\n",
    "        for c in com:\n",
    "            idx_x, idx_y = int(c[0]*x_factor)+rad, int(c[1]*y_factor)+rad\n",
    "            patch = input_hid[idx_x-rad:idx_x+rad+1, idx_y-rad:idx_y+rad+1]\n",
    "            valid_patches.append(patch)\n",
    "            valid_x.append(idx_x-rad)\n",
    "            valid_y.append(idx_y-rad)\n",
    "            valid_hids.append(hid)\n",
    "    valid_patches = np.stack(valid_patches)\n",
    "    valid_x = np.array(valid_x).reshape(-1,1)\n",
    "    valid_y = np.array(valid_y).reshape(-1,1)\n",
    "    valid_hids = np.array(valid_hids).reshape(-1,1)\n",
    "    valid_hids = np.hstack((valid_hids, valid_x, valid_y))\n",
    "    print(\"Finished extracting valid patches\")\n",
    "    \n",
    "    train_outputs = train_outputs.drop(['hid'], axis=1)\n",
    "    train_outputs = scaler_out.fit_transform(train_outputs)\n",
    "    \n",
    "    valid_outputs = valid_outputs.drop(['hid'], axis=1)\n",
    "    valid_outputs = scaler_out.transform(valid_outputs)\n",
    "    \n",
    "    return train_patches, train_hids, train_outputs, valid_patches, valid_hids, valid_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-lending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading raw datasets\n",
      "Finished extracting coms from train preds\n"
     ]
    }
   ],
   "source": [
    "path_preds = \"/glade/p/cisl/aiml/ggantos/holodec/unet/j_10_fulldata/\"\n",
    "\n",
    "train_patches,\\\n",
    "train_patch_hids,\\\n",
    "train_patch_outputs,\\\n",
    "valid_patches,\\\n",
    "valid_patch_hids,\\\n",
    "valid_patch_outputs = load_pred_patches(path_preds,\n",
    "                                        path_data,\n",
    "                                        num_particles,\n",
    "                                        output_cols,\n",
    "                                        scaler_out,\n",
    "                                        subset,\n",
    "                                        rad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = \"/glade/p/cisl/aiml/ggantos/holodec/jnet/zd/rad128/\"\n",
    "new_model = tf.keras.models.load_model(path_model + \"cnn.h5\")\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_patch_outputs_pred = new_model.predict(valid_patches)\n",
    "valid_patch_outputs_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = np.hstack((valid_patch_hids, valid_patch_outputs_pred))\n",
    "df_pred = pd.DataFrame(data=df_pred, columns=[\"hid\", \"x\", \"y\", \"z\", \"d\"])\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs,\\\n",
    "train_hids,\\\n",
    "train_outputs,\\\n",
    "valid_inputs,\\\n",
    "valid_hids,\\\n",
    "valid_outputs = load_train_patches(path_data,\n",
    "                                   num_particles,\n",
    "                                   output_cols,\n",
    "                                   scaler_out,\n",
    "                                   subset,\n",
    "                                   rad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_hids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = np.hstack((valid_hids, valid_outputs))\n",
    "df_true = pd.DataFrame(data=df_true, columns=[\"hid\", \"x\", \"y\", \"z\", \"d\"])\n",
    "df_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_counts = []\n",
    "loss_maxima = []\n",
    "loss_zs = []\n",
    "loss_ds = []\n",
    "true_z = []\n",
    "true_d = []\n",
    "\n",
    "for hid in df_pred[\"hid\"].unique().astype(int):\n",
    "    df_true_hid = df_true.loc[df_true['hid'] == hid]\n",
    "    df_pred_hid = df_pred.loc[df_pred['hid'] == hid]\n",
    "    dist_x = (df_true_hid['x'].to_numpy() - df_pred_hid['x'].to_numpy()[:, np.newaxis]) ** 2\n",
    "    dist_y = (df_true_hid['y'].to_numpy() - df_pred_hid['y'].to_numpy()[:, np.newaxis]) ** 2\n",
    "    dist_z = (df_true_hid['z'].to_numpy() - df_pred_hid['z'].to_numpy()[:, np.newaxis]) ** 2\n",
    "    dist_d = (df_true_hid['d'].to_numpy() - df_pred_hid['d'].to_numpy()[:, np.newaxis]) ** 2\n",
    "    dist_xy = dist_x + dist_y\n",
    "    matches = np.argmin(dist_xy, axis=1)\n",
    "    loss_counts.append((df_true_hid.shape[0] - df_pred_hid.shape[0]) ** 2)\n",
    "    loss_maxima.append(df_true_hid.shape[0] - df_pred_hid.shape[0])\n",
    "    loss_zs.append(np.sum(dist_z[(np.arange(dist_z.shape[0]), matches)]))\n",
    "    loss_ds.append(np.sum(dist_d[(np.arange(dist_d.shape[0]), matches)]))\n",
    "    true_z.append(df_true_hid[\"z\"].to_numpy()[matches])\n",
    "    true_d.append(df_true_hid[\"d\"].to_numpy()[matches])\n",
    "loss_count = math.sqrt(sum(loss_counts))\n",
    "loss_max = max(loss_maxima)\n",
    "loss_z = math.sqrt(sum(loss_zs))\n",
    "loss_d = math.sqrt(sum(loss_ds))\n",
    "true_z = np.hstack(true_z)\n",
    "true_d = np.hstack(true_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 10000\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(df_true.loc[df_true['hid'] == h, \"x\"],\n",
    "            df_true.loc[df_true['hid'] == h, \"y\"],\n",
    "            df_true.loc[df_true['hid'] == h, \"d\"]*35 ** 2,\n",
    "            df_true.loc[df_true['hid'] == h, \"z\"],\n",
    "            edgecolors=\"blue\",\n",
    "            vmin=df_true[\"z\"].min(),\n",
    "            vmax=df_true[\"z\"].max(),\n",
    "            cmap=\"cool\",\n",
    "            label=\"True\",\n",
    "            zorder=1)\n",
    "plt.scatter(df_pred.loc[df_pred['hid'] == h, \"x\"],\n",
    "            df_pred.loc[df_pred['hid'] == h, \"y\"],\n",
    "            df_pred.loc[df_pred['hid'] == h, \"d\"]*35 ** 2,\n",
    "            df_pred.loc[df_pred['hid'] == h, \"z\"],\n",
    "            edgecolors=\"magenta\",\n",
    "            vmin=df_pred[\"z\"].min(),\n",
    "            vmax=df_pred[\"z\"].max(),\n",
    "            cmap=\"cool\",\n",
    "            label=\"Predicted\",\n",
    "            zorder=2)\n",
    "plt.colorbar().set_label(label=\"z-axis particle position (µm)\", size=16)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-uganda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(true_d, df_pred[\"d\"])\n",
    "print(\"r-squared = {:.3f}\".format(r2_score(true_d, df_pred[\"d\"])))\n",
    "plt.axis('square')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(true_z, df_pred[\"z\"])\n",
    "print(\"r-squared = {:.3f}\".format(r2_score(true_z, df_pred[\"z\"])))\n",
    "plt.axis('square')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_count)\n",
    "print(loss_max)\n",
    "print(loss_z)\n",
    "print(loss_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-trigger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
