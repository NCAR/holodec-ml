{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import yaml\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_x = 2.96e-06\n",
    "scale_y = 2.96e-06\n",
    "scale_z = 1.00e-06\n",
    "scale_d = 2.96e-06\n",
    "\n",
    "\n",
    "class Threshold:\n",
    "\n",
    "    def __init_(self):\n",
    "        self.n = None\n",
    "\n",
    "    def load_dist_matrix(self, h_idx, coordinates):\n",
    "\n",
    "        vol = np.array(coordinates[h_idx], dtype=float)\n",
    "\n",
    "        # apply the relevant scale transformation\n",
    "        vol[:, 0] *= scale_x\n",
    "        vol[:, 1] *= scale_y\n",
    "        vol[:, 2] *= scale_z\n",
    "        vol[:, 3] *= scale_d\n",
    "\n",
    "        self.vol = vol\n",
    "\n",
    "        self.dist_matrix = distance_matrix(vol, vol)\n",
    "        self.n = self.dist_matrix.shape[0]\n",
    "\n",
    "    def search(self, threshold):\n",
    "        results = {}\n",
    "        for label in range(self.n):\n",
    "            # Find experiments at or below the threshold\n",
    "            members = np.where(self.dist_matrix[label] <= threshold)[0]\n",
    "            results[label] = [x for x in members if x != label]\n",
    "        results = sorted([\n",
    "            [len(x), i, x] for (i, x) in results.items()\n",
    "        ])\n",
    "        return results  # sorted(results, reverse=True)\n",
    "\n",
    "    def Cluster(self, threshold):\n",
    "        # Use Leader clustering\n",
    "        true_singletons = []\n",
    "        false_singletons = []\n",
    "        clusters = []\n",
    "        seen = set()\n",
    "        for (size, experiment, members) in self.search(threshold):\n",
    "            if experiment in seen:\n",
    "                # Can't use a centroid which is already assigned\n",
    "                continue\n",
    "            seen.add(experiment)\n",
    "            # Figure out which ones haven't yet been assigned\n",
    "            unassigned = set(members) - seen\n",
    "            if not unassigned:\n",
    "                false_singletons.append(experiment)\n",
    "                continue\n",
    "            # this is a new cluster\n",
    "            clusters.append((experiment, unassigned))\n",
    "            seen.update(unassigned)\n",
    "\n",
    "        seen = []\n",
    "        for a, b in clusters:\n",
    "            seen.append(a)\n",
    "            for c in b:\n",
    "                if c not in seen:\n",
    "                    seen.append(c)\n",
    "\n",
    "        not_clustered = set(range(self.n)) - set(seen)\n",
    "        return sorted(clusters, key=lambda x: -len(x[1])), list(not_clustered)\n",
    "\n",
    "\n",
    "scales = [scale_x, scale_y, scale_z, scale_d]\n",
    "\n",
    "\n",
    "def diameter_average(coors, centroid, clusters):\n",
    "    centroid = [coors[centroid]]\n",
    "    centroid += [coors[x] for x in clusters]\n",
    "    centroid = np.array(centroid).astype(float)\n",
    "    centroid = np.average(centroid, axis=0)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "def distance(x, y):\n",
    "    return math.sqrt(sum([(scales[i] * x[i] - scales[i] * y[i])**2 for i, _ in enumerate(x)]))\n",
    "\n",
    "\n",
    "def create_table(distance_threshold=0.001, \n",
    "                 true_coordinates=None, \n",
    "                 pred_coordinates=None,  \n",
    "                 match=False):\n",
    "\n",
    "    mapping_table = defaultdict(list)\n",
    "    \n",
    "    for h_idx in tqdm.tqdm(sorted(list(true_coordinates.keys()))):\n",
    "\n",
    "        if match:\n",
    "            if h_idx not in pred_coordinates:\n",
    "                for p in true_coordinates[h_idx]:\n",
    "                    mapping_table[h_idx].append([\" \".join(map(str, p)), None])\n",
    "                continue\n",
    "\n",
    "            if len(pred_coordinates[h_idx]) == 0:\n",
    "                for p in true_coordinates[h_idx]:\n",
    "                    mapping_table[h_idx].append([\" \".join(map(str, p)), None])\n",
    "                continue\n",
    "\n",
    "        t = Threshold()\n",
    "        t.load_dist_matrix(h_idx, pred_coordinates)\n",
    "        clusters, unassigned = t.Cluster(distance_threshold)\n",
    "\n",
    "        # Create numpy arrays\n",
    "        pred_r_centroids = np.array([diameter_average(\n",
    "            pred_coordinates[h_idx], centroid, members) for centroid, members in clusters]).astype(float)\n",
    "        pred_r_not_matched = np.array(\n",
    "            [pred_coordinates[h_idx][idx] for idx in unassigned]).astype(float)\n",
    "        if pred_r_centroids.shape[0] > 0:\n",
    "            if pred_r_not_matched.shape[0] > 0:\n",
    "                pred_r = np.concatenate([pred_r_centroids, pred_r_not_matched])\n",
    "            else:\n",
    "                pred_r = pred_r_centroids\n",
    "        else:\n",
    "            pred_r = pred_r_not_matched\n",
    "\n",
    "        if not match:\n",
    "            mapping_table[h_idx] = [list(x) for x in pred_r]\n",
    "            continue\n",
    "            \n",
    "        # Match the clustered particles against the true particles (if they exist)\n",
    "        mapping_table[\"rmse\"] = {}\n",
    "        true_r = np.array(true_coordinates[h_idx]).astype(float)\n",
    "        \n",
    "        # Compute the distance matrix b/t the two datasets --> pandas df\n",
    "        result_dict = defaultdict(list)\n",
    "        for k1, x in enumerate(pred_r):\n",
    "            for k2, y in enumerate(true_r):\n",
    "                error = distance(x, y)\n",
    "                result_dict[\"pred_id\"].append(k1)\n",
    "                result_dict[\"true_id\"].append(k2)\n",
    "                result_dict[\"pred_coor\"].append(\n",
    "                    \" \".join([str(xx) for xx in list(x)]))\n",
    "                result_dict[\"true_coor\"].append(\n",
    "                    \" \".join([str(yy) for yy in list(y)]))\n",
    "                result_dict[\"error\"].append(np.mean(np.abs(error)))\n",
    "        df = pd.DataFrame(result_dict)\n",
    "        # add to the mapping table\n",
    "        pred_seen = []\n",
    "        true_seen = []\n",
    "        error = []\n",
    "        while True:\n",
    "            c1 = df[\"true_id\"].isin(true_seen)\n",
    "            c2 = df[\"pred_id\"].isin(pred_seen)\n",
    "            c = c1 | c2\n",
    "            if c.sum() == df.shape[0]:\n",
    "                break\n",
    "            smallest_error = df[~c][\"error\"] == min(df[~c][\"error\"])\n",
    "            error.append(list(df[~c][smallest_error][\"error\"])[0])\n",
    "            true_id = list(df[~c][smallest_error][\"true_id\"])[0]\n",
    "            pred_id = list(df[~c][smallest_error][\"pred_id\"])[0]\n",
    "            pred_seen.append(pred_id)\n",
    "            true_seen.append(true_id)\n",
    "\n",
    "            if match:\n",
    "                pred_n = list(df[~c][smallest_error][\"pred_coor\"])[0]\n",
    "                true_n = list(df[~c][smallest_error][\"true_coor\"])[0]\n",
    "                mapping_table[h_idx].append([true_n, pred_n])\n",
    "\n",
    "        if match:\n",
    "            true_unmatched = list(\n",
    "                set(df[\"true_coor\"].unique()) - set([x[0] for x in mapping_table[h_idx]]))\n",
    "            pred_unmatched = list(\n",
    "                set(df[\"pred_coor\"].unique()) - set([x[1] for x in mapping_table[h_idx]]))\n",
    "            for p in true_unmatched:\n",
    "                mapping_table[h_idx].append([p, None])\n",
    "            for p in pred_unmatched:\n",
    "                mapping_table[h_idx].append([None, p])\n",
    "\n",
    "        mapping_table[\"rmse\"][h_idx] = error\n",
    "\n",
    "    return mapping_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config/unet_propagation.yml\") as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = conf[\"save_loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_save_loc = conf[\"inference\"][\"data_set\"][\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_preds = os.path.join(save_loc, inf_save_loc, \"propagated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = conf[\"inference\"][\"distance_threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = glob.glob(os.path.join(path_to_preds, \"*txt\"))\n",
    "preds = [x for x in fns if \"pred\" in x.split(\"/\")[-1]]\n",
    "truth = [x for x in fns if \"true\" in x.split(\"/\")[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_coordinates = defaultdict(list)\n",
    "pred_coordinates = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in preds:\n",
    "    with open(fn, \"r\") as fid:\n",
    "        for line in fid.readlines():\n",
    "            h, x, y, z, d = list(map(int, line.split(\" \")))\n",
    "            pred_coordinates[h].append([x,y,z,d])\n",
    "for fn in truth:\n",
    "    with open(fn, \"r\") as fid:\n",
    "        for line in fid.readlines():\n",
    "            h, x, y, z, d = list(map(int, line.split(\" \")))\n",
    "            true_coordinates[h].append([x,y,z,d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "coors_table = create_table(\n",
    "    distance_threshold=distance_threshold,\n",
    "    true_coordinates=true_coordinates,\n",
    "    pred_coordinates=pred_coordinates,\n",
    "    match=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_loc, inf_save_loc, f\"clustered_matched_{str(distance_threshold)}.pkl\"), \"wb\") as fid:\n",
    "    joblib.dump(coors_table, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
