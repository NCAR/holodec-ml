{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.fft\n",
    "import subprocess\n",
    "import logging\n",
    "import random\n",
    "import shutil\n",
    "import psutil\n",
    "import scipy\n",
    "import torch\n",
    "import copy\n",
    "import yaml\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from holodecml.data import PickleReader, UpsamplingReader\n",
    "from holodecml.propagation import InferencePropagator\n",
    "from holodecml.transforms import LoadTransformations\n",
    "from holodecml.models import load_model\n",
    "from holodecml.losses import load_loss\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_ncpus = len(psutil.Process().cpu_affinity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set seeds for reproducibility\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../results/generator/model.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "tile_size = int(conf[\"data\"][\"tile_size\"])\n",
    "step_size = int(conf[\"data\"][\"step_size\"])\n",
    "data_path = conf[\"data\"][\"output_path\"]\n",
    "data_path_raw = conf[\"data\"][\"output_path_raw\"]\n",
    "\n",
    "total_positive = int(conf[\"data\"][\"total_positive\"])\n",
    "total_negative = int(conf[\"data\"][\"total_negative\"])\n",
    "total_examples = int(conf[\"data\"][\"total_training\"])\n",
    "\n",
    "transform_mode = \"None\" if \"transform_mode\" not in conf[\"data\"] else conf[\"data\"][\"transform_mode\"]\n",
    "config_ncpus = int(conf[\"data\"][\"cores\"])\n",
    "use_cached = False if \"use_cached\" not in conf[\"data\"] else conf[\"data\"][\"use_cached\"]\n",
    "\n",
    "name_tag = f\"{tile_size}_{step_size}_{total_positive}_{total_negative}_{total_examples}_{transform_mode}\"\n",
    "fn_train = f\"{data_path}/training_{name_tag}.pkl\"\n",
    "fn_valid = f\"{data_path}/validation_{name_tag}.pkl\"\n",
    "fn_train_raw = f\"{data_path_raw}/training_{name_tag}.pkl\"\n",
    "fn_valid_raw = f\"{data_path_raw}/validation_{name_tag}.pkl\"\n",
    "\n",
    "output_path = conf[\"data\"][\"output_path\"]\n",
    "\n",
    "train_batch_size = 16\n",
    "valid_batch_size = 16\n",
    "\n",
    "latent_dim = 128\n",
    "img_shape = (1, tile_size, tile_size)\n",
    "\n",
    "d_learning_rate = 0.0004\n",
    "g_learning_rate = 0.0001\n",
    "\n",
    "b1 = 0.0\n",
    "b2 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "data_device = torch.device(\"cpu\") if \"device\" not in conf[\"data\"] else conf[\"data\"][\"device\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing transforms\n",
    "if \"Normalize\" in conf[\"transforms\"][\"training\"]:\n",
    "    conf[\"transforms\"][\"validation\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "    conf[\"transforms\"][\"inference\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "\n",
    "train_transforms = LoadTransformations(conf[\"transforms\"][\"training\"])\n",
    "valid_transforms = LoadTransformations(conf[\"transforms\"][\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_dataset = PickleReader(\n",
    "    fn_train,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_synthetic_dataset = PickleReader(\n",
    "    fn_valid,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    train_synthetic_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0, #available_ncpus//2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    test_synthetic_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_dataset = PickleReader(\n",
    "    fn_train_raw,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_holodec_dataset = PickleReader(\n",
    "    fn_valid_raw,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_loader = torch.utils.data.DataLoader(\n",
    "    train_holodec_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_holodec_loader = torch.utils.data.DataLoader(\n",
    "    test_holodec_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = smp.Linknet(\n",
    "    encoder_name=\"xception\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset),\n",
    "    activation = \"tanh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /glade/work/schreck/py37/lib/python3.7/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.L1Loss()\n",
    "perceptual_alex = lpips.LPIPS(net='alex').cuda()\n",
    "\n",
    "if is_cuda:\n",
    "    generator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    \n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, generator.parameters()), \n",
    "    lr=g_learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_scheduler = ReduceLROnPlateau(\n",
    "#         optimizer_G,\n",
    "#         patience=1,\n",
    "#         min_lr=g_learning_rate * 1e-3,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "\n",
    "T_0=10\n",
    "T_mult=1\n",
    "eta_min=0.001\n",
    "last_epoch=-1\n",
    "\n",
    "lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer_G, \n",
    "    T_0=T_0, \n",
    "    T_mult=T_mult,\n",
    "    eta_min=eta_min,\n",
    "    last_epoch=last_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss 0.142729 train_percept 0.265159 lr 0.00012193727965514856: 100%|██████████| 500/500 [05:59<00:00,  1.39it/s]\n",
      "Epoch 0 valid_loss 0.094591 valid_percept 0.464260: 100%|██████████| 100/100 [00:26<00:00,  3.79it/s]\n",
      "Epoch 1 train_loss 0.079879 train_percept 0.084073 lr 0.0001857762320395614: 100%|██████████| 500/500 [06:20<00:00,  1.31it/s] \n",
      "Epoch 1 valid_loss 0.065890 valid_percept 0.438006: 100%|██████████| 100/100 [00:26<00:00,  3.79it/s]\n",
      "Epoch 2 train_loss 0.059784 train_percept 0.043496 lr 0.00028526794452815325: 100%|██████████| 500/500 [06:21<00:00,  1.31it/s]\n",
      "Epoch 2 valid_loss 0.049015 valid_percept 0.509616: 100%|██████████| 100/100 [00:26<00:00,  3.77it/s]\n",
      "Epoch 3 train_loss 0.044944 train_percept 0.030726 lr 0.00041067347510301863: 100%|██████████| 500/500 [06:22<00:00,  1.31it/s]\n",
      "Epoch 3 valid_loss 0.043115 valid_percept 0.449869: 100%|██████████| 100/100 [00:26<00:00,  3.85it/s]\n",
      "Epoch 4 train_loss 0.035364 train_percept 0.019876 lr 0.0005497172566797806: 100%|██████████| 500/500 [06:20<00:00,  1.31it/s] \n",
      "Epoch 4 valid_loss 0.031221 valid_percept 0.497130: 100%|██████████| 100/100 [00:26<00:00,  3.83it/s]\n",
      "Epoch 5 train_loss 0.031966 train_percept 0.015904 lr 0.0006887887151427144: 100%|██████████| 500/500 [06:21<00:00,  1.31it/s]\n",
      "Epoch 5 valid_loss 0.033157 valid_percept 0.456920: 100%|██████████| 100/100 [00:27<00:00,  3.69it/s]\n",
      "Epoch 6 train_loss 0.031194 train_percept 0.015670 lr 0.00081427456716964: 100%|██████████| 500/500 [06:20<00:00,  1.31it/s]  \n",
      "Epoch 6 valid_loss 0.051350 valid_percept 0.542728: 100%|██████████| 100/100 [00:25<00:00,  3.87it/s]\n",
      "Epoch 7 train_loss 0.027882 train_percept 0.013429 lr 0.0009138913832528203: 100%|██████████| 500/500 [06:19<00:00,  1.32it/s]\n",
      "Epoch 7 valid_loss 0.020951 valid_percept 0.423485: 100%|██████████| 100/100 [00:25<00:00,  3.93it/s]\n",
      "Epoch 8 train_loss 0.023979 train_percept 0.010328 lr 0.0009778879753628638: 100%|██████████| 500/500 [06:20<00:00,  1.32it/s]\n",
      "Epoch 8 valid_loss 0.023998 valid_percept 0.469149: 100%|██████████| 100/100 [00:25<00:00,  3.88it/s]\n",
      "Epoch 9 train_loss 0.025427 train_percept 0.010553 lr 0.0009999999111735634: 100%|██████████| 500/500 [06:20<00:00,  1.31it/s]\n",
      "Epoch 9 valid_loss 0.023735 valid_percept 0.505973: 100%|██████████| 100/100 [00:25<00:00,  3.86it/s]\n",
      "Epoch 10 train_loss 0.019615 train_percept 0.007173 lr 0.00012193727965514856: 100%|██████████| 500/500 [06:19<00:00,  1.32it/s]\n",
      "Epoch 10 valid_loss 0.018105 valid_percept 0.447842: 100%|██████████| 100/100 [00:25<00:00,  3.93it/s]\n",
      "Epoch 11 train_loss 0.019261 train_percept 0.006666 lr 0.0001857762320395614: 100%|██████████| 500/500 [06:19<00:00,  1.32it/s] \n",
      "Epoch 11 valid_loss 0.018843 valid_percept 0.445661: 100%|██████████| 100/100 [00:25<00:00,  3.93it/s]\n",
      "Epoch 12 train_loss 0.018656 train_percept 0.006390 lr 0.00028526794452815314: 100%|██████████| 500/500 [06:21<00:00,  1.31it/s]\n",
      "Epoch 12 valid_loss 0.018856 valid_percept 0.479595: 100%|██████████| 100/100 [00:27<00:00,  3.60it/s]\n",
      "Epoch 13 train_loss 0.019956 train_percept 0.006838 lr 0.0004106734751030185: 100%|██████████| 500/500 [06:32<00:00,  1.27it/s] \n",
      "Epoch 13 valid_loss 0.017317 valid_percept 0.490456: 100%|██████████| 100/100 [00:25<00:00,  3.90it/s]\n",
      "Epoch 14 train_loss 0.019219 train_percept 0.006704 lr 0.0004662339452217901:  41%|████      | 203/500 [02:37<03:49,  1.29it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9b7df09d1f06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "batches_per_epoch = 500\n",
    "valid_batches_per_epoch = 100\n",
    "stopping_patience = 4\n",
    "metric = \"valid_loss\"\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if is_cuda else torch.FloatTensor\n",
    "\n",
    "results = defaultdict(list)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    ### Train\n",
    "    dual_iter = tqdm.tqdm(\n",
    "        enumerate(iter(train_holodec_loader)),\n",
    "        total = batches_per_epoch,\n",
    "        leave = True\n",
    "    )\n",
    "    \n",
    "    train_results = defaultdict(list)\n",
    "    for i, (holo_img, holo_label) in dual_iter:\n",
    "                                \n",
    "        optimizer_G.zero_grad()\n",
    "        requires_grad(generator, True)\n",
    "        requires_grad(perceptual_alex, False)\n",
    "\n",
    "        # Configure input\n",
    "        batched_imgs = Variable(holo_img.type(Tensor))\n",
    "        #synthethic_imgs = Variable(synth_img.type(Tensor))\n",
    "        #batched_imgs = torch.cat([real_imgs, synthethic_imgs], 0)\n",
    "        \n",
    "        pred_imgs = generator(batched_imgs)\n",
    "        loss = adversarial_loss(pred_imgs, batched_imgs)\n",
    "        perceptual_score = perceptual_alex(pred_imgs, batched_imgs).mean()\n",
    "        \n",
    "        train_results[\"loss\"].append(loss.item())\n",
    "        train_results[\"lpips\"].append(perceptual_score.item())\n",
    "        \n",
    "        to_print = f'Epoch {epoch} train_loss {np.mean(train_results[\"loss\"]):.6f}'\n",
    "        to_print += f' train_percept {np.mean(train_results[\"lpips\"]):.6f}'\n",
    "        to_print += f' lr {optimizer_G.param_groups[0][\"lr\"]}'\n",
    "        dual_iter.set_description(to_print)\n",
    "        dual_iter.update()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        lr_scheduler.step(epoch + i / batches_per_epoch)\n",
    "            \n",
    "        if i == batches_per_epoch and i > 0:\n",
    "            break\n",
    "    \n",
    "    ### Validation\n",
    "    requires_grad(generator, False)\n",
    "    requires_grad(perceptual_alex, False)\n",
    "\n",
    "    dual_iter = tqdm.tqdm(\n",
    "        enumerate(iter(test_holodec_loader)),\n",
    "        total = valid_batches_per_epoch, \n",
    "        leave = True)\n",
    "    \n",
    "    valid_results = defaultdict(list)\n",
    "    for i, (holo_img, holo_label) in dual_iter:\n",
    "                    \n",
    "        # Configure input\n",
    "        batched_imgs = Variable(holo_img.type(Tensor))\n",
    "        pred_images = generator(batched_imgs)\n",
    "        loss = adversarial_loss(pred_images, batched_imgs)\n",
    "        perceptual_score = perceptual_alex(pred_imgs, batched_imgs).mean()\n",
    "        \n",
    "        valid_results[\"loss\"].append(loss.item())\n",
    "        valid_results[\"lpips\"].append(perceptual_score.item())\n",
    "        \n",
    "        to_print = f'Epoch {epoch} valid_loss {np.mean(valid_results[\"loss\"]):.6f}'\n",
    "        to_print += f' valid_percept {np.mean(valid_results[\"lpips\"]):.6f}'\n",
    "        dual_iter.set_description(to_print)\n",
    "        dual_iter.update()\n",
    "        \n",
    "        if i == valid_batches_per_epoch and i > 0:\n",
    "            break\n",
    "         \n",
    "    # Save the last validation batch images\n",
    "    save_image(batched_imgs.data[:16], f'{conf[\"save_loc\"]}/images/real_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(pred_images.data[:16], f'{conf[\"save_loc\"]}/images/pred_{epoch}.png', nrow=4, normalize=True)\n",
    "            \n",
    "    results[\"epoch\"].append(epoch)\n",
    "    results[\"train_loss\"].append(np.mean(train_results[\"loss\"]))\n",
    "    results[\"train_perception\"].append(np.mean(train_results[\"lpips\"]))\n",
    "    results[\"valid_loss\"].append(np.mean(valid_results[\"loss\"]))\n",
    "    results[\"valid_perception\"].append(np.mean(valid_results[\"lpips\"]))\n",
    "    \n",
    "    #print_str = f\"Epoch {epoch}\"\n",
    "    #print_str += f' train_loss {np.mean(train_results[\"loss\"])} valid_loss {np.mean(valid_results[\"loss\"])}'\n",
    "    #print(print_str)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results).reset_index()\n",
    "\n",
    "    # Save the dataframe to disk\n",
    "    df.to_csv(f'{conf[\"save_loc\"]}/training_log.csv', index=False)\n",
    "    \n",
    "    # If the best model, save to disk\n",
    "    # Save the model if its the best so far.\n",
    "    if results[metric][-1] == min(results[metric]):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'loss': results[\"valid_loss\"][-1]\n",
    "        }\n",
    "        torch.save(state_dict, f'{conf[\"save_loc\"]}/best.pt')\n",
    "    \n",
    "    # Lower the learning rate\n",
    "    #lr_scheduler.step(np.mean(valid_results[\"loss\"]))\n",
    "    \n",
    "    # Save some images\n",
    "    #save_image(synthethic_imgs.data[:16], f'{conf[\"save_loc\"]}/images/synth_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(batched_imgs.data[:16], f'{conf[\"save_loc\"]}/images/real_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(pred_images.data[:16], f'{conf[\"save_loc\"]}/images/pred_{epoch}.png', nrow=4, normalize=True)\n",
    "    \n",
    "    # Early stopping\n",
    "    best_epoch = [i for i, j in enumerate(\n",
    "        results[metric]) if j == min(results[metric])][0]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= stopping_patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad(generator, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "genn = iter(test_synthetic_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(genn)[0].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_imgs = generator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(batch.data[:16], f'{conf[\"save_loc\"]}/images/synth_{epoch}.png', nrow=4, normalize=True)\n",
    "save_image(pred_imgs.data[:16], f'{conf[\"save_loc\"]}/images/synth_pred_{epoch}.png', nrow=4, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
