{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.fft\n",
    "import subprocess\n",
    "import logging\n",
    "import random\n",
    "import shutil\n",
    "import psutil\n",
    "import scipy\n",
    "import torch\n",
    "import copy\n",
    "import yaml\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from holodecml.data import PickleReader, UpsamplingReader\n",
    "from holodecml.propagation import InferencePropagator\n",
    "from holodecml.transforms import LoadTransformations\n",
    "from holodecml.models import load_model\n",
    "from holodecml.losses import load_loss\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_ncpus = len(psutil.Process().cpu_affinity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set seeds for reproducibility\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../results/generator/model.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "tile_size = int(conf[\"data\"][\"tile_size\"])\n",
    "step_size = int(conf[\"data\"][\"step_size\"])\n",
    "data_path = conf[\"data\"][\"output_path\"]\n",
    "data_path_raw = conf[\"data\"][\"output_path_raw\"]\n",
    "\n",
    "total_positive = int(conf[\"data\"][\"total_positive\"])\n",
    "total_negative = int(conf[\"data\"][\"total_negative\"])\n",
    "total_examples = int(conf[\"data\"][\"total_training\"])\n",
    "\n",
    "transform_mode = \"None\" if \"transform_mode\" not in conf[\"data\"] else conf[\"data\"][\"transform_mode\"]\n",
    "config_ncpus = int(conf[\"data\"][\"cores\"])\n",
    "use_cached = False if \"use_cached\" not in conf[\"data\"] else conf[\"data\"][\"use_cached\"]\n",
    "\n",
    "name_tag = f\"{tile_size}_{step_size}_{total_positive}_{total_negative}_{total_examples}_{transform_mode}\"\n",
    "fn_train = f\"{data_path}/training_{name_tag}.pkl\"\n",
    "fn_valid = f\"{data_path}/validation_{name_tag}.pkl\"\n",
    "fn_train_raw = f\"{data_path_raw}/training_{name_tag}.pkl\"\n",
    "fn_valid_raw = f\"{data_path_raw}/validation_{name_tag}.pkl\"\n",
    "\n",
    "output_path = conf[\"data\"][\"output_path\"]\n",
    "\n",
    "train_batch_size = 16\n",
    "valid_batch_size = 16\n",
    "\n",
    "latent_dim = 128\n",
    "img_shape = (1, tile_size, tile_size)\n",
    "\n",
    "d_learning_rate = 0.0004\n",
    "g_learning_rate = 0.0001\n",
    "\n",
    "b1 = 0.0\n",
    "b2 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "data_device = torch.device(\"cpu\") if \"device\" not in conf[\"data\"] else conf[\"data\"][\"device\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing transforms\n",
    "if \"Normalize\" in conf[\"transforms\"][\"training\"]:\n",
    "    conf[\"transforms\"][\"validation\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "    conf[\"transforms\"][\"inference\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "\n",
    "train_transforms = LoadTransformations(conf[\"transforms\"][\"training\"])\n",
    "valid_transforms = LoadTransformations(conf[\"transforms\"][\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_dataset = PickleReader(\n",
    "    fn_train,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_synthetic_dataset = PickleReader(\n",
    "    fn_valid,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    train_synthetic_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0, #available_ncpus//2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    test_synthetic_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holo_conf = copy.deepcopy(conf)\n",
    "# holo_conf[\"data\"][\"data_path\"] = holo_conf[\"data\"][\"raw_data\"]\n",
    "\n",
    "\n",
    "# train_holodec_dataset = UpsamplingReader(\n",
    "#     holo_conf,\n",
    "#     transform=train_transforms,\n",
    "#     device=data_device\n",
    "# )\n",
    "\n",
    "\n",
    "# # test_holodec_inputs = torch.from_numpy(np.load(os.path.join(\n",
    "# #             output_path, f'manual_images_{transform_mode}.npy'))).float()\n",
    "# # test_holodec_labels = torch.from_numpy(np.load(os.path.join(\n",
    "# #             output_path, f'manual_labels_{transform_mode}.npy'))).float()\n",
    "\n",
    "# # test_holodec_dataset = torch.utils.data.TensorDataset(test_holodec_inputs, test_holodec_labels)\n",
    "\n",
    "\n",
    "# holo_conf = copy.deepcopy(conf)\n",
    "# holo_conf[\"data\"][\"raw_data\"] = \"/glade/p/cisl/aiml/ai4ess_hackathon/holodec/real_holograms_CSET_RF07_20150719_203600-203700.nc\"\n",
    "# holo_conf[\"data\"][\"data_path\"] = holo_conf[\"data\"][\"raw_data\"]\n",
    "\n",
    "# test_holodec_dataset = UpsamplingReader(\n",
    "#     holo_conf,\n",
    "#     transform=valid_transforms,\n",
    "#     device=data_device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_dataset = PickleReader(\n",
    "    fn_train_raw,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_holodec_dataset = PickleReader(\n",
    "    fn_valid_raw,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_loader = torch.utils.data.DataLoader(\n",
    "    train_holodec_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=available_ncpus,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_holodec_loader = torch.utils.data.DataLoader(\n",
    "    test_holodec_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=available_ncpus,\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = smp.Unet(\n",
    "    encoder_name=\"resnet18\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset),\n",
    "    activation = \"tanh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.L1Loss()\n",
    "\n",
    "if is_cuda:\n",
    "    generator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    \n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, generator.parameters()), \n",
    "    lr=g_learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(\n",
    "        optimizer_G,\n",
    "        patience=1,\n",
    "        min_lr=g_learning_rate * 1e-3,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss 0.1900023131393919: 100%|██████████| 50/50 [03:29<00:00,  4.18s/it] \n",
      "Epoch 0 valid_loss 0.10530071968541425: 100%|██████████| 50/50 [03:37<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss 0.1900023131393919 valid_loss 0.10530071968541425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss 0.09508555163355435: 100%|██████████| 50/50 [03:33<00:00,  4.27s/it]\n",
      "Epoch 1 valid_loss 0.09006606349173714: 100%|██████████| 50/50 [03:32<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss 0.09508555163355435 valid_loss 0.09006606349173714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss 0.08504742442392836: 100%|██████████| 50/50 [03:36<00:00,  4.33s/it]\n",
      "Epoch 2 valid_loss 0.08281190649551504: 100%|██████████| 50/50 [03:34<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss 0.08504742442392836 valid_loss 0.08281190649551504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss 0.07833634010132622: 100%|██████████| 50/50 [03:36<00:00,  4.34s/it]\n",
      "Epoch 3 valid_loss 0.07767726831576403: 100%|██████████| 50/50 [03:32<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss 0.07833634010132622 valid_loss 0.07767726831576403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss 0.07700869411814447: 100%|██████████| 50/50 [03:41<00:00,  4.43s/it]\n",
      "Epoch 4 valid_loss 0.07633406478984683: 100%|██████████| 50/50 [03:33<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss 0.07700869411814447 valid_loss 0.07633406478984683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss 0.07263892874413845: 100%|██████████| 50/50 [03:35<00:00,  4.31s/it]\n",
      "Epoch 5 valid_loss 0.0715823461319886: 100%|██████████| 50/50 [03:28<00:00,  4.17s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss 0.07263892874413845 valid_loss 0.0715823461319886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss 0.06938006100701351: 100%|██████████| 50/50 [03:39<00:00,  4.38s/it]\n",
      "Epoch 6 valid_loss 0.07189084136602926: 100%|██████████| 50/50 [03:31<00:00,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss 0.06938006100701351 valid_loss 0.07189084136602926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 train_loss 0.06950113885835105: 100%|██████████| 50/50 [03:37<00:00,  4.36s/it]\n",
      "Epoch 7 valid_loss 0.070238297476488: 100%|██████████| 50/50 [03:27<00:00,  4.15s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train_loss 0.06950113885835105 valid_loss 0.070238297476488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 train_loss 0.06826724820569449: 100%|██████████| 50/50 [03:42<00:00,  4.45s/it]\n",
      "Epoch 8 valid_loss 0.06666889881678656: 100%|██████████| 50/50 [03:27<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train_loss 0.06826724820569449 valid_loss 0.06666889881678656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 train_loss 0.06332850751156609: : 57it [03:10,  2.07s/it]                      "
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batches_per_epoch = 50\n",
    "valid_batches_per_epoch = 50\n",
    "stopping_patience = 4\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if is_cuda else torch.FloatTensor\n",
    "\n",
    "results = defaultdict(list)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    ### Train\n",
    "    real_images = iter(train_holodec_loader)\n",
    "    synthethic_images = iter(train_synthetic_loader)\n",
    "    dual_iter = tqdm.tqdm(\n",
    "        enumerate(zip(real_images, synthethic_images)),\n",
    "        total = batches_per_epoch, \n",
    "        leave = True)\n",
    "    \n",
    "    train_results = defaultdict(list)\n",
    "    for i, ((holo_img, holo_label), (synth_img, synth_label)) in dual_iter:\n",
    "                    \n",
    "        if holo_img.shape[0] != synth_img.shape[0]:\n",
    "            continue\n",
    "            \n",
    "        optimizer_G.zero_grad()\n",
    "        requires_grad(generator, True)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(holo_img.type(Tensor))\n",
    "        synthethic_imgs = Variable(synth_img.type(Tensor))\n",
    "        batched_imgs = torch.cat([real_imgs, synthethic_imgs], 0)\n",
    "        \n",
    "        pred_imgs = generator(batched_imgs)\n",
    "        loss = adversarial_loss(pred_imgs, batched_imgs)\n",
    "        \n",
    "        train_results[\"loss\"].append(loss.item())\n",
    "        \n",
    "        to_print = f'Epoch {epoch} train_loss {np.mean(train_results[\"loss\"])}'\n",
    "        dual_iter.set_description(to_print)\n",
    "        dual_iter.update()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_G.step()\n",
    "            \n",
    "        if i == batches_per_epoch and i > 0:\n",
    "            break\n",
    "    \n",
    "    ### Validation\n",
    "    requires_grad(generator, False)\n",
    "    \n",
    "    real_images = iter(test_holodec_loader)\n",
    "    synthethic_images = iter(test_synthetic_loader)\n",
    "    dual_iter = tqdm.tqdm(\n",
    "        enumerate(zip(real_images, synthethic_images)),\n",
    "        total = valid_batches_per_epoch, \n",
    "        leave = True)\n",
    "    \n",
    "    valid_results = defaultdict(list)\n",
    "    for i, ((holo_img, holo_label), (synth_img, synth_label)) in dual_iter:\n",
    "                    \n",
    "        if holo_img.shape[0] != synth_img.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(holo_img.type(Tensor))\n",
    "        synthethic_imgs = Variable(synth_img.type(Tensor))\n",
    "        batched_imgs = torch.cat([real_imgs, synthethic_imgs], 0)\n",
    "        \n",
    "        pred_images = generator(batched_imgs)\n",
    "        loss = adversarial_loss(pred_images, batched_imgs)\n",
    "        \n",
    "        valid_results[\"loss\"].append(loss.item())\n",
    "        \n",
    "        to_print = f'Epoch {epoch} valid_loss {np.mean(valid_results[\"loss\"])}'\n",
    "        dual_iter.set_description(to_print)\n",
    "        dual_iter.update()\n",
    "        \n",
    "        if i == valid_batches_per_epoch and i > 0:\n",
    "            break\n",
    "            \n",
    "    results[\"epoch\"].append(epoch)\n",
    "    results[\"train_loss\"].append(np.mean(train_results[\"loss\"]))\n",
    "    results[\"valid_loss\"].append(np.mean(valid_results[\"loss\"]))\n",
    "    \n",
    "    print_str = f\"Epoch {epoch}\"\n",
    "    print_str += f' train_loss {np.mean(train_results[\"loss\"])} valid_loss {np.mean(valid_results[\"loss\"])}'\n",
    "    print(print_str)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results).reset_index()\n",
    "\n",
    "    # Save the dataframe to disk\n",
    "    df.to_csv(f'{conf[\"save_loc\"]}/training_log.csv', index=False)\n",
    "    \n",
    "    # If the best model, save to disk\n",
    "    # Save the model if its the best so far.\n",
    "    if results[\"valid_loss\"][-1] == min(results[\"valid_loss\"]):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'loss': results[\"valid_loss\"][-1]\n",
    "        }\n",
    "        torch.save(state_dict, f'{conf[\"save_loc\"]}/best.pt')\n",
    "    \n",
    "    # Lower the learning rate\n",
    "    lr_scheduler.step(np.mean(valid_results[\"loss\"]))\n",
    "    \n",
    "    # Early stopping\n",
    "    best_epoch = [i for i, j in enumerate(\n",
    "        results[\"valid_loss\"]) if j == min(results[\"valid_loss\"])][0]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= stopping_patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
