{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.fft\n",
    "import subprocess\n",
    "import logging\n",
    "import random\n",
    "import shutil\n",
    "import psutil\n",
    "import scipy\n",
    "import torch\n",
    "import copy\n",
    "import yaml\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from holodecml.data import PickleReader, UpsamplingReader\n",
    "from holodecml.propagation import InferencePropagator\n",
    "from holodecml.transforms import LoadTransformations\n",
    "from holodecml.models import load_model\n",
    "from holodecml.losses import load_loss\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from stylegan import StyledGenerator\n",
    "import lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "available_ncpus = len(psutil.Process().cpu_affinity())\n",
    "\n",
    "# Set up the GPU\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\") if not is_cuda else torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set seeds for reproducibility\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/gan.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "save_loc = conf[\"save_loc\"]\n",
    "os.makedirs(save_loc, exist_ok = True)\n",
    "os.makedirs(os.path.join(save_loc, \"images\"), exist_ok = True)\n",
    "shutil.copyfile(config, os.path.join(save_loc, \"model.yml\"))\n",
    "\n",
    "tile_size = int(conf[\"data\"][\"tile_size\"])\n",
    "step_size = int(conf[\"data\"][\"step_size\"])\n",
    "data_path = conf[\"data\"][\"output_path\"]\n",
    "data_path_raw = conf[\"data\"][\"output_path_raw\"]\n",
    "\n",
    "total_positive = int(conf[\"data\"][\"total_positive\"])\n",
    "total_negative = int(conf[\"data\"][\"total_negative\"])\n",
    "total_examples = int(conf[\"data\"][\"total_training\"])\n",
    "\n",
    "transform_mode = \"None\" if \"transform_mode\" not in conf[\"data\"] else conf[\"data\"][\"transform_mode\"]\n",
    "config_ncpus = int(conf[\"data\"][\"cores\"])\n",
    "use_cached = False if \"use_cached\" not in conf[\"data\"] else conf[\"data\"][\"use_cached\"]\n",
    "\n",
    "name_tag = f\"{tile_size}_{step_size}_{total_positive}_{total_negative}_{total_examples}_{transform_mode}\"\n",
    "fn_train = f\"{data_path}/training_{name_tag}.pkl\"\n",
    "fn_valid = f\"{data_path}/validation_{name_tag}.pkl\"\n",
    "fn_train_raw = f\"{data_path_raw}/training_{name_tag}.pkl\"\n",
    "fn_valid_raw = f\"{data_path_raw}/validation_{name_tag}.pkl\"\n",
    "\n",
    "# Trainer params\n",
    "train_batch_size = conf[\"trainer\"][\"train_batch_size\"]\n",
    "valid_batch_size = conf[\"trainer\"][\"valid_batch_size\"]\n",
    "\n",
    "epochs = conf[\"trainer\"][\"epochs\"]\n",
    "batches_per_epoch = conf[\"trainer\"][\"batches_per_epoch\"]\n",
    "Tensor = torch.cuda.FloatTensor if is_cuda else torch.FloatTensor\n",
    "adv_loss = conf[\"trainer\"][\"adv_loss\"]\n",
    "lambda_gp = conf[\"trainer\"][\"lambda_gp\"]\n",
    "train_gen_every = conf[\"trainer\"][\"train_gen_every\"]\n",
    "train_disc_every = conf[\"trainer\"][\"train_disc_every\"]\n",
    "threshold = conf[\"trainer\"][\"threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing transforms\n",
    "if \"Normalize\" in conf[\"transforms\"][\"training\"]:\n",
    "    conf[\"transforms\"][\"validation\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "    conf[\"transforms\"][\"inference\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "\n",
    "train_transforms = LoadTransformations(conf[\"transforms\"][\"training\"])\n",
    "valid_transforms = LoadTransformations(conf[\"transforms\"][\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_dataset = PickleReader(\n",
    "    fn_train,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"discriminator\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_synthetic_dataset = PickleReader(\n",
    "    fn_valid,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"discriminator\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    train_synthetic_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    test_synthetic_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_dataset = PickleReader(\n",
    "    fn_train_raw,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"discriminator\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_holodec_dataset = PickleReader(\n",
    "    fn_valid_raw,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"discriminator\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_loader = torch.utils.data.DataLoader(\n",
    "    train_holodec_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_holodec_loader = torch.utils.data.DataLoader(\n",
    "    test_holodec_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim = 512, img_shape = (1, 512, 512)):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], 1, *self.img_shape)\n",
    "        return img\n",
    "    \n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim = 512, img_shape = (1, 512, 512)):\n",
    "        \n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        \n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(2 * latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 1024),\n",
    "            *block(1024, 512),\n",
    "            *block(512, 256),\n",
    "            *block(256, 128),\n",
    "            *block(128, latent_dim, normalize=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        x = self.encoder(x.reshape(x.shape[0], int(np.prod(self.img_shape))))\n",
    "        z = torch.cat([x, z], 1)\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], 1, *self.img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = load_model(conf[\"generator\"]).to(device) #ConditionalGenerator(latent_dim = 512, img_shape = (512, 512)).to(device) \n",
    "discriminator = load_model(conf[\"discriminator\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = StyledGenerator(512).to(device)\n",
    "# zz = torch.randn(2, 32, 512, device='cuda').chunk(2, 0)[0].squeeze(0)\n",
    "# generator(zz, step = int(math.log2(512)) - 2, alpha = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /glade/work/schreck/py37/lib/python3.7/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "adv_loss = conf[\"trainer\"][\"adv_loss\"]\n",
    "if adv_loss == \"bce\":\n",
    "    adversarial_loss = torch.nn.BCELoss().to(device)\n",
    "    \n",
    "perceptual_alex = lpips.LPIPS(net='alex').cuda()\n",
    "\n",
    "Mask_Loss = load_loss(\"focal-tyversky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, generator.parameters()),\n",
    "    lr = conf[\"optimizer_G\"][\"learning_rate\"],\n",
    "    betas = (conf[\"optimizer_G\"][\"b0\"], conf[\"optimizer_G\"][\"b1\"]))\n",
    "\n",
    "optimizer_D = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, discriminator.parameters()), \n",
    "    lr = conf[\"optimizer_D\"][\"learning_rate\"], \n",
    "    betas = (conf[\"optimizer_D\"][\"b0\"], conf[\"optimizer_D\"][\"b1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_imgs, gen_imgs):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    alpha = torch.rand(real_imgs.size(0), 1, 1, 1).cuda().expand_as(real_imgs)\n",
    "    interpolated = Variable(alpha * real_imgs.data + (1 - alpha) * gen_imgs.data, requires_grad=True)\n",
    "    out = discriminator(interpolated)[1]\n",
    "    grad = torch.autograd.grad(outputs=out,\n",
    "                               inputs=interpolated,\n",
    "                               grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                               retain_graph=True,\n",
    "                               create_graph=True,\n",
    "                               only_inputs=True)[0]\n",
    "    grad = grad.view(grad.size(0), -1)\n",
    "    grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "    d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "    return d_loss_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 D_loss 2.101567 G_loss 2.603015 mask_loss 0.998724 fake_mask_loss 0.999392 h_acc 0.6975 s_p_acc 0.8875 s_t_acc 0.7600 perc_syn 0.8900 perc_real 0.6055: 100%|██████████| 250/250 [04:29<00:00,  1.08s/it]\n",
      "Epoch 1 D_loss 1.779466 G_loss 3.157093 mask_loss 0.997504 fake_mask_loss 0.999379 h_acc 0.7575 s_p_acc 0.8650 s_t_acc 0.8625 perc_syn 0.8086 perc_real 0.6560: 100%|██████████| 250/250 [04:48<00:00,  1.16s/it]\n",
      "Epoch 2 D_loss 2.074030 G_loss 2.747243 mask_loss 0.994488 fake_mask_loss 0.999172 h_acc 0.5675 s_p_acc 0.8075 s_t_acc 0.8450 perc_syn 0.9103 perc_real 0.6429: 100%|██████████| 250/250 [05:04<00:00,  1.22s/it]\n",
      "Epoch 3 D_loss 2.033770 G_loss 2.620184 mask_loss 0.988791 fake_mask_loss 0.995693 h_acc 0.6050 s_p_acc 0.8300 s_t_acc 0.9000 perc_syn 0.7446 perc_real 0.6580: 100%|██████████| 250/250 [05:05<00:00,  1.22s/it]\n",
      "Epoch 4 D_loss 1.807148 G_loss 2.667739 mask_loss 0.967748 fake_mask_loss 0.981550 h_acc 0.5650 s_p_acc 0.8325 s_t_acc 1.0000 perc_syn 0.6655 perc_real 0.6145: 100%|██████████| 250/250 [05:02<00:00,  1.21s/it]\n",
      "Epoch 5 D_loss 1.756293 G_loss 2.584666 mask_loss 0.804808 fake_mask_loss 0.906270 h_acc 0.3450 s_p_acc 0.9050 s_t_acc 0.9625 perc_syn 0.7003 perc_real 0.5446: 100%|██████████| 250/250 [05:04<00:00,  1.22s/it]\n",
      "Epoch 6 D_loss 1.507267 G_loss 2.293092 mask_loss 0.639775 fake_mask_loss 0.811267 h_acc 0.4091 s_p_acc 0.8239 s_t_acc 1.0000 perc_syn 0.7119 perc_real 0.5502:  44%|████▍     | 110/250 [02:14<03:09,  1.36s/it]"
     ]
    }
   ],
   "source": [
    "results = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    ### Train\n",
    "    real_images = iter(train_holodec_loader)\n",
    "    synthethic_images = iter(train_synthetic_loader)\n",
    "    dual_iter = tqdm.tqdm(\n",
    "        enumerate(zip(real_images, synthethic_images)),\n",
    "        total = batches_per_epoch, \n",
    "        leave = True)\n",
    "    \n",
    "    train_results = defaultdict(list)\n",
    "    for i, ((holo_img, holo_label), (synth_img, synth_label)) in dual_iter:\n",
    "            \n",
    "        if holo_img.shape[0] != synth_img.shape[0]:\n",
    "            continue\n",
    "                            \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(holo_img.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(holo_img.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(holo_img.type(Tensor))\n",
    "        synthethic_imgs = Variable(synth_img.type(Tensor))\n",
    "        #stacked = torch.cat([real_imgs, synthethic_imgs])\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        #z = Variable(Tensor(np.random.normal(0, 1.0, (holo_img.shape[0], 512))))\n",
    "        z = Variable(Tensor(np.random.normal(0, 1.0, synthethic_imgs.shape)))\n",
    "        # C-GAN-like input using the synthethic image as conditional input\n",
    "        gen_input = torch.cat([synthethic_imgs, z], 1)\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(gen_input)\n",
    "        # Discriminate the fake images\n",
    "        pred_masks, verdict = discriminator(gen_imgs)\n",
    "        \n",
    "        # If using D for mask prediction, compute the loss as the metric\n",
    "        #mask_loss = Mask_Loss(pred_masks, synth_label.to(device))\n",
    "        #train_results[\"fake_mask_loss\"].append(mask_loss.item())\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "            \n",
    "        if (i + 1) % train_gen_every == 0:\n",
    "            \n",
    "            optimizer_G.zero_grad()\n",
    "            requires_grad(generator, True)\n",
    "            requires_grad(discriminator, False)\n",
    "            \n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            if adv_loss == 'wgan-gp':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'hinge':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'bce':\n",
    "                g_loss = adversarial_loss(verdict, valid)\n",
    "                \n",
    "            mask_loss = Mask_Loss(pred_masks, synth_label.to(device))\n",
    "            g_loss += mask_loss\n",
    "                \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            train_results[\"g_loss\"].append(g_loss.item())\n",
    "            train_results[\"fake_mask_loss\"].append(mask_loss.item())\n",
    "            \n",
    "            # compute perception scores\n",
    "            p_score_syn = perceptual_alex(gen_imgs, synthethic_imgs).mean()\n",
    "            p_score_real = perceptual_alex(gen_imgs, real_imgs).mean()\n",
    "            train_results[\"g_perc_syn\"].append(p_score_syn.item())\n",
    "            train_results[\"g_perc_real\"].append(p_score_real.item())\n",
    "            \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        if (i + 1) % train_disc_every == 0:\n",
    "        \n",
    "            optimizer_D.zero_grad()\n",
    "            requires_grad(generator, False)\n",
    "            requires_grad(discriminator, True)\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            _, disc_real = discriminator(real_imgs)\n",
    "            _, disc_synth = discriminator(gen_imgs.detach())\n",
    "            pred_masks, disc_synth_true = discriminator(synthethic_imgs)\n",
    "            \n",
    "            train_results[\"real_acc\"].append(((disc_real > threshold) == valid).float().mean().item())\n",
    "            train_results[\"syn_acc\"].append(((disc_synth > threshold) == fake).float().mean().item())\n",
    "            train_results[\"syn_true_acc\"].append(((disc_synth_true > threshold) == valid).float().mean().item())\n",
    "            \n",
    "            if adv_loss == 'wgan-gp':\n",
    "                real_loss = -torch.mean(disc_real) - torch.mean(disc_synth_true)\n",
    "                fake_loss = disc_synth.mean() \n",
    "            elif adv_loss == 'hinge':\n",
    "                real_loss = torch.nn.ReLU()(1.0 - disc_real).mean() + torch.nn.ReLU()(1.0 - disc_synth_true).mean()\n",
    "                fake_loss = torch.nn.ReLU()(1.0 + disc_synth).mean()             \n",
    "            elif adv_loss == 'bce':\n",
    "                real_loss = adversarial_loss(torch.cat([disc_real, disc_synth_true]), torch.cat([valid, valid]))\n",
    "                #real_loss = adversarial_loss(disc_real, valid) #+ adversarial_loss(disc_synth_true, valid)\n",
    "                fake_loss = adversarial_loss(disc_synth, fake) \n",
    "                \n",
    "            #reg_loss = torch.nn.L1Loss()(gen_imgs.detach(), synthethic_imgs)\n",
    "            mask_loss = Mask_Loss(pred_masks, synth_label.to(device))\n",
    "            d_loss = real_loss + fake_loss + mask_loss\n",
    "            #d_loss = real_loss + fake_loss + reg_loss + mask_loss\n",
    "            \n",
    "            train_results[\"d_loss\"].append(d_loss.item())\n",
    "            train_results[\"mask_loss\"].append(mask_loss.item())\n",
    "            \n",
    "            if adv_loss == 'wgan-gp':\n",
    "                # Compute gradient penalty\n",
    "                alpha = torch.rand(real_imgs.size(0), 1, 1, 1).cuda().expand_as(real_imgs)\n",
    "                interpolated = Variable(alpha * real_imgs.data + (1 - alpha) * gen_imgs.data, requires_grad=True)\n",
    "                out = discriminator(interpolated)[1]\n",
    "\n",
    "                grad = torch.autograd.grad(outputs=out,\n",
    "                                           inputs=interpolated,\n",
    "                                           grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                           retain_graph=True,\n",
    "                                           create_graph=True,\n",
    "                                           only_inputs=True)[0]\n",
    "\n",
    "                grad = grad.view(grad.size(0), -1)\n",
    "                grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "                d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "                d_loss_reg = lambda_gp * d_loss_gp\n",
    "                #optimizer_D.zero_grad()\n",
    "                #d_loss_reg.backward()\n",
    "                #optimizer_D.step()\n",
    "                d_loss += d_loss_reg\n",
    "                train_results[\"d_reg\"].append(d_loss_reg.item())\n",
    "            \n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        print_str =  f'Epoch {epoch}'\n",
    "        print_str += f' D_loss {np.mean(train_results[\"d_loss\"]):.6f}'\n",
    "        if adv_loss == 'wgan-gp':\n",
    "            print_str += f' D_reg {np.mean(train_results[\"d_reg\"]):.6f}'\n",
    "        print_str += f' G_loss {np.mean(train_results[\"g_loss\"]):6f}'\n",
    "        print_str += f' mask_loss {np.mean(train_results[\"mask_loss\"]):6f}'\n",
    "        print_str += f' fake_mask_loss {np.mean(train_results[\"fake_mask_loss\"]):6f}'\n",
    "        print_str += f' h_acc {np.mean(train_results[\"real_acc\"]):.4f}'\n",
    "        print_str += f' s_p_acc {np.mean(train_results[\"syn_acc\"]):.4f}'\n",
    "        print_str += f' s_t_acc {np.mean(train_results[\"syn_true_acc\"]):.4f}'\n",
    "        print_str += f' perc_syn {np.mean(train_results[\"g_perc_syn\"]):.4f}'\n",
    "        print_str += f' perc_real {np.mean(train_results[\"g_perc_real\"]):.4f}'\n",
    "        dual_iter.set_description(print_str)\n",
    "        dual_iter.refresh()\n",
    "        \n",
    "        if i == batches_per_epoch and i > 0:\n",
    "            break\n",
    "        \n",
    "    # Validate \n",
    "        \n",
    "    # Epoch is over. Save some stuff.\n",
    "    save_image(synthethic_imgs.data[:16], f'{conf[\"save_loc\"]}/images/synth_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(real_imgs.data[:16], f'{conf[\"save_loc\"]}/images/real_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(gen_imgs.data[:16], f'{conf[\"save_loc\"]}/images/pred_{epoch}.png', nrow=4, normalize=True)\n",
    "\n",
    "    # Save the dataframe to disk\n",
    "    results[\"epoch\"].append(epoch)\n",
    "    results[\"d_loss\"].append(np.mean(train_results[\"d_loss\"]))\n",
    "    if adv_loss == 'wgan-gp':\n",
    "        results[\"d_loss_reg\"].append(np.mean(train_results[\"d_reg\"]))\n",
    "    results[\"g_loss\"].append(np.mean(train_results[\"g_loss\"]))\n",
    "    results[\"mask_loss\"].append(np.mean(train_results[\"mask_loss\"]))\n",
    "    results[\"fake_mask_loss\"].append(np.mean(train_results[\"fake_mask_loss\"]))\n",
    "    results[\"holo_acc\"].append(np.mean(train_results[\"real_acc\"]))\n",
    "    results[\"pred_synth_acc\"].append(np.mean(train_results[\"syn_acc\"]))\n",
    "    results[\"true_synth_acc\"].append(np.mean(train_results[\"syn_true_acc\"]))\n",
    "    results[\"perception_syn\"].append(np.mean(train_results[\"g_perc_syn\"]))\n",
    "    results[\"perception_holo\"].append(np.mean(train_results[\"g_perc_real\"]))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results).reset_index()\n",
    "    df.to_csv(f'{conf[\"save_loc\"]}/training_log.csv', index=False)\n",
    "    \n",
    "    # Save the model\n",
    "    state_dict = {\n",
    "        'epoch': epoch,\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "    }\n",
    "    torch.save(state_dict, f'{conf[\"save_loc\"]}/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Images0\" and \"training_log_0\" used both real and synthetic in D and G, and the L1 loss term on G. \n",
    "# \"Images1\" and \"training_log_1\" used both real and synthetic in D, and the L1 loss term on D. \n",
    "\n",
    "# \"Images2\" and \"training_log_2\" used both real and synthetic in D, and the mask loss term on D\n",
    "# \"Images3\" and \"training_log_3\" used both real and synthetic in D, and the L1 loss and mask loss term on D\n",
    "# \"Images4\" and \"training_log_4\" used both real and synthetic in D, and the L1 loss and mask loss term on D, mask loss on G."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
