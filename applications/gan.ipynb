{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.fft\n",
    "import subprocess\n",
    "import logging\n",
    "import random\n",
    "import shutil\n",
    "import psutil\n",
    "import scipy\n",
    "import torch\n",
    "import copy\n",
    "import yaml\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from holodecml.data import PickleReader, UpsamplingReader\n",
    "from holodecml.propagation import InferencePropagator\n",
    "from holodecml.transforms import LoadTransformations\n",
    "from holodecml.models import load_model\n",
    "from holodecml.losses import load_loss\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "available_ncpus = len(psutil.Process().cpu_affinity())\n",
    "\n",
    "# Set up the GPU\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\") if not is_cuda else torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set seeds for reproducibility\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../results/gan/model.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "save_loc = conf[\"save_loc\"]\n",
    "os.makedirs(save_loc, exist_ok = True)\n",
    "os.makedirs(os.path.join(save_loc, \"images\"), exist_ok = True)\n",
    "shutil.copyfile(config, os.path.join(save_loc, \"model.yml\"))\n",
    "\n",
    "tile_size = int(conf[\"data\"][\"tile_size\"])\n",
    "step_size = int(conf[\"data\"][\"step_size\"])\n",
    "data_path = conf[\"data\"][\"output_path\"]\n",
    "data_path_raw = conf[\"data\"][\"output_path_raw\"]\n",
    "\n",
    "total_positive = int(conf[\"data\"][\"total_positive\"])\n",
    "total_negative = int(conf[\"data\"][\"total_negative\"])\n",
    "total_examples = int(conf[\"data\"][\"total_training\"])\n",
    "\n",
    "transform_mode = \"None\" if \"transform_mode\" not in conf[\"data\"] else conf[\"data\"][\"transform_mode\"]\n",
    "config_ncpus = int(conf[\"data\"][\"cores\"])\n",
    "use_cached = False if \"use_cached\" not in conf[\"data\"] else conf[\"data\"][\"use_cached\"]\n",
    "\n",
    "name_tag = f\"{tile_size}_{step_size}_{total_positive}_{total_negative}_{total_examples}_{transform_mode}\"\n",
    "fn_train = f\"{data_path}/training_{name_tag}.pkl\"\n",
    "fn_valid = f\"{data_path}/validation_{name_tag}.pkl\"\n",
    "fn_train_raw = f\"{data_path_raw}/training_{name_tag}.pkl\"\n",
    "fn_valid_raw = f\"{data_path_raw}/validation_{name_tag}.pkl\"\n",
    "\n",
    "# Trainer params\n",
    "train_batch_size = conf[\"trainer\"][\"train_batch_size\"]\n",
    "valid_batch_size = conf[\"trainer\"][\"valid_batch_size\"]\n",
    "\n",
    "epochs = conf[\"trainer\"][\"epochs\"]\n",
    "batches_per_epoch = conf[\"trainer\"][\"batches_per_epoch\"]\n",
    "Tensor = torch.cuda.FloatTensor if is_cuda else torch.FloatTensor\n",
    "adv_loss = conf[\"trainer\"][\"adv_loss\"]\n",
    "lambda_gp = conf[\"trainer\"][\"lambda_gp\"]\n",
    "train_gen_every = conf[\"trainer\"][\"train_gen_every\"]\n",
    "train_disc_every = conf[\"trainer\"][\"train_disc_every\"]\n",
    "threshold = conf[\"trainer\"][\"threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing transforms\n",
    "if \"Normalize\" in conf[\"transforms\"][\"training\"]:\n",
    "    conf[\"transforms\"][\"validation\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "    conf[\"transforms\"][\"inference\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "\n",
    "train_transforms = LoadTransformations(conf[\"transforms\"][\"training\"])\n",
    "valid_transforms = LoadTransformations(conf[\"transforms\"][\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_dataset = PickleReader(\n",
    "    fn_train,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_synthetic_dataset = PickleReader(\n",
    "    fn_valid,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    train_synthetic_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0, #available_ncpus//2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    test_synthetic_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_dataset = PickleReader(\n",
    "    fn_train_raw,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_holodec_dataset = PickleReader(\n",
    "    fn_valid_raw,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_loader = torch.utils.data.DataLoader(\n",
    "    train_holodec_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_holodec_loader = torch.utils.data.DataLoader(\n",
    "    test_holodec_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = load_model(conf[\"generator\"]).to(device)\n",
    "discriminator = load_model(conf[\"discriminator\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_loss = conf[\"trainer\"][\"adv_loss\"]\n",
    "if adv_loss == \"bce\":\n",
    "    adversarial_loss = torch.nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, generator.parameters()),\n",
    "    lr = conf[\"optimizer_G\"][\"learning_rate\"],\n",
    "    betas = (conf[\"optimizer_G\"][\"b0\"], conf[\"optimizer_G\"][\"b1\"]))\n",
    "\n",
    "optimizer_D = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, discriminator.parameters()), \n",
    "    lr = conf[\"optimizer_D\"][\"learning_rate\"], \n",
    "    betas = (conf[\"optimizer_D\"][\"b0\"], conf[\"optimizer_D\"][\"b1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 D_loss 0.03949616 D_reg 7.34634815 G_loss -0.376280 h_acc 0.679688 s_pred_acc 0.232813 s_true_acc 0.342187: 100%|██████████| 200/200 [03:10<00:00,  1.05it/s]\n",
      "Epoch 1 D_loss 0.10818136 D_reg 4.37663180 G_loss -0.491381 h_acc 0.501563 s_pred_acc 0.248438 s_true_acc 0.496875: 100%|██████████| 200/200 [03:23<00:00,  1.02s/it]\n",
      "Epoch 2 D_loss 0.08446530 D_reg 6.14117680 G_loss -0.385922 h_acc 0.368750 s_pred_acc 0.412500 s_true_acc 0.639062: 100%|██████████| 200/200 [03:39<00:00,  1.10s/it]\n",
      "Epoch 3 D_loss 0.07391746 D_reg 5.38814204 G_loss -0.569660 h_acc 0.539062 s_pred_acc 0.237500 s_true_acc 0.503125: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 4 D_loss 0.08538443 D_reg 6.62940566 G_loss -0.496440 h_acc 0.432812 s_pred_acc 0.321875 s_true_acc 0.565625: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 5 D_loss 0.10152526 D_reg 9.13502411 G_loss -0.447607 h_acc 0.410938 s_pred_acc 0.393750 s_true_acc 0.621875: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 6 D_loss 0.15476378 D_reg 6.59558102 G_loss -0.647395 h_acc 0.512500 s_pred_acc 0.187500 s_true_acc 0.456250: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 7 D_loss 0.03634225 D_reg 8.22896118 G_loss -0.603528 h_acc 0.729688 s_pred_acc 0.170313 s_true_acc 0.307812: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 8 D_loss 0.04097472 D_reg 7.58420269 G_loss -0.515641 h_acc 0.546875 s_pred_acc 0.270313 s_true_acc 0.442188: 100%|██████████| 200/200 [03:45<00:00,  1.13s/it]\n",
      "Epoch 9 D_loss 0.03998028 D_reg 6.78348005 G_loss -0.448562 h_acc 0.464062 s_pred_acc 0.307812 s_true_acc 0.571875: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 10 D_loss 0.01680205 D_reg 9.52913465 G_loss -0.197477 h_acc 0.137500 s_pred_acc 0.828125 s_true_acc 0.865625: 100%|██████████| 200/200 [03:45<00:00,  1.13s/it]  \n",
      "Epoch 11 D_loss 0.05210573 D_reg 8.16446480 G_loss -0.345595 h_acc 0.140625 s_pred_acc 0.617188 s_true_acc 0.853125: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it] \n",
      "Epoch 12 D_loss 0.09497288 D_reg 7.98874463 G_loss -0.573601 h_acc 0.564063 s_pred_acc 0.221875 s_true_acc 0.510938: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 13 D_loss 0.04558193 D_reg 8.66735390 G_loss -0.519483 h_acc 0.710938 s_pred_acc 0.225000 s_true_acc 0.239063: 100%|██████████| 200/200 [03:42<00:00,  1.11s/it]\n",
      "Epoch 14 D_loss 0.06663878 D_reg 8.08196125 G_loss -0.591995 h_acc 0.695312 s_pred_acc 0.198437 s_true_acc 0.318750: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 15 D_loss 0.13227446 D_reg 7.87729309 G_loss -0.628990 h_acc 0.326562 s_pred_acc 0.275000 s_true_acc 0.665625: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 16 D_loss 0.14731310 D_reg 7.11129160 G_loss -0.716029 h_acc 0.478125 s_pred_acc 0.137500 s_true_acc 0.515625: 100%|██████████| 200/200 [03:44<00:00,  1.12s/it]\n",
      "Epoch 17 D_loss 0.07809923 D_reg 7.95329212 G_loss -0.674314 h_acc 0.617188 s_pred_acc 0.176563 s_true_acc 0.395313: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 18 D_loss 0.00694000 D_reg 7.41028076 G_loss -0.529146 h_acc 0.703125 s_pred_acc 0.245312 s_true_acc 0.367188: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it] \n",
      "Epoch 19 D_loss 0.12592158 D_reg 7.14737443 G_loss -0.615850 h_acc 0.481250 s_pred_acc 0.171875 s_true_acc 0.507812: 100%|██████████| 200/200 [03:43<00:00,  1.12s/it]\n",
      "Epoch 20 D_loss -0.00304405 D_reg 8.12846597 G_loss -0.373819 h_acc 0.417187 s_pred_acc 0.529687 s_true_acc 0.628125: 100%|██████████| 200/200 [03:44<00:00,  1.12s/it]\n",
      "Epoch 21 D_loss 0.04094812 D_reg 8.45215586 G_loss -0.354613 h_acc 0.151562 s_pred_acc 0.618750 s_true_acc 0.837500: 100%|██████████| 200/200 [03:44<00:00,  1.12s/it]\n",
      "Epoch 22 D_loss -0.00230415 D_reg 7.80625459 G_loss -0.506611 h_acc 0.639062 s_pred_acc 0.232813 s_true_acc 0.481250: 100%|██████████| 200/200 [03:56<00:00,  1.18s/it]\n",
      "Epoch 23 D_loss 0.03936794 D_reg 7.81351514 G_loss -0.493878 h_acc 0.396875 s_pred_acc 0.315625 s_true_acc 0.706250: 100%|██████████| 200/200 [03:59<00:00,  1.20s/it]\n",
      "Epoch 24 D_loss -0.01800961 D_reg 8.30740247 G_loss -0.338139 h_acc 0.446875 s_pred_acc 0.521875 s_true_acc 0.653125: 100%|██████████| 200/200 [04:00<00:00,  1.20s/it]\n",
      "Epoch 25 D_loss -0.00677038 D_reg 8.93019605 G_loss -0.204139 h_acc 0.131250 s_pred_acc 0.775000 s_true_acc 0.890625: 100%|██████████| 200/200 [03:59<00:00,  1.20s/it]\n",
      "Epoch 26 D_loss 0.01082182 D_reg 8.78963012 G_loss -0.250475 h_acc 0.081250 s_pred_acc 0.887500 s_true_acc 0.896875: 100%|██████████| 200/200 [04:01<00:00,  1.21s/it]\n",
      "Epoch 27 D_loss 0.03040168 D_reg 8.47140731 G_loss -0.357349 h_acc 0.178125 s_pred_acc 0.656250 s_true_acc 0.898438: 100%|██████████| 200/200 [03:59<00:00,  1.20s/it]\n",
      "Epoch 28 D_loss 0.00685106 D_reg 7.85539011 G_loss -0.344021 h_acc 0.326562 s_pred_acc 0.565625 s_true_acc 0.731250: 100%|██████████| 200/200 [03:58<00:00,  1.19s/it]\n",
      "Epoch 29 D_loss 0.00221042 D_reg 7.70406302 G_loss -0.342441 h_acc 0.390625 s_pred_acc 0.559375 s_true_acc 0.651563: 100%|██████████| 200/200 [04:03<00:00,  1.22s/it] \n",
      "Epoch 30 D_loss 0.01284749 D_reg 8.06089637 G_loss -0.388570 h_acc 0.421875 s_pred_acc 0.392188 s_true_acc 0.620313: 100%|██████████| 200/200 [03:59<00:00,  1.20s/it]\n",
      "Epoch 31 D_loss 0.01043054 D_reg 8.27863129 G_loss -0.306579 h_acc 0.279687 s_pred_acc 0.600000 s_true_acc 0.771875: 100%|██████████| 200/200 [03:58<00:00,  1.19s/it]\n",
      "Epoch 32 D_loss 0.01830184 D_reg 8.59492711 G_loss -0.207239 h_acc 0.012500 s_pred_acc 0.984375 s_true_acc 1.000000: 100%|██████████| 200/200 [04:00<00:00,  1.20s/it]\n",
      "Epoch 33 D_loss -0.00511772 D_reg 9.10094091 G_loss -0.115563 h_acc 0.001563 s_pred_acc 1.000000 s_true_acc 0.996875: 100%|██████████| 200/200 [04:00<00:00,  1.20s/it]\n",
      "Epoch 34 D_loss -0.00898717 D_reg 8.06957134 G_loss -0.393577 h_acc 0.409375 s_pred_acc 0.468750 s_true_acc 0.731250: 100%|██████████| 200/200 [03:58<00:00,  1.19s/it]\n",
      "Epoch 35 D_loss 0.03454036 D_reg 8.74056976 G_loss -0.496052 h_acc 0.617188 s_pred_acc 0.303125 s_true_acc 0.467187: 100%|██████████| 200/200 [04:00<00:00,  1.20s/it]\n",
      "Epoch 36 D_loss 0.00978873 D_reg 7.64666940 G_loss -0.369891 h_acc 0.370312 s_pred_acc 0.478125 s_true_acc 0.757812: 100%|██████████| 200/200 [04:03<00:00,  1.22s/it] \n",
      "Epoch 37 D_loss -0.04351261 D_reg 7.99257622 G_loss -0.478025 h_acc 0.581250 s_pred_acc 0.282813 s_true_acc 0.712500: 100%|██████████| 200/200 [04:01<00:00,  1.21s/it]\n",
      "Epoch 38 D_loss -0.05954117 D_reg 8.44667276 G_loss -0.546463 h_acc 0.750000 s_pred_acc 0.125000 s_true_acc 0.529687: 100%|██████████| 200/200 [04:00<00:00,  1.20s/it]\n",
      "Epoch 39 D_loss 0.02307687 D_reg 7.62294146 G_loss -0.492512 h_acc 0.617188 s_pred_acc 0.220312 s_true_acc 0.482812: 100%|██████████| 200/200 [03:59<00:00,  1.20s/it]\n",
      "Epoch 40 D_loss -0.02694538 D_reg 7.79322505 G_loss -0.398374 h_acc 0.376563 s_pred_acc 0.404687 s_true_acc 0.879687: 100%|██████████| 200/200 [03:59<00:00,  1.20s/it]\n",
      "Epoch 41 D_loss nan D_reg nan G_loss -0.069128 h_acc nan s_pred_acc nan s_true_acc nan:   2%|▏         | 3/200 [00:03<03:38,  1.11s/it]"
     ]
    }
   ],
   "source": [
    "results = defaultdict(list)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    ### Train\n",
    "    real_images = iter(train_holodec_loader)\n",
    "    synthethic_images = iter(train_synthetic_loader)\n",
    "    dual_iter = tqdm.tqdm(\n",
    "        enumerate(zip(real_images, synthethic_images)),\n",
    "        total = batches_per_epoch, \n",
    "        leave = True)\n",
    "    \n",
    "    train_results = defaultdict(list)\n",
    "    for i, ((holo_img, holo_label), (synth_img, synth_label)) in dual_iter:\n",
    "            \n",
    "        if holo_img.shape[0] != synth_img.shape[0]:\n",
    "            continue\n",
    "                \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(holo_img.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(holo_img.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(holo_img.type(Tensor))\n",
    "        synthethic_imgs = Variable(synth_img.type(Tensor))\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1.0, holo_img.shape)))\n",
    "        # C-GAN-like input using the synthethic image as conditional input\n",
    "        gen_input = torch.cat([z, synthethic_imgs], 1)\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(gen_input)\n",
    "        # Discriminate the fake images\n",
    "        _, verdict = discriminator(gen_imgs)\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        \n",
    "        if (i + 1) % train_gen_every == 0:\n",
    "            \n",
    "            optimizer_G.zero_grad()\n",
    "            requires_grad(generator, True)\n",
    "            requires_grad(discriminator, False)\n",
    "            \n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            if adv_loss == 'wgan-gp':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'hinge':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'bce':\n",
    "                g_loss = adversarial_loss(verdict, valid)\n",
    "                \n",
    "            g_loss += torch.nn.L1Loss()(gen_imgs, synthethic_imgs)\n",
    "                \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            train_results[\"g_loss\"].append(g_loss.item())\n",
    "            \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        if (i + 1) % train_disc_every == 0:\n",
    "        \n",
    "            optimizer_D.zero_grad()\n",
    "            requires_grad(generator, False)\n",
    "            requires_grad(discriminator, True)\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            _, disc_real = discriminator(real_imgs)\n",
    "            _, disc_synth = discriminator(gen_imgs.detach())\n",
    "            _, disc_synth_true = discriminator(synthethic_imgs)\n",
    "            \n",
    "            train_results[\"real_acc\"].append(((disc_real > threshold) == valid).float().mean().item())\n",
    "            train_results[\"syn_acc\"].append(((disc_synth > threshold) == fake).float().mean().item())\n",
    "            train_results[\"syn_true_acc\"].append(((disc_synth_true > threshold) == fake).float().mean().item())\n",
    "            \n",
    "            if adv_loss == 'wgan-gp':\n",
    "                real_loss = -torch.mean(disc_real)\n",
    "                fake_loss = disc_synth.mean() \n",
    "                fake_loss += disc_synth_true.mean()\n",
    "            elif adv_loss == 'hinge':\n",
    "                real_loss = torch.nn.ReLU()(1.0 - disc_real).mean()\n",
    "                fake_loss = torch.nn.ReLU()(1.0 + disc_synth).mean() \n",
    "                fake_loss += torch.nn.ReLU()(1.0 + disc_synth_true).mean()\n",
    "            elif adv_loss == 'bce':\n",
    "                real_loss = adversarial_loss(disc_real, valid)\n",
    "                fake_loss = adversarial_loss(disc_synth, fake) \n",
    "                fake_loss += adversarial_loss(disc_synth_true, fake)\n",
    "            d_loss = real_loss + fake_loss / 2.\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            train_results[\"d_loss\"].append(d_loss.item())\n",
    "\n",
    "            if adv_loss == 'wgan-gp':\n",
    "                # Compute gradient penalty\n",
    "                alpha = torch.rand(real_imgs.size(0), 1, 1, 1).cuda().expand_as(real_imgs)\n",
    "                interpolated = Variable(alpha * real_imgs.data + (1 - alpha) * gen_imgs.data, requires_grad=True)\n",
    "                out = discriminator(interpolated)[1]\n",
    "\n",
    "                grad = torch.autograd.grad(outputs=out,\n",
    "                                           inputs=interpolated,\n",
    "                                           grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                           retain_graph=True,\n",
    "                                           create_graph=True,\n",
    "                                           only_inputs=True)[0]\n",
    "\n",
    "                grad = grad.view(grad.size(0), -1)\n",
    "                grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "                d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "\n",
    "                # Backward + Optimize\n",
    "                d_loss_reg = lambda_gp * d_loss_gp\n",
    "\n",
    "                optimizer_D.zero_grad()\n",
    "                d_loss_reg.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "                train_results[\"d_reg\"].append(d_loss_reg.item())\n",
    "\n",
    "        print_str =  f'Epoch {epoch}'\n",
    "        print_str += f' D_loss {np.mean(train_results[\"d_loss\"]):.8f}'\n",
    "        print_str += f' D_reg {np.mean(train_results[\"d_reg\"]):.8f}'\n",
    "        print_str += f' G_loss {np.mean(train_results[\"g_loss\"]):8f}'\n",
    "        print_str += f' h_acc {np.mean(train_results[\"real_acc\"]):.6f}'\n",
    "        print_str += f' s_pred_acc {np.mean(train_results[\"syn_acc\"]):.6f}'\n",
    "        print_str += f' s_true_acc {np.mean(train_results[\"syn_true_acc\"]):.6f}'\n",
    "        dual_iter.set_description(print_str)\n",
    "        dual_iter.refresh()\n",
    "        \n",
    "        if i == batches_per_epoch and i > 0:\n",
    "            break\n",
    "        \n",
    "    # Epoch is over. Save some stuff.\n",
    "    save_image(synthethic_imgs.data[:16], f'{conf[\"save_loc\"]}/images/synth_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(real_imgs.data[:16], f'{conf[\"save_loc\"]}/images/real_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(gen_imgs.data[:16], f'{conf[\"save_loc\"]}/images/pred_{epoch}.png', nrow=4, normalize=True)\n",
    "    #save_image(gen_noise.data[:16], f\"../results/gan/images/noise_{epoch}.png\", nrow=4, normalize=True)\n",
    "            \n",
    "\n",
    "    # Save the dataframe to disk\n",
    "    results[\"epoch\"].append(epoch)\n",
    "    results[\"d_loss\"].append(np.mean(train_results[\"d_loss\"]))\n",
    "    results[\"d_loss_reg\"].append(np.mean(train_results[\"d_reg\"]))\n",
    "    results[\"g_loss\"].append(np.mean(train_results[\"g_loss\"]))\n",
    "    results[\"real_acc\"].append(np.mean(train_results[\"real_acc\"]))\n",
    "    results[\"pred_synth_acc\"].append(np.mean(train_results[\"syn_acc\"]))\n",
    "    results[\"true_synth_acc\"].append(np.mean(train_results[\"syn_true_acc\"]))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results).reset_index()\n",
    "    df.to_csv(f'{conf[\"save_loc\"]}/training_log.csv', index=False)\n",
    "    \n",
    "    # Save the model\n",
    "    state_dict = {\n",
    "        'epoch': epoch,\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "    }\n",
    "    torch.save(state_dict, f'{conf[\"save_loc\"]}/best.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
