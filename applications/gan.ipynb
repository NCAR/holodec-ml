{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.fft\n",
    "import subprocess\n",
    "import logging\n",
    "import random\n",
    "import shutil\n",
    "import psutil\n",
    "import scipy\n",
    "import torch\n",
    "import copy\n",
    "import yaml\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from holodecml.data import PickleReader, UpsamplingReader\n",
    "from holodecml.propagation import InferencePropagator\n",
    "from holodecml.transforms import LoadTransformations\n",
    "from holodecml.models import load_model\n",
    "from holodecml.losses import load_loss\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_ncpus = len(psutil.Process().cpu_affinity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set seeds for reproducibility\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../results/gan/model.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "tile_size = int(conf[\"data\"][\"tile_size\"])\n",
    "step_size = int(conf[\"data\"][\"step_size\"])\n",
    "data_path = conf[\"data\"][\"output_path\"]\n",
    "data_path_raw = conf[\"data\"][\"output_path_raw\"]\n",
    "\n",
    "total_positive = int(conf[\"data\"][\"total_positive\"])\n",
    "total_negative = int(conf[\"data\"][\"total_negative\"])\n",
    "total_examples = int(conf[\"data\"][\"total_training\"])\n",
    "\n",
    "transform_mode = \"None\" if \"transform_mode\" not in conf[\"data\"] else conf[\"data\"][\"transform_mode\"]\n",
    "config_ncpus = int(conf[\"data\"][\"cores\"])\n",
    "use_cached = False if \"use_cached\" not in conf[\"data\"] else conf[\"data\"][\"use_cached\"]\n",
    "\n",
    "name_tag = f\"{tile_size}_{step_size}_{total_positive}_{total_negative}_{total_examples}_{transform_mode}\"\n",
    "fn_train = f\"{data_path}/training_{name_tag}.pkl\"\n",
    "fn_valid = f\"{data_path}/validation_{name_tag}.pkl\"\n",
    "fn_train_raw = f\"{data_path_raw}/training_{name_tag}.pkl\"\n",
    "fn_valid_raw = f\"{data_path_raw}/validation_{name_tag}.pkl\"\n",
    "\n",
    "output_path = conf[\"data\"][\"output_path\"]\n",
    "\n",
    "train_batch_size = 16\n",
    "valid_batch_size = 16\n",
    "\n",
    "latent_dim = 128\n",
    "img_shape = (1, tile_size, tile_size)\n",
    "\n",
    "d_learning_rate = 0.0004\n",
    "g_learning_rate = 0.0001\n",
    "\n",
    "b1 = 0.0\n",
    "b2 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "data_device = torch.device(\"cpu\") if \"device\" not in conf[\"data\"] else conf[\"data\"][\"device\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing transforms\n",
    "if \"Normalize\" in conf[\"transforms\"][\"training\"]:\n",
    "    conf[\"transforms\"][\"validation\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "    conf[\"transforms\"][\"inference\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "\n",
    "train_transforms = LoadTransformations(conf[\"transforms\"][\"training\"])\n",
    "valid_transforms = LoadTransformations(conf[\"transforms\"][\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_dataset = PickleReader(\n",
    "    fn_train,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_synthetic_dataset = PickleReader(\n",
    "    fn_valid,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    train_synthetic_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0, #available_ncpus//2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    test_synthetic_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holo_conf = copy.deepcopy(conf)\n",
    "# holo_conf[\"data\"][\"data_path\"] = holo_conf[\"data\"][\"raw_data\"]\n",
    "\n",
    "\n",
    "# train_holodec_dataset = UpsamplingReader(\n",
    "#     holo_conf,\n",
    "#     transform=train_transforms,\n",
    "#     device=data_device\n",
    "# )\n",
    "\n",
    "# holo_conf = copy.deepcopy(conf)\n",
    "# holo_conf[\"data\"][\"raw_data\"] = \"/glade/p/cisl/aiml/ai4ess_hackathon/holodec/real_holograms_CSET_RF07_20150719_203600-203700.nc\"\n",
    "# holo_conf[\"data\"][\"data_path\"] = holo_conf[\"data\"][\"raw_data\"]\n",
    "\n",
    "# test_holodec_dataset = UpsamplingReader(\n",
    "#     holo_conf,\n",
    "#     transform=valid_transforms,\n",
    "#     device=data_device\n",
    "# )\n",
    "\n",
    "train_holodec_dataset = PickleReader(\n",
    "    fn_train_raw,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_holodec_dataset = PickleReader(\n",
    "    fn_valid_raw,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_loader = torch.utils.data.DataLoader(\n",
    "    train_holodec_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_holodec_loader = torch.utils.data.DataLoader(\n",
    "    test_holodec_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = smp.Unet(\n",
    "    encoder_name=\"resnet18\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=2,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset),\n",
    "    activation = \"tanh\"\n",
    ")\n",
    "\n",
    "\n",
    "aux_params=dict(\n",
    "    pooling='avg',             # one of 'avg', 'max'\n",
    "    dropout=0.2,               # dropout ratio, default is None\n",
    "    activation='sigmoid',      # activation function, default is None\n",
    "    classes=1,                 # define number of output labels\n",
    ")\n",
    "\n",
    "discriminator = smp.Unet(\n",
    "#     encoder_name=\"resnet18\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1, \n",
    "    aux_params=aux_params\n",
    ")\n",
    "\n",
    "#print(generator(torch.ones([1, 1, 64, 64])).shape)\n",
    "#print(discriminator(torch.ones([1, 1, 64, 64]))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "if is_cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    \n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()), lr=g_learning_rate, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(filter(lambda p: p.requires_grad, discriminator.parameters()), lr=d_learning_rate, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 D_loss 0.32681587 D_reg 8.74229616 G_loss -0.340354 h_acc 0.385354 s_pred_acc 0.677894 s_true_acc 0.632610: 100%|██████████| 500/500 [17:02<00:00,  2.04s/it] \n",
      "Epoch 1 D_loss 0.07869994 D_reg 7.29737211 G_loss -0.130341 h_acc 0.268214 s_pred_acc 0.935254 s_true_acc 0.795160: 100%|██████████| 500/500 [17:49<00:00,  2.14s/it]\n",
      "Epoch 2 D_loss -0.13861550 D_reg 5.16274157 G_loss -0.059109 h_acc 0.246881 s_pred_acc 0.994511 s_true_acc 0.972305: 100%|██████████| 500/500 [17:51<00:00,  2.14s/it]\n",
      "Epoch 3 D_loss -0.27649639 D_reg 2.25340946 G_loss -0.015994 h_acc 0.261976 s_pred_acc 0.999875 s_true_acc 0.999875: 100%|██████████| 500/500 [17:50<00:00,  2.14s/it]\n",
      "Epoch 4 D_loss -0.33729177 D_reg 1.89187824 G_loss -0.014369 h_acc 0.381861 s_pred_acc 0.999501 s_true_acc 0.999875: 100%|██████████| 500/500 [17:50<00:00,  2.14s/it]\n",
      "Epoch 5 D_loss -0.41444625 D_reg 1.64227292 G_loss -0.031997 h_acc 0.561502 s_pred_acc 0.995509 s_true_acc 0.999875: 100%|██████████| 500/500 [17:50<00:00,  2.14s/it]\n",
      "Epoch 6 D_loss -0.39346928 D_reg 2.70847926 G_loss -0.026732 h_acc 0.552645 s_pred_acc 0.996382 s_true_acc 0.984281: 100%|██████████| 500/500 [17:50<00:00,  2.14s/it]\n",
      "Epoch 7 D_loss -0.34362488 D_reg 2.09267878 G_loss -0.012504 h_acc 0.536195 s_pred_acc 1.000000 s_true_acc 0.970328:  59%|█████▉    | 297/500 [10:32<07:14,  2.14s/it]"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "batches_per_epoch = 200\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if is_cuda else torch.FloatTensor\n",
    "adv_loss = 'wgan-gp'\n",
    "lambda_gp = 10\n",
    "train_gen_every = 1\n",
    "train_disc_every = 1\n",
    "threshold = 0.5\n",
    "\n",
    "results = defaultdict(list)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    ### Train\n",
    "    real_images = iter(train_holodec_loader)\n",
    "    synthethic_images = iter(train_synthetic_loader)\n",
    "    dual_iter = tqdm.tqdm(\n",
    "        enumerate(zip(real_images, synthethic_images)),\n",
    "        total = batches_per_epoch, \n",
    "        leave = True)\n",
    "    \n",
    "    train_results = defaultdict(list)\n",
    "    for i, ((holo_img, holo_label), (synth_img, synth_label)) in dual_iter:\n",
    "            \n",
    "        if holo_img.shape[0] != synth_img.shape[0]:\n",
    "            continue\n",
    "                \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(holo_img.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(holo_img.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(holo_img.type(Tensor))\n",
    "        synthethic_imgs = Variable(synth_img.type(Tensor))\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 0.2, holo_img.shape)))\n",
    "        # C-GAN-like input using the synthethic image as conditional input\n",
    "        gen_input = torch.cat([z, synthethic_imgs], 1)\n",
    "        # Generate a batch of images\n",
    "        gen_noise = generator(gen_input)\n",
    "        # Add to the synthetic images\n",
    "        gen_imgs = 0.5 * (synthethic_imgs + gen_noise)\n",
    "        # Discriminate the fake images\n",
    "        _, verdict = discriminator(gen_imgs)\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        \n",
    "        if (i + 1) % train_gen_every == 0:\n",
    "            \n",
    "            optimizer_G.zero_grad()\n",
    "            requires_grad(generator, True)\n",
    "            requires_grad(discriminator, False)\n",
    "            \n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            if adv_loss == 'wgan-gp':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'hinge':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'bce':\n",
    "                g_loss = adversarial_loss(verdict, valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            train_results[\"g_loss\"].append(g_loss.item())\n",
    "            \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        if (i + 1) % train_disc_every == 0:\n",
    "        \n",
    "            optimizer_D.zero_grad()\n",
    "            requires_grad(generator, False)\n",
    "            requires_grad(discriminator, True)\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            _, disc_real = discriminator(real_imgs)\n",
    "            _, disc_synth = discriminator(gen_imgs.detach())\n",
    "            _, disc_synth_true = discriminator(synthethic_imgs)\n",
    "            \n",
    "            train_results[\"real_acc\"].append(((disc_real > threshold) == valid).float().mean().item())\n",
    "            train_results[\"syn_acc\"].append(((disc_synth > threshold) == fake).float().mean().item())\n",
    "            train_results[\"syn_true_acc\"].append(((disc_synth_true > threshold) == fake).float().mean().item())\n",
    "            \n",
    "            if adv_loss == 'wgan-gp':\n",
    "                real_loss = -torch.mean(disc_real)\n",
    "                fake_loss = disc_synth.mean() \n",
    "                fake_loss += disc_synth_true.mean()\n",
    "            elif adv_loss == 'hinge':\n",
    "                real_loss = torch.nn.ReLU()(1.0 - disc_real).mean()\n",
    "                fake_loss = torch.nn.ReLU()(1.0 + disc_synth).mean() \n",
    "                fake_loss += torch.nn.ReLU()(1.0 + disc_synth_true).mean()\n",
    "            elif adv_loss == 'bce':\n",
    "                real_loss = adversarial_loss(disc_real, valid)\n",
    "                fake_loss = adversarial_loss(disc_synth, fake) \n",
    "                fake_loss += adversarial_loss(disc_synth_true, fake)\n",
    "            d_loss = real_loss + fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            train_results[\"d_loss\"].append(d_loss.item())\n",
    "\n",
    "            if adv_loss == 'wgan-gp':\n",
    "                # Compute gradient penalty\n",
    "                alpha = torch.rand(real_imgs.size(0), 1, 1, 1).cuda().expand_as(real_imgs)\n",
    "                interpolated = Variable(alpha * real_imgs.data + (1 - alpha) * gen_imgs.data, requires_grad=True)\n",
    "                out = discriminator(interpolated)[1]\n",
    "\n",
    "                grad = torch.autograd.grad(outputs=out,\n",
    "                                           inputs=interpolated,\n",
    "                                           grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                           retain_graph=True,\n",
    "                                           create_graph=True,\n",
    "                                           only_inputs=True)[0]\n",
    "\n",
    "                grad = grad.view(grad.size(0), -1)\n",
    "                grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "                d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "\n",
    "                # Backward + Optimize\n",
    "                d_loss_reg = lambda_gp * d_loss_gp\n",
    "\n",
    "                optimizer_D.zero_grad()\n",
    "                d_loss_reg.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "                train_results[\"d_reg\"].append(d_loss_reg.item())\n",
    "\n",
    "        \n",
    "        print_str =  f'Epoch {epoch}'\n",
    "        print_str += f' D_loss {np.mean(train_results[\"d_loss\"]):.8f}'\n",
    "        print_str += f' D_reg {np.mean(train_results[\"d_reg\"]):.8f}'\n",
    "        print_str += f' G_loss {np.mean(train_results[\"g_loss\"]):8f}'\n",
    "        print_str += f' h_acc {np.mean(train_results[\"real_acc\"]):.6f}'\n",
    "        print_str += f' s_pred_acc {np.mean(train_results[\"syn_acc\"]):.6f}'\n",
    "        print_str += f' s_true_acc {np.mean(train_results[\"syn_true_acc\"]):.6f}'\n",
    "        dual_iter.set_description(print_str)\n",
    "        dual_iter.refresh()\n",
    "        \n",
    "        if i == batches_per_epoch and i > 0:\n",
    "            break\n",
    "        \n",
    "    # Epoch is over. Save some stuff.\n",
    "    save_image(synthethic_imgs.data[:16], f'{conf[\"save_loc\"]}/images/synth_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(real_imgs.data[:16], f'{conf[\"save_loc\"]}/images/real_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(gen_imgs.data[:16], f'{conf[\"save_loc\"]}/images/pred_{epoch}.png', nrow=4, normalize=True)\n",
    "    #save_image(gen_noise.data[:16], f\"../results/gan/images/noise_{epoch}.png\", nrow=4, normalize=True)\n",
    "            \n",
    "\n",
    "    # Save the dataframe to disk\n",
    "    results[\"epoch\"].append(epoch)\n",
    "    results[\"d_loss\"].append(np.mean(train_results[\"d_loss\"]))\n",
    "    results[\"d_loss_reg\"].append(np.mean(train_results[\"d_reg\"]))\n",
    "    results[\"g_loss\"].append(np.mean(train_results[\"g_loss\"]))\n",
    "    results[\"real_acc\"].append(np.mean(train_results[\"real_acc\"]))\n",
    "    results[\"pred_synth_acc\"].append(np.mean(train_results[\"syn_acc\"]))\n",
    "    results[\"true_synth_acc\"].append(np.mean(train_results[\"syn_true_acc\"]))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results).reset_index()\n",
    "    df.to_csv(f'{conf[\"save_loc\"]}/training_log.csv', index=False)\n",
    "    \n",
    "    # Save the model\n",
    "    state_dict = {\n",
    "        'epoch': epoch,\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "    }\n",
    "    torch.save(state_dict, f'{conf[\"save_loc\"]}/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt, pandas as pd, numpy as np\n",
    "\n",
    "# with open(f'{conf[\"save_loc\"]}/training_log.csv', \"r\") as fid:\n",
    "#     lines = np.array([[float(g) for g in f.strip(\"\\n\").split(\",\")] for f in fid.readlines()])\n",
    "# batch_updates = range(len(lines[:, 0]))\n",
    "\n",
    "# plt.plot(batch_updates, lines[:, 2])\n",
    "# plt.plot(batch_updates, lines[:, 3])\n",
    "# plt.plot(batch_updates, lines[:, 4])\n",
    "# plt.legend([\"d-loss\", \"d-reg-loss\", \"g-loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other custom examples of architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "\n",
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self, in_dim, activation):\n",
    "        super(Self_Attn,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.query_conv = SpectralNorm(nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1))\n",
    "        self.key_conv = SpectralNorm(nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1))\n",
    "        self.value_conv = SpectralNorm(nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1))\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1) #\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N) \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        \n",
    "        out = self.gamma*out + x\n",
    "        return out, attention\n",
    "    \n",
    "# def weights_init_normal(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find(\"Conv\") != -1:\n",
    "#         torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "#     elif classname.find(\"BatchNorm2d\") != -1:\n",
    "#         torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "#         torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "# def weights_init_normal(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         torch.nn.init.xavier_normal_(m.weight)\n",
    "#         m.bias.data.zero_()\n",
    "#     elif isinstance(m, nn.Conv2d):\n",
    "#         pass\n",
    "#     elif isinstance(m, torch.nn.GRU or torch.nn.LSTM):\n",
    "#         for name, param in m.named_parameters():\n",
    "#             if 'bias' in name:\n",
    "#                 torch.nn.init.constant_(param, 0.0)\n",
    "#             elif 'weight_ih' in name:\n",
    "#                 torch.nn.init.kaiming_normal_(param)\n",
    "#             elif 'weight_hh' in name:\n",
    "#                 torch.nn.init.orthogonal_(param)\n",
    "#     elif isinstance(m, torch.nn.BatchNorm2d or torch.nn.BatchNorm1d):\n",
    "#         m.weight.data.fill_(1)\n",
    "#         m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = tile_size // 4\n",
    "        self.l1 = nn.Sequential(SpectralNorm(nn.Linear(latent_dim, 128 * self.init_size ** 2)))\n",
    "        #self.attn1 = Self_Attn( 128 * 8, 'relu')\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            SpectralNorm(nn.Conv2d(128, 128, 3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            SpectralNorm(nn.Conv2d(128, 64, 3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            SpectralNorm(nn.Conv2d(64, 1, 3, stride=1, padding=1)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "    \n",
    "    \n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [SpectralNorm(nn.Conv2d(in_filters, out_filters, 3, 2, 1)), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.init_size = tile_size // 4\n",
    "        self.l1 = nn.Sequential(SpectralNorm(nn.Linear(latent_dim, 128 * self.init_size ** 2)))\n",
    "        #self.attn1 = Self_Attn( 128 * 8, 'relu')\n",
    "    \n",
    "        self.downsample = nn.Sequential(\n",
    "            *discriminator_block(1, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "        \n",
    "        # The height and width of downsampled image\n",
    "        ds_size = tile_size // 2 ** 4\n",
    "        self.down_layer = nn.Sequential(SpectralNorm(nn.Linear(128 * ds_size ** 2, latent_dim)))\n",
    "        self.up_layer = nn.Sequential(SpectralNorm(nn.Linear(2 * latent_dim, 128 * self.init_size ** 2)))\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            SpectralNorm(nn.Conv2d(128, 128, 3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            SpectralNorm(nn.Conv2d(128, 64, 3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            SpectralNorm(nn.Conv2d(64, 1, 3, stride=1, padding=1)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, im, z):\n",
    "        out = self.downsample(im)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.down_layer(out)\n",
    "        out = torch.cat([out, z], 1)\n",
    "        out = self.up_layer(out)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.upsample(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [SpectralNorm(nn.Conv2d(in_filters, out_filters, 3, 2, 1)), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(1, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = tile_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity\n",
    "    \n",
    "    \n",
    "    \n",
    "# class Generator(nn.Module):\n",
    "#     \"\"\"Generator.\"\"\"\n",
    "\n",
    "#     def __init__(self, batch_size = 32, image_size=512, z_dim=128, conv_dim=64):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.imsize = image_size\n",
    "#         layer1 = []\n",
    "#         layer2 = []\n",
    "#         layer3 = []\n",
    "#         layer4 = []\n",
    "#         layer5 = []\n",
    "#         layer6 = []\n",
    "#         layer7 = []\n",
    "#         last = []\n",
    "\n",
    "#         repeat_num = int(np.log2(self.imsize)) - 3\n",
    "#         mult = 2 ** repeat_num # 8\n",
    "#         layer1.append(SpectralNorm(nn.ConvTranspose2d(z_dim, conv_dim * mult, 4)))\n",
    "#         layer1.append(nn.BatchNorm2d(conv_dim * mult))\n",
    "#         layer1.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "#         #layer1.append(nn.Dropout2d(0.25))\n",
    "\n",
    "#         curr_dim = conv_dim * mult\n",
    "\n",
    "#         layer2.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "#         layer2.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "#         layer2.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "#         #layer2.append(nn.Dropout2d(0.25))\n",
    "\n",
    "#         curr_dim = int(curr_dim / 2)\n",
    "\n",
    "#         layer3.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "#         layer3.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "#         layer3.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "#         #layer3.append(nn.Dropout2d(0.25))\n",
    "        \n",
    "#         curr_dim = int(curr_dim / 2)\n",
    "\n",
    "#         layer4.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "#         layer4.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "#         layer4.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "#         #layer4.append(nn.Dropout2d(0.25))\n",
    "        \n",
    "#         curr_dim = int(curr_dim / 2)\n",
    "\n",
    "#         layer5.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "#         layer5.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "#         layer5.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "#         #layer5.append(nn.Dropout2d(0.25))\n",
    "        \n",
    "#         curr_dim = int(curr_dim / 2)\n",
    "\n",
    "#         layer6.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "#         layer6.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "#         layer6.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "#         #layer6.append(nn.Dropout2d(0.25))\n",
    "        \n",
    "#         curr_dim = int(curr_dim / 2)\n",
    "        \n",
    "#         layer7.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "#         layer7.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "#         layer7.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "#         #layer7.append(nn.Dropout2d(0.25))\n",
    "        \n",
    "#         curr_dim = int(curr_dim / 2)\n",
    "\n",
    "#         self.l1 = nn.Sequential(*layer1)\n",
    "#         self.l2 = nn.Sequential(*layer2)\n",
    "#         self.l3 = nn.Sequential(*layer3)\n",
    "#         self.l4 = nn.Sequential(*layer4)\n",
    "#         self.l5 = nn.Sequential(*layer5)\n",
    "#         self.l6 = nn.Sequential(*layer6)\n",
    "#         self.l7 = nn.Sequential(*layer7)\n",
    "\n",
    "#         last.append(nn.ConvTranspose2d(curr_dim, 1, 4, 2, 1))\n",
    "#         last.append(nn.Tanh())\n",
    "#         self.last = nn.Sequential(*last)\n",
    "\n",
    "#         self.attn1 = Self_Attn( 512 * 8, 'relu')\n",
    "#         self.attn2 = Self_Attn( 256 * 8, 'relu')\n",
    "#         self.attn3 = Self_Attn( 128 * 8, 'relu')\n",
    "#         self.attn4 = Self_Attn( 64 * 8,  'relu')\n",
    "#         #self.attn4 = Self_Attn( 32 * 8, 'relu')\n",
    "#         #self.attn5 = Self_Attn( 16 * 8, 'relu')\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "#         out=self.l1(z)\n",
    "#         #out, _ = self.attn1(out)\n",
    "#         out=self.l2(out)\n",
    "#         #out, _ = self.attn2(out)\n",
    "#         out=self.l3(out)\n",
    "#         #out, _ = self.attn3(out)\n",
    "#         out=self.l4(out)\n",
    "#         #out, _ = self.attn4(out)\n",
    "#         out = self.l5(out)\n",
    "#         #out,p5 = self.attn5(out)\n",
    "#         out = self.l6(out)\n",
    "#         out = self.l7(out)\n",
    "#         out=self.last(out)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     \"\"\"Discriminator, Auxiliary Classifier.\"\"\"\n",
    "\n",
    "#     def __init__(self, color_dim = 1, batch_size=32, image_size=512, conv_dim=64):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.imsize = image_size\n",
    "#         layer1 = []\n",
    "#         layer2 = []\n",
    "#         layer3 = []\n",
    "#         layer4 = []\n",
    "#         layer5 = []\n",
    "#         layer6 = []\n",
    "#         layer7 = []\n",
    "#         last = []\n",
    "\n",
    "#         layer1.append(SpectralNorm(nn.Conv2d(color_dim, conv_dim, 4, 2, 1)))\n",
    "#         layer1.append(nn.LeakyReLU(0.1))\n",
    "#         layer1.append(nn.Dropout2d(0.25))\n",
    "\n",
    "#         curr_dim = conv_dim\n",
    "\n",
    "#         layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "#         layer2.append(nn.LeakyReLU(0.1))\n",
    "#         layer2.append(nn.Dropout2d(0.25))\n",
    "#         curr_dim = curr_dim * 2\n",
    "\n",
    "#         layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "#         layer3.append(nn.LeakyReLU(0.1))\n",
    "#         layer3.append(nn.Dropout2d(0.25))\n",
    "#         curr_dim = curr_dim * 2\n",
    "        \n",
    "#         layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "#         layer4.append(nn.LeakyReLU(0.1))\n",
    "#         layer4.append(nn.Dropout2d(0.25))\n",
    "#         curr_dim = curr_dim * 2\n",
    "        \n",
    "#         layer5.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "#         layer5.append(nn.LeakyReLU(0.1))\n",
    "#         layer5.append(nn.Dropout2d(0.25))\n",
    "#         curr_dim = curr_dim * 2\n",
    "        \n",
    "#         layer6.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "#         layer6.append(nn.LeakyReLU(0.1))\n",
    "#         layer6.append(nn.Dropout2d(0.25))\n",
    "#         curr_dim = curr_dim * 2\n",
    "        \n",
    "#         layer7.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "#         layer7.append(nn.LeakyReLU(0.1))\n",
    "#         layer7.append(nn.Dropout2d(0.25))\n",
    "#         curr_dim = curr_dim * 2\n",
    "            \n",
    "#         self.l1 = nn.Sequential(*layer1)\n",
    "#         self.l2 = nn.Sequential(*layer2)\n",
    "#         self.l3 = nn.Sequential(*layer3)\n",
    "#         self.l4 = nn.Sequential(*layer4)\n",
    "#         self.l5 = nn.Sequential(*layer5)\n",
    "#         self.l6 = nn.Sequential(*layer6)\n",
    "#         self.l7 = nn.Sequential(*layer7)\n",
    "\n",
    "#         last.append(nn.Conv2d(curr_dim, 1, 4))\n",
    "#         self.last = nn.Sequential(*last)\n",
    "\n",
    "#         #self.attn1 = Self_Attn(128, 'relu')\n",
    "#         #self.attn2 = Self_Attn(256, 'relu')\n",
    "#         self.attn3 = Self_Attn(512, 'relu')\n",
    "#         self.attn4 = Self_Attn(1024, 'relu')\n",
    "#         self.attn5 = Self_Attn(2048, 'relu')\n",
    "#         self.attn6 = Self_Attn(4096, 'relu')\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.l1(x)\n",
    "#         out = self.l2(out)\n",
    "#         #out, p1 = self.attn1(out)\n",
    "#         out = self.l3(out)\n",
    "#         #out, p2 = self.attn2(out)\n",
    "#         out = self.l4(out)\n",
    "#         #out, _ = self.attn3(out)\n",
    "#         out = self.l5(out)\n",
    "#         #out, _ = self.attn4(out)\n",
    "#         out = self.l6(out)\n",
    "#         #out, _ = self.attn5(out)\n",
    "#         out = self.l7(out)\n",
    "#         #out, _ = self.attn6(out)\n",
    "#         out = self.last(out)\n",
    "\n",
    "#         return out.squeeze()\n",
    "    \n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
