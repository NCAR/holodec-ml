{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.fft\n",
    "import subprocess\n",
    "import logging\n",
    "import random\n",
    "import shutil\n",
    "import psutil\n",
    "import sklearn\n",
    "import scipy\n",
    "import torch\n",
    "import copy\n",
    "import yaml\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from holodecml.data import PickleReader, UpsamplingReader, XarrayReader\n",
    "from holodecml.propagation import InferencePropagator\n",
    "from holodecml.transforms import LoadTransformations\n",
    "from holodecml.models import load_model\n",
    "from holodecml.losses import load_loss\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "available_ncpus = len(psutil.Process().cpu_affinity())\n",
    "\n",
    "# Set up the GPU\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\") if not is_cuda else torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set seeds for reproducibility\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/gan.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "save_loc = conf[\"save_loc\"]\n",
    "os.makedirs(save_loc, exist_ok = True)\n",
    "os.makedirs(os.path.join(save_loc, \"images\"), exist_ok = True)\n",
    "shutil.copyfile(config, os.path.join(save_loc, \"model.yml\"))\n",
    "\n",
    "tile_size = int(conf[\"data\"][\"tile_size\"])\n",
    "step_size = int(conf[\"data\"][\"step_size\"])\n",
    "data_path = conf[\"data\"][\"output_path\"]\n",
    "data_path_raw = conf[\"data\"][\"output_path_raw\"]\n",
    "\n",
    "total_positive = int(conf[\"data\"][\"total_positive\"])\n",
    "total_negative = int(conf[\"data\"][\"total_negative\"])\n",
    "total_examples = int(conf[\"data\"][\"total_training\"])\n",
    "\n",
    "transform_mode = \"None\" if \"transform_mode\" not in conf[\"data\"] else conf[\"data\"][\"transform_mode\"]\n",
    "config_ncpus = int(conf[\"data\"][\"cores\"])\n",
    "use_cached = False if \"use_cached\" not in conf[\"data\"] else conf[\"data\"][\"use_cached\"]\n",
    "\n",
    "name_tag = f\"{tile_size}_{step_size}_{total_positive}_{total_negative}_{total_examples}_{transform_mode}\"\n",
    "fn_train = f\"{data_path}/training_{name_tag}.nc\"\n",
    "fn_valid = f\"{data_path}/validation_{name_tag}.nc\"\n",
    "fn_train_raw = f\"{data_path_raw}/training_{name_tag}.nc\"\n",
    "fn_valid_raw = f\"{data_path_raw}/validation_{name_tag}.nc\"\n",
    "\n",
    "# Trainer params\n",
    "train_batch_size = conf[\"trainer\"][\"train_batch_size\"]\n",
    "valid_batch_size = conf[\"trainer\"][\"valid_batch_size\"]\n",
    "\n",
    "epochs = conf[\"trainer\"][\"epochs\"]\n",
    "batches_per_epoch = conf[\"trainer\"][\"batches_per_epoch\"]\n",
    "Tensor = torch.cuda.FloatTensor if is_cuda else torch.FloatTensor\n",
    "adv_loss = conf[\"trainer\"][\"adv_loss\"]\n",
    "lambda_gp = conf[\"trainer\"][\"lambda_gp\"]\n",
    "mask_penalty = conf[\"trainer\"][\"mask_penalty\"]\n",
    "regression_penalty = conf[\"trainer\"][\"regression_penalty\"]\n",
    "train_gen_every = conf[\"trainer\"][\"train_gen_every\"]\n",
    "train_disc_every = conf[\"trainer\"][\"train_disc_every\"]\n",
    "threshold = conf[\"trainer\"][\"threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing transforms\n",
    "if \"Normalize\" in conf[\"transforms\"][\"training\"]:\n",
    "    conf[\"transforms\"][\"validation\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "    conf[\"transforms\"][\"inference\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "\n",
    "train_transforms = LoadTransformations(conf[\"transforms\"][\"training\"])\n",
    "valid_transforms = LoadTransformations(conf[\"transforms\"][\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_dataset = XarrayReader(fn_train, train_transforms)\n",
    "test_synthetic_dataset = XarrayReader(fn_valid, valid_transforms)\n",
    "\n",
    "# train_synthetic_dataset = PickleReader(\n",
    "#     fn_train,\n",
    "#     transform=train_transforms,\n",
    "#     max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "#     max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "#     color_dim=conf[\"discriminator\"][\"in_channels\"],\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# test_synthetic_dataset = PickleReader(\n",
    "#     fn_valid,\n",
    "#     transform=valid_transforms,\n",
    "#     max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "#     max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "#     color_dim=conf[\"discriminator\"][\"in_channels\"],\n",
    "#     shuffle=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    train_synthetic_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=available_ncpus//2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    test_synthetic_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_dataset = XarrayReader(fn_train_raw, train_transforms)\n",
    "test_holodec_dataset = XarrayReader(fn_valid_raw, valid_transforms)\n",
    "\n",
    "# train_holodec_dataset = PickleReader(\n",
    "#     fn_train_raw,\n",
    "#     transform=train_transforms,\n",
    "#     max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "#     max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "#     color_dim=conf[\"discriminator\"][\"in_channels\"],\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# test_holodec_dataset = PickleReader(\n",
    "#     fn_valid_raw,\n",
    "#     transform=valid_transforms,\n",
    "#     max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "#     max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "#     color_dim=conf[\"discriminator\"][\"in_channels\"],\n",
    "#     shuffle=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_loader = torch.utils.data.DataLoader(\n",
    "    train_holodec_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=available_ncpus//2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_holodec_loader = torch.utils.data.DataLoader(\n",
    "    test_holodec_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load manual label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_transforms(transforms, image):\n",
    "#     im = {\"image\": image}\n",
    "#     for image_transform in transforms:\n",
    "#         im = image_transform(im)\n",
    "#     image = im[\"image\"]\n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = torch.from_numpy(np.load(os.path.join(\n",
    "#     data_path, f'manual_images_{transform_mode}.npy'))).float()\n",
    "# labels = torch.from_numpy(np.load(os.path.join(\n",
    "#     data_path, f'manual_labels_{transform_mode}.npy'))).float()\n",
    "# labels = torch.from_numpy(np.expand_dims(\n",
    "#     np.vstack([apply_transforms(valid_transforms, x) for x in inputs.numpy()]), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = load_model(conf[\"generator\"]).to(device) #ConditionalGenerator(latent_dim = 512, img_shape = (512, 512)).to(device) \n",
    "discriminator = load_model(conf[\"discriminator\"]).to(device)\n",
    "model = load_model(conf[\"discriminator\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /glade/work/schreck/py37/lib/python3.7/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "adv_loss = conf[\"trainer\"][\"adv_loss\"]\n",
    "if adv_loss == \"bce\":\n",
    "    adversarial_loss = torch.nn.BCELoss().to(device)\n",
    "    \n",
    "perceptual_alex = lpips.LPIPS(net='alex').cuda()\n",
    "\n",
    "Mask_Loss = load_loss(\"focal-tyversky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, generator.parameters()),\n",
    "    lr = conf[\"optimizer_G\"][\"learning_rate\"],\n",
    "    betas = (conf[\"optimizer_G\"][\"b0\"], conf[\"optimizer_G\"][\"b1\"]))\n",
    "\n",
    "optimizer_D = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, discriminator.parameters()), \n",
    "    lr = conf[\"optimizer_D\"][\"learning_rate\"], \n",
    "    betas = (conf[\"optimizer_D\"][\"b0\"], conf[\"optimizer_D\"][\"b1\"]))\n",
    "\n",
    "model_D = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr = conf[\"optimizer_D\"][\"learning_rate\"],\n",
    "    betas = (conf[\"optimizer_D\"][\"b0\"], conf[\"optimizer_D\"][\"b1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_imgs, gen_imgs):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    alpha = torch.rand(real_imgs.size(0), 1, 1, 1).cuda().expand_as(real_imgs)\n",
    "    interpolated = Variable(alpha * real_imgs.data + (1 - alpha) * gen_imgs.data, requires_grad=True)\n",
    "    out = discriminator(interpolated)[1]\n",
    "    grad = torch.autograd.grad(outputs=out,\n",
    "                               inputs=interpolated,\n",
    "                               grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                               retain_graph=True,\n",
    "                               create_graph=True,\n",
    "                               only_inputs=True)[0]\n",
    "    grad = grad.view(grad.size(0), -1)\n",
    "    grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "    d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "    return d_loss_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_G_decay = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=30, gamma=0.2)\n",
    "lr_D_decay = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=30, gamma=0.2)\n",
    "lr_M_decay = torch.optim.lr_scheduler.StepLR(model_D, step_size=30, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 D_loss 1.972601 G_loss 0.117701 G_reg 0.143417 mask_loss 0.983068 h_acc 0.5175 f_acc 0.5025 s_acc 0.2925 p_syn 0.7502 p_real 0.5919: 100%|██████████| 250/250 [05:18<00:00,  1.28s/it]\n",
      "Epoch 1 D_loss 1.970919 G_loss -0.296741 G_reg 0.085400 mask_loss 0.388507 h_acc 0.6350 f_acc 0.3975 s_acc 0.4175 p_syn 0.7344 p_real 0.5666: 100%|██████████| 250/250 [05:09<00:00,  1.24s/it]\n",
      "Epoch 2 D_loss 1.967794 G_loss -0.465541 G_reg 0.103107 mask_loss 0.195684 h_acc 0.7000 f_acc 0.3575 s_acc 0.4350 p_syn 0.8043 p_real 0.5274: 100%|██████████| 250/250 [05:09<00:00,  1.24s/it]\n",
      "Epoch 3 D_loss 1.960306 G_loss -0.292921 G_reg 0.122094 mask_loss 0.191703 h_acc 0.5125 f_acc 0.5275 s_acc 0.2225 p_syn 0.8164 p_real 0.5451: 100%|██████████| 250/250 [05:09<00:00,  1.24s/it]\n",
      "Epoch 4 D_loss 1.992321 G_loss -0.422636 G_reg 0.096733 mask_loss 0.142640 h_acc 0.6075 f_acc 0.4000 s_acc 0.2575 p_syn 0.8215 p_real 0.5147: 100%|██████████| 250/250 [05:10<00:00,  1.24s/it]\n",
      "Epoch 5 D_loss 1.996202 G_loss -0.493545 G_reg 0.118023 mask_loss 0.124626 h_acc 0.7100 f_acc 0.3075 s_acc 0.1900 p_syn 0.8170 p_real 0.5231: 100%|██████████| 250/250 [05:09<00:00,  1.24s/it]\n",
      "Epoch 6 D_loss 1.989006 G_loss -0.295662 G_reg 0.129092 mask_loss 0.134153 h_acc 0.4625 f_acc 0.5575 s_acc 0.0925 p_syn 0.7879 p_real 0.5156: 100%|██████████| 250/250 [05:10<00:00,  1.24s/it]\n",
      "Epoch 7 D_loss 2.027391 G_loss -0.197484 G_reg 0.137096 mask_loss 0.144018 h_acc 0.2625 f_acc 0.6775 s_acc 0.0450 p_syn 0.8698 p_real 0.5006: 100%|██████████| 250/250 [05:09<00:00,  1.24s/it]\n",
      "Epoch 8 D_loss 1.991527 G_loss -0.155286 G_reg 0.142037 mask_loss 0.116753 h_acc 0.2125 f_acc 0.7775 s_acc 0.0100 p_syn 0.8679 p_real 0.4965: 100%|██████████| 250/250 [05:09<00:00,  1.24s/it]\n",
      "Epoch 9 D_loss 1.993550 G_loss -0.207787 G_reg 0.175892 mask_loss 0.144905 h_acc 0.3500 f_acc 0.6375 s_acc 0.0225 p_syn 0.8483 p_real 0.5328: 100%|██████████| 250/250 [05:10<00:00,  1.24s/it]\n",
      "Epoch 10 D_loss 1.989334 G_loss -0.357362 G_reg 0.135245 mask_loss 0.132389 h_acc 0.5225 f_acc 0.4775 s_acc 0.0675 p_syn 0.9037 p_real 0.4920: 100%|██████████| 250/250 [05:10<00:00,  1.24s/it]\n",
      "Epoch 11 D_loss 1.961754 G_loss -0.405823 G_reg 0.162194 mask_loss 0.114253 h_acc 0.6250 f_acc 0.4350 s_acc 0.1600 p_syn 0.7953 p_real 0.4953: 100%|██████████| 250/250 [05:09<00:00,  1.24s/it]\n",
      "Epoch 12 D_loss 1.940588 G_loss -0.404248 G_reg 0.166428 mask_loss 0.109511 h_acc 0.6600 f_acc 0.4175 s_acc 0.1250 p_syn 0.8296 p_real 0.5089: 100%|██████████| 250/250 [05:09<00:00,  1.24s/it]\n",
      "Epoch 13 D_loss 1.985959 G_loss -0.406209 G_reg 0.129341 mask_loss 0.101999 h_acc 0.6200 f_acc 0.3900 s_acc 0.1525 p_syn 0.9089 p_real 0.4945: 100%|██████████| 250/250 [05:10<00:00,  1.24s/it]\n",
      "Epoch 14 D_loss 1.984287 G_loss -0.436116 G_reg 0.135890 mask_loss 0.114085 h_acc 0.6275 f_acc 0.4100 s_acc 0.2825 p_syn 0.9727 p_real 0.5229: 100%|██████████| 250/250 [05:10<00:00,  1.24s/it]\n",
      "Epoch 15 D_loss 1.944731 G_loss -0.486487 G_reg 0.165615 mask_loss 0.123143 h_acc 0.7650 f_acc 0.3200 s_acc 0.1725 p_syn 0.8951 p_real 0.5101: 100%|██████████| 250/250 [05:09<00:00,  1.24s/it]\n",
      "Epoch 16 D_loss 1.921933 G_loss -0.218100 G_reg 0.148208 mask_loss 0.113825 h_acc 0.4200 f_acc 0.6925 s_acc 0.0100 p_syn 0.8863 p_real 0.5192: 100%|██████████| 250/250 [05:10<00:00,  1.24s/it]\n",
      "Epoch 17 D_loss 2.004859 G_loss -0.279240 G_reg 0.149109 mask_loss 0.112636 h_acc 0.4437 f_acc 0.5125 s_acc 0.0250 p_syn 0.8738 p_real 0.5142:  43%|████▎     | 108/250 [02:14<02:55,  1.23s/it]"
     ]
    }
   ],
   "source": [
    "results = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    ### Train\n",
    "    real_images = iter(train_holodec_loader)\n",
    "    synthethic_images = iter(train_synthetic_loader)\n",
    "    dual_iter = tqdm.tqdm(\n",
    "        enumerate(zip(real_images, synthethic_images)),\n",
    "        total = batches_per_epoch, \n",
    "        leave = True)\n",
    "    \n",
    "    train_results = defaultdict(list)\n",
    "    for i, ((holo_img, holo_label), (synth_img, synth_label)) in dual_iter:\n",
    "            \n",
    "        if holo_img.shape[0] != synth_img.shape[0]:\n",
    "            continue\n",
    "                            \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(holo_img.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(holo_img.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(holo_img.type(Tensor))\n",
    "        synthethic_imgs = Variable(synth_img.type(Tensor))\n",
    "                \n",
    "        # Sample noise as generator input\n",
    "        #z = Variable(Tensor(np.random.normal(0, 1.0, (holo_img.shape[0], 512))))\n",
    "        z = Variable(Tensor(np.random.normal(0, 1.0, synthethic_imgs.shape)))\n",
    "        # C-GAN-like input using the synthethic image as conditional input\n",
    "        gen_input = torch.cat([synthethic_imgs, z], 1)\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(gen_input)\n",
    "        # Discriminate the fake images\n",
    "        pred_masks, verdict = discriminator(gen_imgs)\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train mask model\n",
    "        # -----------------\n",
    "        \n",
    "        model_D.zero_grad()\n",
    "        requires_grad(model, True)\n",
    "        pred_masks, _ = model(gen_imgs.detach())\n",
    "        mask_loss = Mask_Loss(pred_masks, synth_label.to(device))\n",
    "        train_results[\"mask_loss\"].append(mask_loss.item())\n",
    "        mask_loss.backward()\n",
    "        model_D.step()\n",
    "        requires_grad(model, False)\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "            \n",
    "        if (i + 1) % train_gen_every == 0:\n",
    "            \n",
    "            optimizer_G.zero_grad()\n",
    "            requires_grad(generator, True)\n",
    "            requires_grad(discriminator, False)\n",
    "            \n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            if adv_loss == 'wgan-gp':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'hinge':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'bce':\n",
    "                g_loss = adversarial_loss(verdict, valid)\n",
    "                \n",
    "            # compute L1/L2 reg term\n",
    "#             mask = synth_label.to(device).bool()\n",
    "#             reg_loss = torch.nn.MSELoss()(\n",
    "#                 torch.masked_select(gen_imgs, mask),\n",
    "#                 torch.masked_select(synthethic_imgs, mask)\n",
    "#             )\n",
    "\n",
    "            mask = synth_label.to(device)\n",
    "            reg_loss = torch.nn.MSELoss()(gen_imgs, synthethic_imgs)\n",
    "\n",
    "            train_results[\"g_reg\"].append(reg_loss.item())\n",
    "            g_loss += regression_penalty * reg_loss\n",
    "            \n",
    "            # compute mask loss reg term\n",
    "            pred_masks, _ = model(gen_imgs)\n",
    "            mask_loss = Mask_Loss(pred_masks, synth_label.to(device))\n",
    "            train_results[\"fake_mask_loss\"].append(mask_loss.item())\n",
    "            g_loss += mask_penalty * mask_loss\n",
    "            train_results[\"g_loss\"].append(g_loss.item())\n",
    "                \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # compute perception scores\n",
    "            p_score_syn = perceptual_alex(gen_imgs, synthethic_imgs).mean()\n",
    "            p_score_real = perceptual_alex(gen_imgs, real_imgs).mean()\n",
    "            train_results[\"p_syn\"].append(p_score_syn.item())\n",
    "            train_results[\"p_real\"].append(p_score_real.item())\n",
    "            \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        if (i + 1) % train_disc_every == 0:\n",
    "        \n",
    "            optimizer_D.zero_grad()\n",
    "            requires_grad(generator, False)\n",
    "            requires_grad(discriminator, True)\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            _, disc_real = discriminator(real_imgs)\n",
    "            _, disc_synth = discriminator(gen_imgs.detach())\n",
    "            _, disc_synth_true = discriminator(synthethic_imgs)\n",
    "            \n",
    "            train_results[\"real_acc\"].append(((disc_real > threshold) == valid).float().mean().item())\n",
    "            train_results[\"syn_acc\"].append(((disc_synth > threshold) == fake).float().mean().item())\n",
    "            train_results[\"syn_true_acc\"].append(((disc_synth_true > threshold) == valid).float().mean().item())\n",
    "            \n",
    "            if adv_loss == 'wgan-gp':\n",
    "                real_loss = -torch.mean(disc_real) \n",
    "                fake_loss = disc_synth.mean() \n",
    "            elif adv_loss == 'hinge':\n",
    "                real_loss = torch.nn.ReLU()(1.0 - disc_real).mean() \n",
    "                fake_loss = torch.nn.ReLU()(1.0 + disc_synth).mean()             \n",
    "            elif adv_loss == 'bce':\n",
    "                real_loss = adversarial_loss(disc_real, valid) \n",
    "                fake_loss = adversarial_loss(disc_synth, fake) \n",
    "                \n",
    "            d_loss = real_loss + fake_loss \n",
    "            train_results[\"d_loss\"].append(d_loss.item())\n",
    "            \n",
    "            if adv_loss == 'wgan-gp':\n",
    "                # Compute gradient penalty\n",
    "                alpha = torch.rand(real_imgs.size(0), 1, 1, 1).cuda().expand_as(real_imgs)\n",
    "                interpolated = Variable(alpha * real_imgs.data + (1 - alpha) * gen_imgs.data, requires_grad=True)\n",
    "                out = discriminator(interpolated)[1]\n",
    "\n",
    "                grad = torch.autograd.grad(outputs=out,\n",
    "                                           inputs=interpolated,\n",
    "                                           grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                           retain_graph=True,\n",
    "                                           create_graph=True,\n",
    "                                           only_inputs=True)[0]\n",
    "\n",
    "                grad = grad.view(grad.size(0), -1)\n",
    "                grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "                d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "                d_loss_reg = lambda_gp * d_loss_gp\n",
    "                d_loss += d_loss_reg\n",
    "                train_results[\"d_reg\"].append(d_loss_reg.item())\n",
    "            \n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        print_str =  f'Epoch {epoch}'\n",
    "        print_str += f' D_loss {np.mean(train_results[\"d_loss\"]):.6f}'\n",
    "        if adv_loss == 'wgan-gp':\n",
    "            print_str += f' D_reg {np.mean(train_results[\"d_reg\"]):.6f}'\n",
    "        print_str += f' G_loss {np.mean(train_results[\"g_loss\"]):6f}'\n",
    "        print_str += f' G_reg {np.mean(train_results[\"g_reg\"]):6f}'\n",
    "        print_str += f' mask_loss {np.mean(train_results[\"mask_loss\"]):6f}'\n",
    "        #print_str += f' fake_mask_loss {np.mean(train_results[\"fake_mask_loss\"]):6f}'\n",
    "        print_str += f' h_acc {np.mean(train_results[\"real_acc\"]):.4f}'\n",
    "        print_str += f' f_acc {np.mean(train_results[\"syn_acc\"]):.4f}'\n",
    "        print_str += f' s_acc {np.mean(train_results[\"syn_true_acc\"]):.4f}'\n",
    "        print_str += f' p_syn {np.mean(train_results[\"p_syn\"]):.4f}'\n",
    "        print_str += f' p_real {np.mean(train_results[\"p_real\"]):.4f}'\n",
    "        dual_iter.set_description(print_str)\n",
    "        dual_iter.refresh()\n",
    "        \n",
    "        if i == batches_per_epoch and i > 0:\n",
    "            break\n",
    "        \n",
    "    # Validate \n",
    "        \n",
    "    # Epoch is over. Save some stuff.\n",
    "    save_image(synthethic_imgs.data[:16], f'{conf[\"save_loc\"]}/images/synth_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(real_imgs.data[:16], f'{conf[\"save_loc\"]}/images/real_{epoch}.png', nrow=4, normalize=True)\n",
    "    save_image(gen_imgs.data[:16], f'{conf[\"save_loc\"]}/images/pred_{epoch}.png', nrow=4, normalize=True)\n",
    "\n",
    "    # Save the dataframe to disk\n",
    "    results[\"epoch\"].append(epoch)\n",
    "    results[\"d_loss\"].append(np.mean(train_results[\"d_loss\"]))\n",
    "    if adv_loss == 'wgan-gp':\n",
    "        results[\"d_loss_reg\"].append(np.mean(train_results[\"d_reg\"]))\n",
    "    results[\"g_loss\"].append(np.mean(train_results[\"g_loss\"]))\n",
    "    results[\"g_reg\"].append(np.mean(train_results[\"g_reg\"]))\n",
    "    results[\"mask_loss\"].append(np.mean(train_results[\"mask_loss\"]))\n",
    "    #results[\"fake_mask_loss\"].append(np.mean(train_results[\"fake_mask_loss\"]))\n",
    "    results[\"holo_acc\"].append(np.mean(train_results[\"real_acc\"]))\n",
    "    results[\"fake_acc\"].append(np.mean(train_results[\"syn_acc\"]))\n",
    "    results[\"synth_acc\"].append(np.mean(train_results[\"syn_true_acc\"]))\n",
    "    results[\"perception_syn\"].append(np.mean(train_results[\"p_syn\"]))\n",
    "    results[\"perception_holo\"].append(np.mean(train_results[\"p_real\"]))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results).reset_index()\n",
    "    df.to_csv(f'{conf[\"save_loc\"]}/training_log.csv', index=False)\n",
    "\n",
    "    # Save the model\n",
    "    state_dict = {\n",
    "        'epoch': epoch,\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_optimizer_state_dict': model_D.state_dict()\n",
    "    }\n",
    "    torch.save(state_dict, f'{conf[\"save_loc\"]}/best.pt')\n",
    "    \n",
    "    # Anneal learning rates \n",
    "    lr_G_decay.step(epoch)\n",
    "    lr_D_decay.step(epoch)\n",
    "    lr_M_decay.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Validate on the manual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, sklearn.metrics\n",
    "\n",
    "def man_metrics(results):\n",
    "    result = {}\n",
    "    for metric in [\"f1\", \"auc\", 'pod', \"far\", \"csi\"]: #\"man_prec\", \"man_recall\",\n",
    "        if metric == 'f1':\n",
    "            score = sklearn.metrics.f1_score(results[\"true\"], results[\"pred\"], average = \"weighted\")\n",
    "        elif metric == 'prec':\n",
    "            score = sklearn.metrics.precision_score(results[\"true\"], results[\"pred\"], average = \"weighted\")\n",
    "        elif metric == 'recall':\n",
    "            score = sklearn.metrics.recall_score(results[\"true\"], results[\"pred\"], average = \"weighted\")\n",
    "        elif metric == 'auc':\n",
    "            try:\n",
    "                score = sklearn.metrics.roc_auc_score(results[\"true\"], results[\"pred\"], average = \"weighted\")\n",
    "            except:\n",
    "                score = 1.0\n",
    "        elif metric ==  \"csi\":\n",
    "            try:\n",
    "                TN, FP, FN, TP = sklearn.metrics.confusion_matrix(results[\"true\"], results[\"pred\"]).ravel()\n",
    "                score = TP / (TP + FN + FP)\n",
    "            except:\n",
    "                score = 1\n",
    "        elif metric == 'far':\n",
    "            try:\n",
    "                TN, FP, FN, TP = sklearn.metrics.confusion_matrix(results[\"true\"], results[\"pred\"]).ravel()\n",
    "                score = FP / (TP + FP)\n",
    "            except:\n",
    "                score = 1\n",
    "        elif metric == 'pod':\n",
    "            try:\n",
    "                TN, FP, FN, TP = sklearn.metrics.confusion_matrix(results[\"true\"], results[\"pred\"]).ravel()\n",
    "                score = TP / (TP + FN)\n",
    "            except: \n",
    "                score = 1\n",
    "        result[metric] = score\n",
    "        #print(metric, round(score, 3))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transforms(transforms, image):\n",
    "    im = {\"image\": image}\n",
    "    for image_transform in transforms:\n",
    "        im = image_transform(im)\n",
    "    image = im[\"image\"]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(conf[\"discriminator\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\n",
    "    f\"{save_loc}/best.pt\",\n",
    "    map_location=lambda storage, loc: storage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(np.load(os.path.join(\n",
    "    data_path, f'manual_images_{transform_mode}.npy'))).float()\n",
    "labels = torch.from_numpy(np.load(os.path.join(\n",
    "    data_path, f'manual_labels_{transform_mode}.npy'))).float()\n",
    "inputs = torch.from_numpy(np.expand_dims(\n",
    "    np.vstack([apply_transforms(valid_transforms, x) for x in inputs.numpy()]), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F1: 0.964 POD: 0.981 FAR: 0.093 CSI: 0.891089: 100%|██████████| 1202/1202 [00:18<00:00, 63.79it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dict_set1 = defaultdict(list)\n",
    "my_iter = tqdm.tqdm(enumerate(zip(inputs, labels)), total = inputs.shape[0], leave = True)\n",
    "for k, (x, y) in my_iter:\n",
    "    pred_label, _ = model(x.unsqueeze(0).to(device))\n",
    "    arr, n = scipy.ndimage.label(pred_label.cpu() > 0.5)\n",
    "    centroid = scipy.ndimage.find_objects(arr)\n",
    "    pred_label = len(centroid)\n",
    "    if pred_label > 0 and pred_label <= 10000:\n",
    "        pred_label = 1\n",
    "    else:\n",
    "        pred_label = 0\n",
    "    results_dict_set1[\"pred\"].append(pred_label)\n",
    "    results_dict_set1[\"true\"].append(y[0].item())\n",
    "    mets = man_metrics(results_dict_set1)\n",
    "    f1, pod, far, csi = mets[\"f1\"], mets[\"pod\"], mets[\"far\"], mets[\"csi\"]\n",
    "    my_iter.set_description(f\"F1: {f1:.3f} POD: {pod:.3f} FAR: {far:.3f} CSI: {csi:3f}\")\n",
    "    #my_iter.set_description(f\"Accuracy: {np.mean(results_dict_set1['accuracy'])}\")\n",
    "    my_iter.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1_metrics = man_metrics(results_dict_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.9637782323803324\n",
      "auc 0.9683075266361012\n",
      "pod 0.9809264305177112\n",
      "far 0.09319899244332494\n",
      "csi 0.8910891089108911\n"
     ]
    }
   ],
   "source": [
    "for key, val in set1_metrics.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2 = torch.from_numpy(np.load(os.path.join(\n",
    "    data_path, f'manual_images_{transform_mode}_test.npy'))).float()\n",
    "labels_2 = torch.from_numpy(np.load(os.path.join(\n",
    "    data_path, f'manual_labels_{transform_mode}_test.npy'))).float()\n",
    "confs_2 = torch.from_numpy(np.load(os.path.join(\n",
    "    data_path, f'manual_conf_{transform_mode}_test.npy'))).float()\n",
    "inputs_2 = torch.from_numpy(np.expand_dims(\n",
    "    np.vstack([apply_transforms(valid_transforms, x) for x in inputs_2.numpy()]), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F1: 0.871 POD: 0.894 FAR: 0.070 CSI: 0.837085: 100%|██████████| 1154/1154 [00:17<00:00, 64.17it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dict_set2 = defaultdict(list)\n",
    "my_iter = tqdm.tqdm(enumerate(zip(inputs_2, labels_2)), total = inputs_2.shape[0], leave = True)\n",
    "for k, (x, y) in my_iter:\n",
    "    pred_label, _ = model(x.unsqueeze(0).to(device))\n",
    "    arr, n = scipy.ndimage.label(pred_label.cpu() > 0.5)\n",
    "    centroid = scipy.ndimage.find_objects(arr)\n",
    "    pred_label = len(centroid)\n",
    "    if pred_label > 0 and pred_label <= 10000:\n",
    "        pred_label = 1\n",
    "    else:\n",
    "        pred_label = 0\n",
    "    results_dict_set2[\"pred\"].append(pred_label)\n",
    "    results_dict_set2[\"true\"].append(y[0].item())\n",
    "    mets = man_metrics(results_dict_set2)\n",
    "    f1, pod, far, csi = mets[\"f1\"], mets[\"pod\"], mets[\"far\"], mets[\"csi\"]\n",
    "    my_iter.set_description(f\"F1: {f1:.3f} POD: {pod:.3f} FAR: {far:.3f} CSI: {csi:3f}\")\n",
    "    my_iter.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "set2_metrics = man_metrics(results_dict_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.8707474729226476\n",
      "auc 0.841439195815626\n",
      "pod 0.8935926773455377\n",
      "far 0.07023809523809524\n",
      "csi 0.8370846730975349\n"
     ]
    }
   ],
   "source": [
    "for key, val in set2_metrics.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
