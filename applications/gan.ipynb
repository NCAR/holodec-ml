{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.fft\n",
    "import subprocess\n",
    "import logging\n",
    "import random\n",
    "import shutil\n",
    "import psutil\n",
    "import scipy\n",
    "import torch\n",
    "import copy\n",
    "import yaml\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from holodecml.data import PickleReader, UpsamplingReader\n",
    "from holodecml.propagation import InferencePropagator\n",
    "from holodecml.transforms import LoadTransformations\n",
    "from holodecml.models import load_model\n",
    "from holodecml.losses import load_loss\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_ncpus = len(psutil.Process().cpu_affinity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set seeds for reproducibility\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../results/gan/model.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 1000 if \"seed\" not in conf else conf[\"seed\"]\n",
    "seed_everything(seed)\n",
    "\n",
    "tile_size = int(conf[\"data\"][\"tile_size\"])\n",
    "step_size = int(conf[\"data\"][\"step_size\"])\n",
    "data_path = conf[\"data\"][\"output_path\"]\n",
    "\n",
    "total_positive = int(conf[\"data\"][\"total_positive\"])\n",
    "total_negative = int(conf[\"data\"][\"total_negative\"])\n",
    "total_examples = int(conf[\"data\"][\"total_training\"])\n",
    "\n",
    "transform_mode = \"None\" if \"transform_mode\" not in conf[\"data\"] else conf[\"data\"][\"transform_mode\"]\n",
    "config_ncpus = int(conf[\"data\"][\"cores\"])\n",
    "use_cached = False if \"use_cached\" not in conf[\"data\"] else conf[\"data\"][\"use_cached\"]\n",
    "\n",
    "name_tag = f\"{tile_size}_{step_size}_{total_positive}_{total_negative}_{total_examples}_{transform_mode}\"\n",
    "fn_train = f\"{data_path}/training_{name_tag}.pkl\"\n",
    "fn_valid = f\"{data_path}/validation_{name_tag}.pkl\"\n",
    "\n",
    "output_path = conf[\"data\"][\"output_path\"]\n",
    "\n",
    "train_batch_size = 32\n",
    "valid_batch_size = 32\n",
    "\n",
    "latent_dim = 1024\n",
    "img_shape = (1, tile_size, tile_size)\n",
    "\n",
    "d_learning_rate = 0.0004\n",
    "g_learning_rate = 0.0001\n",
    "\n",
    "b1 = 0.0\n",
    "b2 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "data_device = torch.device(\"cpu\") if \"device\" not in conf[\"data\"] else conf[\"data\"][\"device\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing transforms\n",
    "if \"Normalize\" in conf[\"transforms\"][\"training\"]:\n",
    "    conf[\"transforms\"][\"validation\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "    conf[\"transforms\"][\"inference\"][\"Normalize\"][\"mode\"] = conf[\"transforms\"][\"training\"][\"Normalize\"][\"mode\"]\n",
    "\n",
    "train_transforms = LoadTransformations(conf[\"transforms\"][\"training\"])\n",
    "valid_transforms = LoadTransformations(conf[\"transforms\"][\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_synthetic_dataset = UpsamplingReader(\n",
    "#     conf,\n",
    "#     transform=train_transforms,\n",
    "#     max_size=100,\n",
    "#     device=data_device\n",
    "# )\n",
    "\n",
    "train_synthetic_dataset = PickleReader(\n",
    "    fn_train,\n",
    "    transform=train_transforms,\n",
    "    max_images=int(0.8 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_synthetic_dataset = PickleReader(\n",
    "    fn_valid,\n",
    "    transform=valid_transforms,\n",
    "    max_images=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    max_buffer_size=int(0.1 * conf[\"data\"][\"total_training\"]),\n",
    "    color_dim=conf[\"model\"][\"in_channels\"],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    train_synthetic_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=0, #available_ncpus//2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_synthetic_loader = torch.utils.data.DataLoader(\n",
    "    test_synthetic_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "holo_conf = copy.deepcopy(conf)\n",
    "holo_conf[\"data\"][\"data_path\"] = holo_conf[\"data\"][\"raw_data\"]\n",
    "\n",
    "# class UpsamplingReaderLimited(UpsamplingReader):\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(list(self.prop.h_ds[\"hid\"].values))\n",
    "\n",
    "\n",
    "train_holodec_dataset = UpsamplingReader(\n",
    "    holo_conf,\n",
    "    transform=train_transforms,\n",
    "    max_size=100,\n",
    "    device=data_device\n",
    ")\n",
    "\n",
    "test_holodec_inputs = torch.from_numpy(np.load(os.path.join(\n",
    "            output_path, f'manual_images_{transform_mode}.npy'))).float()\n",
    "test_holodec_labels = torch.from_numpy(np.load(os.path.join(\n",
    "            output_path, f'manual_labels_{transform_mode}.npy'))).float()\n",
    "\n",
    "test_holodec_dataset = torch.utils.data.TensorDataset(test_holodec_inputs, test_holodec_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holodec_loader = torch.utils.data.DataLoader(\n",
    "    train_holodec_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=available_ncpus,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_holodec_loader = torch.utils.data.DataLoader(\n",
    "    test_holodec_dataset,\n",
    "    batch_size=valid_batch_size,\n",
    "    num_workers=0,  # 0 = One worker with the main process\n",
    "    pin_memory=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "\n",
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self, in_dim, activation):\n",
    "        super(Self_Attn,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.query_conv = SpectralNorm(nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1))\n",
    "        self.key_conv = SpectralNorm(nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1))\n",
    "        self.value_conv = SpectralNorm(nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1))\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1) #\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N) \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        \n",
    "        out = self.gamma*out + x\n",
    "        return out, attention\n",
    "    \n",
    "# def weights_init_normal(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find(\"Conv\") != -1:\n",
    "#         torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "#     elif classname.find(\"BatchNorm2d\") != -1:\n",
    "#         torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "#         torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "# def weights_init_normal(m):\n",
    "#     if isinstance(m, nn.Conv2d or nn.Linear):\n",
    "#         torch.nn.init.xavier_normal_(m.weight)\n",
    "#         m.bias.data.zero_()\n",
    "#     elif isinstance(m, torch.nn.GRU or torch.nn.LSTM):\n",
    "#         for name, param in m.named_parameters():\n",
    "#             if 'bias' in name:\n",
    "#                 torch.nn.init.constant_(param, 0.0)\n",
    "#             elif 'weight_ih' in name:\n",
    "#                 torch.nn.init.kaiming_normal_(param)\n",
    "#             elif 'weight_hh' in name:\n",
    "#                 torch.nn.init.orthogonal_(param)\n",
    "#     elif isinstance(m, torch.nn.BatchNorm2d or torch.nn.BatchNorm1d):\n",
    "#         m.weight.data.fill_(1)\n",
    "#         m.bias.data.zero_()\n",
    "\n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Generator, self).__init__()\n",
    "\n",
    "#         self.init_size = tile_size // 4\n",
    "#         self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "#         self.attn1 = Self_Attn( 128 * 8, 'relu')\n",
    "\n",
    "#         self.conv_blocks = nn.Sequential(\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             SpectralNorm(nn.Conv2d(128, 128, 3, stride=1, padding=1)),\n",
    "#             nn.BatchNorm2d(128, 0.8),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             SpectralNorm(nn.Conv2d(128, 64, 3, stride=1, padding=1)),\n",
    "#             nn.BatchNorm2d(64, 0.8),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             SpectralNorm(nn.Conv2d(64, 1, 3, stride=1, padding=1)),\n",
    "#             nn.Tanh(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         out = self.l1(z)\n",
    "#         out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "#         img = self.conv_blocks(out)\n",
    "#         return img\n",
    "\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "\n",
    "#         def discriminator_block(in_filters, out_filters, bn=True):\n",
    "#             block = [SpectralNorm(nn.Conv2d(in_filters, out_filters, 3, 2, 1)), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "#             if bn:\n",
    "#                 block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "#             return block\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             *discriminator_block(1, 16, bn=False),\n",
    "#             *discriminator_block(16, 32),\n",
    "#             *discriminator_block(32, 64),\n",
    "#             *discriminator_block(64, 128),\n",
    "#         )\n",
    "\n",
    "#         # The height and width of downsampled image\n",
    "#         ds_size = tile_size // 2 ** 4\n",
    "#         self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         out = self.model(img)\n",
    "#         out = out.view(out.shape[0], -1)\n",
    "#         validity = self.adv_layer(out)\n",
    "#         return validity\n",
    "    \n",
    "    \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size = 32, image_size=512, z_dim=128, conv_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.imsize = image_size\n",
    "        layer1 = []\n",
    "        layer2 = []\n",
    "        layer3 = []\n",
    "        layer4 = []\n",
    "        layer5 = []\n",
    "        layer6 = []\n",
    "        layer7 = []\n",
    "        last = []\n",
    "\n",
    "        repeat_num = int(np.log2(self.imsize)) - 3\n",
    "        mult = 2 ** repeat_num # 8\n",
    "        layer1.append(SpectralNorm(nn.ConvTranspose2d(z_dim, conv_dim * mult, 4)))\n",
    "        layer1.append(nn.BatchNorm2d(conv_dim * mult))\n",
    "        layer1.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        #layer1.append(nn.Dropout2d(0.25))\n",
    "\n",
    "        curr_dim = conv_dim * mult\n",
    "\n",
    "        layer2.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "        layer2.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "        layer2.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        #layer2.append(nn.Dropout2d(0.25))\n",
    "\n",
    "        curr_dim = int(curr_dim / 2)\n",
    "\n",
    "        layer3.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "        layer3.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "        layer3.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        #layer3.append(nn.Dropout2d(0.25))\n",
    "        \n",
    "        curr_dim = int(curr_dim / 2)\n",
    "\n",
    "        layer4.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "        layer4.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "        layer4.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        #layer4.append(nn.Dropout2d(0.25))\n",
    "        \n",
    "        curr_dim = int(curr_dim / 2)\n",
    "\n",
    "        layer5.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "        layer5.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "        layer5.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        #layer5.append(nn.Dropout2d(0.25))\n",
    "        \n",
    "        curr_dim = int(curr_dim / 2)\n",
    "\n",
    "        layer6.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "        layer6.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "        layer6.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        #layer6.append(nn.Dropout2d(0.25))\n",
    "        \n",
    "        curr_dim = int(curr_dim / 2)\n",
    "        \n",
    "        layer7.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
    "        layer7.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "        layer7.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        #layer7.append(nn.Dropout2d(0.25))\n",
    "        \n",
    "        curr_dim = int(curr_dim / 2)\n",
    "\n",
    "        self.l1 = nn.Sequential(*layer1)\n",
    "        self.l2 = nn.Sequential(*layer2)\n",
    "        self.l3 = nn.Sequential(*layer3)\n",
    "        self.l4 = nn.Sequential(*layer4)\n",
    "        self.l5 = nn.Sequential(*layer5)\n",
    "        self.l6 = nn.Sequential(*layer6)\n",
    "        self.l7 = nn.Sequential(*layer7)\n",
    "\n",
    "        last.append(nn.ConvTranspose2d(curr_dim, 1, 4, 2, 1))\n",
    "        last.append(nn.Tanh())\n",
    "        self.last = nn.Sequential(*last)\n",
    "\n",
    "        self.attn1 = Self_Attn( 512 * 8, 'relu')\n",
    "        self.attn2 = Self_Attn( 256 * 8, 'relu')\n",
    "        self.attn3 = Self_Attn( 128 * 8, 'relu')\n",
    "        self.attn4 = Self_Attn( 64 * 8,  'relu')\n",
    "        #self.attn4 = Self_Attn( 32 * 8, 'relu')\n",
    "        #self.attn5 = Self_Attn( 16 * 8, 'relu')\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "        out=self.l1(z)\n",
    "        #out, _ = self.attn1(out)\n",
    "        out=self.l2(out)\n",
    "        #out, _ = self.attn2(out)\n",
    "        out=self.l3(out)\n",
    "        #out, _ = self.attn3(out)\n",
    "        out=self.l4(out)\n",
    "        #out, _ = self.attn4(out)\n",
    "        out = self.l5(out)\n",
    "        #out,p5 = self.attn5(out)\n",
    "        out = self.l6(out)\n",
    "        out = self.l7(out)\n",
    "        out=self.last(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator, Auxiliary Classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, color_dim = 1, batch_size=32, image_size=512, conv_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.imsize = image_size\n",
    "        layer1 = []\n",
    "        layer2 = []\n",
    "        layer3 = []\n",
    "        layer4 = []\n",
    "        layer5 = []\n",
    "        layer6 = []\n",
    "        layer7 = []\n",
    "        last = []\n",
    "\n",
    "        layer1.append(SpectralNorm(nn.Conv2d(color_dim, conv_dim, 4, 2, 1)))\n",
    "        layer1.append(nn.LeakyReLU(0.1))\n",
    "        layer1.append(nn.Dropout2d(0.25))\n",
    "\n",
    "        curr_dim = conv_dim\n",
    "\n",
    "        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "        layer2.append(nn.LeakyReLU(0.1))\n",
    "        layer2.append(nn.Dropout2d(0.25))\n",
    "        curr_dim = curr_dim * 2\n",
    "\n",
    "        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "        layer3.append(nn.LeakyReLU(0.1))\n",
    "        layer3.append(nn.Dropout2d(0.25))\n",
    "        curr_dim = curr_dim * 2\n",
    "        \n",
    "        layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "        layer4.append(nn.LeakyReLU(0.1))\n",
    "        layer4.append(nn.Dropout2d(0.25))\n",
    "        curr_dim = curr_dim * 2\n",
    "        \n",
    "        layer5.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "        layer5.append(nn.LeakyReLU(0.1))\n",
    "        layer5.append(nn.Dropout2d(0.25))\n",
    "        curr_dim = curr_dim * 2\n",
    "        \n",
    "        layer6.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "        layer6.append(nn.LeakyReLU(0.1))\n",
    "        layer6.append(nn.Dropout2d(0.25))\n",
    "        curr_dim = curr_dim * 2\n",
    "        \n",
    "        layer7.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "        layer7.append(nn.LeakyReLU(0.1))\n",
    "        layer7.append(nn.Dropout2d(0.25))\n",
    "        curr_dim = curr_dim * 2\n",
    "            \n",
    "        self.l1 = nn.Sequential(*layer1)\n",
    "        self.l2 = nn.Sequential(*layer2)\n",
    "        self.l3 = nn.Sequential(*layer3)\n",
    "        self.l4 = nn.Sequential(*layer4)\n",
    "        self.l5 = nn.Sequential(*layer5)\n",
    "        self.l6 = nn.Sequential(*layer6)\n",
    "        self.l7 = nn.Sequential(*layer7)\n",
    "\n",
    "        last.append(nn.Conv2d(curr_dim, 1, 4))\n",
    "        self.last = nn.Sequential(*last)\n",
    "\n",
    "        #self.attn1 = Self_Attn(128, 'relu')\n",
    "        #self.attn2 = Self_Attn(256, 'relu')\n",
    "        self.attn3 = Self_Attn(512, 'relu')\n",
    "        self.attn4 = Self_Attn(1024, 'relu')\n",
    "        self.attn5 = Self_Attn(2048, 'relu')\n",
    "        self.attn6 = Self_Attn(4096, 'relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        #out, p1 = self.attn1(out)\n",
    "        out = self.l3(out)\n",
    "        #out, p2 = self.attn2(out)\n",
    "        out = self.l4(out)\n",
    "        #out, _ = self.attn3(out)\n",
    "        out = self.l5(out)\n",
    "        #out, _ = self.attn4(out)\n",
    "        out = self.l6(out)\n",
    "        #out, _ = self.attn5(out)\n",
    "        out = self.l7(out)\n",
    "        #out, _ = self.attn6(out)\n",
    "        out = self.last(out)\n",
    "\n",
    "        return out.squeeze()\n",
    "    \n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if is_cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    \n",
    "# Initialize weights\n",
    "#generator.apply(weights_init_normal)\n",
    "#discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()), lr=g_learning_rate, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(filter(lambda p: p.requires_grad, discriminator.parameters()), lr=d_learning_rate, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/100] [D loss: 0.002460] [G loss: 0.018491]\n",
      "[Epoch 0/200] [Batch 1/100] [D loss: -0.038955] [G loss: 0.018491]\n",
      "[Epoch 0/200] [Batch 2/100] [D loss: -0.243357] [G loss: 0.018491]\n",
      "[Epoch 0/200] [Batch 3/100] [D loss: -3.100835] [G loss: 0.018491]\n",
      "[Epoch 0/200] [Batch 4/100] [D loss: -8.437162] [G loss: -10.601519]\n",
      "[Epoch 0/200] [Batch 5/100] [D loss: -17.570305] [G loss: -10.601519]\n",
      "[Epoch 0/200] [Batch 6/100] [D loss: -33.810352] [G loss: -10.601519]\n",
      "[Epoch 0/200] [Batch 7/100] [D loss: -46.744064] [G loss: -10.601519]\n",
      "[Epoch 0/200] [Batch 8/100] [D loss: -55.848511] [G loss: -10.601519]\n",
      "[Epoch 0/200] [Batch 9/100] [D loss: -70.385132] [G loss: -26.575256]\n",
      "[Epoch 0/200] [Batch 10/100] [D loss: -83.034592] [G loss: -26.575256]\n",
      "[Epoch 0/200] [Batch 11/100] [D loss: -103.663055] [G loss: -26.575256]\n",
      "[Epoch 0/200] [Batch 12/100] [D loss: -104.241409] [G loss: -26.575256]\n",
      "[Epoch 0/200] [Batch 13/100] [D loss: -102.394325] [G loss: -26.575256]\n",
      "[Epoch 0/200] [Batch 14/100] [D loss: -146.801361] [G loss: -42.021431]\n",
      "[Epoch 0/200] [Batch 15/100] [D loss: -139.392792] [G loss: -42.021431]\n",
      "[Epoch 0/200] [Batch 16/100] [D loss: -159.991608] [G loss: -42.021431]\n",
      "[Epoch 0/200] [Batch 17/100] [D loss: -170.051010] [G loss: -42.021431]\n",
      "[Epoch 0/200] [Batch 18/100] [D loss: -218.515640] [G loss: -42.021431]\n",
      "[Epoch 0/200] [Batch 19/100] [D loss: -231.441681] [G loss: -89.276657]\n",
      "[Epoch 0/200] [Batch 20/100] [D loss: -272.880127] [G loss: -89.276657]\n",
      "[Epoch 0/200] [Batch 21/100] [D loss: -254.908005] [G loss: -89.276657]\n",
      "[Epoch 0/200] [Batch 22/100] [D loss: -262.963593] [G loss: -89.276657]\n",
      "[Epoch 0/200] [Batch 23/100] [D loss: -304.188629] [G loss: -89.276657]\n",
      "[Epoch 0/200] [Batch 24/100] [D loss: -264.345154] [G loss: -93.586029]\n",
      "[Epoch 0/200] [Batch 25/100] [D loss: -357.288025] [G loss: -93.586029]\n",
      "[Epoch 0/200] [Batch 26/100] [D loss: -301.751129] [G loss: -93.586029]\n",
      "[Epoch 0/200] [Batch 27/100] [D loss: -313.722717] [G loss: -93.586029]\n",
      "[Epoch 0/200] [Batch 28/100] [D loss: -311.526550] [G loss: -93.586029]\n",
      "[Epoch 0/200] [Batch 29/100] [D loss: -346.465942] [G loss: -193.461060]\n",
      "[Epoch 0/200] [Batch 30/100] [D loss: -238.889038] [G loss: -193.461060]\n",
      "[Epoch 0/200] [Batch 31/100] [D loss: -298.805115] [G loss: -193.461060]\n",
      "[Epoch 0/200] [Batch 32/100] [D loss: -379.290375] [G loss: -193.461060]\n",
      "[Epoch 0/200] [Batch 33/100] [D loss: -361.248535] [G loss: -193.461060]\n",
      "[Epoch 0/200] [Batch 34/100] [D loss: -362.508911] [G loss: -147.372925]\n",
      "[Epoch 0/200] [Batch 35/100] [D loss: -433.103851] [G loss: -147.372925]\n",
      "[Epoch 0/200] [Batch 36/100] [D loss: -374.879913] [G loss: -147.372925]\n",
      "[Epoch 0/200] [Batch 37/100] [D loss: -373.291138] [G loss: -147.372925]\n",
      "[Epoch 0/200] [Batch 38/100] [D loss: -422.484253] [G loss: -147.372925]\n",
      "[Epoch 0/200] [Batch 39/100] [D loss: -414.521484] [G loss: -260.615967]\n",
      "[Epoch 0/200] [Batch 40/100] [D loss: -399.210571] [G loss: -260.615967]\n",
      "[Epoch 0/200] [Batch 41/100] [D loss: -432.601807] [G loss: -260.615967]\n",
      "[Epoch 0/200] [Batch 42/100] [D loss: -460.176208] [G loss: -260.615967]\n",
      "[Epoch 0/200] [Batch 43/100] [D loss: -387.503632] [G loss: -260.615967]\n",
      "[Epoch 0/200] [Batch 44/100] [D loss: -446.008301] [G loss: -149.299652]\n",
      "[Epoch 0/200] [Batch 45/100] [D loss: -442.238037] [G loss: -149.299652]\n",
      "[Epoch 0/200] [Batch 46/100] [D loss: -417.341034] [G loss: -149.299652]\n",
      "[Epoch 0/200] [Batch 47/100] [D loss: -478.060669] [G loss: -149.299652]\n",
      "[Epoch 0/200] [Batch 48/100] [D loss: -427.696777] [G loss: -149.299652]\n",
      "[Epoch 0/200] [Batch 49/100] [D loss: -436.355927] [G loss: -312.626129]\n",
      "[Epoch 0/200] [Batch 50/100] [D loss: -401.502289] [G loss: -312.626129]\n",
      "[Epoch 0/200] [Batch 51/100] [D loss: -486.940002] [G loss: -312.626129]\n",
      "[Epoch 0/200] [Batch 52/100] [D loss: -537.159302] [G loss: -312.626129]\n",
      "[Epoch 0/200] [Batch 53/100] [D loss: -444.459106] [G loss: -312.626129]\n",
      "[Epoch 0/200] [Batch 54/100] [D loss: -453.600708] [G loss: -294.361755]\n",
      "[Epoch 0/200] [Batch 55/100] [D loss: -486.687439] [G loss: -294.361755]\n",
      "[Epoch 0/200] [Batch 56/100] [D loss: -496.366821] [G loss: -294.361755]\n",
      "[Epoch 0/200] [Batch 57/100] [D loss: -372.469299] [G loss: -294.361755]\n",
      "[Epoch 0/200] [Batch 58/100] [D loss: -527.033691] [G loss: -294.361755]\n",
      "[Epoch 0/200] [Batch 59/100] [D loss: -514.347534] [G loss: -307.182373]\n",
      "[Epoch 0/200] [Batch 60/100] [D loss: -485.574646] [G loss: -307.182373]\n",
      "[Epoch 0/200] [Batch 61/100] [D loss: -426.459412] [G loss: -307.182373]\n",
      "[Epoch 0/200] [Batch 62/100] [D loss: -476.448608] [G loss: -307.182373]\n",
      "[Epoch 0/200] [Batch 63/100] [D loss: -456.277039] [G loss: -307.182373]\n",
      "[Epoch 0/200] [Batch 64/100] [D loss: -394.973816] [G loss: -293.550201]\n",
      "[Epoch 0/200] [Batch 65/100] [D loss: -474.139587] [G loss: -293.550201]\n",
      "[Epoch 0/200] [Batch 66/100] [D loss: -527.271057] [G loss: -293.550201]\n",
      "[Epoch 0/200] [Batch 67/100] [D loss: -453.839600] [G loss: -293.550201]\n",
      "[Epoch 0/200] [Batch 68/100] [D loss: -487.281555] [G loss: -293.550201]\n",
      "[Epoch 0/200] [Batch 69/100] [D loss: -553.448730] [G loss: -321.383301]\n",
      "[Epoch 0/200] [Batch 70/100] [D loss: -559.727539] [G loss: -321.383301]\n",
      "[Epoch 0/200] [Batch 71/100] [D loss: -456.318695] [G loss: -321.383301]\n",
      "[Epoch 0/200] [Batch 72/100] [D loss: -489.779541] [G loss: -321.383301]\n",
      "[Epoch 0/200] [Batch 73/100] [D loss: -565.091431] [G loss: -321.383301]\n",
      "[Epoch 0/200] [Batch 74/100] [D loss: -546.438049] [G loss: -396.182770]\n",
      "[Epoch 0/200] [Batch 75/100] [D loss: -497.918152] [G loss: -396.182770]\n",
      "[Epoch 0/200] [Batch 76/100] [D loss: -487.748657] [G loss: -396.182770]\n",
      "[Epoch 0/200] [Batch 77/100] [D loss: -469.961853] [G loss: -396.182770]\n",
      "[Epoch 0/200] [Batch 78/100] [D loss: -510.138275] [G loss: -396.182770]\n",
      "[Epoch 0/200] [Batch 79/100] [D loss: -541.647949] [G loss: -347.000763]\n",
      "[Epoch 0/200] [Batch 80/100] [D loss: -486.243286] [G loss: -347.000763]\n",
      "[Epoch 0/200] [Batch 81/100] [D loss: -564.463013] [G loss: -347.000763]\n",
      "[Epoch 0/200] [Batch 82/100] [D loss: -577.300049] [G loss: -347.000763]\n",
      "[Epoch 0/200] [Batch 83/100] [D loss: -401.137756] [G loss: -347.000763]\n",
      "[Epoch 0/200] [Batch 84/100] [D loss: -568.271729] [G loss: -365.794006]\n",
      "[Epoch 0/200] [Batch 85/100] [D loss: -516.792236] [G loss: -365.794006]\n",
      "[Epoch 0/200] [Batch 86/100] [D loss: -517.458130] [G loss: -365.794006]\n",
      "[Epoch 0/200] [Batch 87/100] [D loss: -528.343872] [G loss: -365.794006]\n",
      "[Epoch 0/200] [Batch 88/100] [D loss: -570.662354] [G loss: -365.794006]\n",
      "[Epoch 0/200] [Batch 89/100] [D loss: -497.117188] [G loss: -373.155334]\n",
      "[Epoch 0/200] [Batch 90/100] [D loss: -566.184814] [G loss: -373.155334]\n",
      "[Epoch 0/200] [Batch 91/100] [D loss: -544.373535] [G loss: -373.155334]\n",
      "[Epoch 0/200] [Batch 92/100] [D loss: -526.694641] [G loss: -373.155334]\n",
      "[Epoch 0/200] [Batch 93/100] [D loss: -515.172607] [G loss: -373.155334]\n",
      "[Epoch 0/200] [Batch 94/100] [D loss: -556.168396] [G loss: -375.975555]\n",
      "[Epoch 0/200] [Batch 95/100] [D loss: -607.048096] [G loss: -375.975555]\n",
      "[Epoch 0/200] [Batch 96/100] [D loss: -575.820374] [G loss: -375.975555]\n",
      "[Epoch 0/200] [Batch 97/100] [D loss: -588.915283] [G loss: -375.975555]\n",
      "[Epoch 0/200] [Batch 98/100] [D loss: -548.348633] [G loss: -375.975555]\n",
      "[Epoch 0/200] [Batch 99/100] [D loss: -563.244202] [G loss: -397.163086]\n",
      "[Epoch 1/200] [Batch 0/100] [D loss: -531.255127] [G loss: -470.686951]\n",
      "[Epoch 1/200] [Batch 1/100] [D loss: -604.539551] [G loss: -470.686951]\n",
      "[Epoch 1/200] [Batch 2/100] [D loss: -554.218079] [G loss: -470.686951]\n",
      "[Epoch 1/200] [Batch 3/100] [D loss: -471.626404] [G loss: -470.686951]\n",
      "[Epoch 1/200] [Batch 4/100] [D loss: -437.433289] [G loss: -373.384399]\n",
      "[Epoch 1/200] [Batch 5/100] [D loss: -522.628784] [G loss: -373.384399]\n",
      "[Epoch 1/200] [Batch 6/100] [D loss: -557.960205] [G loss: -373.384399]\n",
      "[Epoch 1/200] [Batch 7/100] [D loss: -548.335327] [G loss: -373.384399]\n",
      "[Epoch 1/200] [Batch 8/100] [D loss: -591.267273] [G loss: -373.384399]\n",
      "[Epoch 1/200] [Batch 9/100] [D loss: -573.040894] [G loss: -400.135437]\n",
      "[Epoch 1/200] [Batch 10/100] [D loss: -628.028259] [G loss: -400.135437]\n",
      "[Epoch 1/200] [Batch 11/100] [D loss: -555.860352] [G loss: -400.135437]\n",
      "[Epoch 1/200] [Batch 12/100] [D loss: -617.452759] [G loss: -400.135437]\n",
      "[Epoch 1/200] [Batch 13/100] [D loss: -602.108521] [G loss: -400.135437]\n",
      "[Epoch 1/200] [Batch 14/100] [D loss: -557.076172] [G loss: -433.562134]\n",
      "[Epoch 1/200] [Batch 15/100] [D loss: -595.626892] [G loss: -433.562134]\n",
      "[Epoch 1/200] [Batch 16/100] [D loss: -604.839111] [G loss: -433.562134]\n",
      "[Epoch 1/200] [Batch 17/100] [D loss: -609.776733] [G loss: -433.562134]\n",
      "[Epoch 1/200] [Batch 18/100] [D loss: -543.438660] [G loss: -433.562134]\n",
      "[Epoch 1/200] [Batch 19/100] [D loss: -601.635315] [G loss: -499.680847]\n",
      "[Epoch 1/200] [Batch 20/100] [D loss: -509.496033] [G loss: -499.680847]\n",
      "[Epoch 1/200] [Batch 21/100] [D loss: -591.598999] [G loss: -499.680847]\n",
      "[Epoch 1/200] [Batch 22/100] [D loss: -663.674316] [G loss: -499.680847]\n",
      "[Epoch 1/200] [Batch 23/100] [D loss: -481.986389] [G loss: -499.680847]\n",
      "[Epoch 1/200] [Batch 24/100] [D loss: -592.174927] [G loss: -448.843781]\n",
      "[Epoch 1/200] [Batch 25/100] [D loss: -699.986511] [G loss: -448.843781]\n",
      "[Epoch 1/200] [Batch 26/100] [D loss: -510.131165] [G loss: -448.843781]\n",
      "[Epoch 1/200] [Batch 27/100] [D loss: -595.585205] [G loss: -448.843781]\n",
      "[Epoch 1/200] [Batch 28/100] [D loss: -555.528320] [G loss: -448.843781]\n",
      "[Epoch 1/200] [Batch 29/100] [D loss: -721.863037] [G loss: -378.268738]\n",
      "[Epoch 1/200] [Batch 30/100] [D loss: -576.252808] [G loss: -378.268738]\n",
      "[Epoch 1/200] [Batch 31/100] [D loss: -643.447266] [G loss: -378.268738]\n",
      "[Epoch 1/200] [Batch 32/100] [D loss: -660.809082] [G loss: -378.268738]\n",
      "[Epoch 1/200] [Batch 33/100] [D loss: -633.483643] [G loss: -378.268738]\n",
      "[Epoch 1/200] [Batch 34/100] [D loss: -612.914429] [G loss: -461.163086]\n",
      "[Epoch 1/200] [Batch 35/100] [D loss: -707.036621] [G loss: -461.163086]\n",
      "[Epoch 1/200] [Batch 36/100] [D loss: -539.898560] [G loss: -461.163086]\n",
      "[Epoch 1/200] [Batch 37/100] [D loss: -647.049316] [G loss: -461.163086]\n",
      "[Epoch 1/200] [Batch 38/100] [D loss: -597.189270] [G loss: -461.163086]\n",
      "[Epoch 1/200] [Batch 39/100] [D loss: -581.234375] [G loss: -496.532440]\n",
      "[Epoch 1/200] [Batch 40/100] [D loss: -594.808105] [G loss: -496.532440]\n",
      "[Epoch 1/200] [Batch 41/100] [D loss: -566.928101] [G loss: -496.532440]\n",
      "[Epoch 1/200] [Batch 42/100] [D loss: -610.018982] [G loss: -496.532440]\n",
      "[Epoch 1/200] [Batch 43/100] [D loss: -657.089233] [G loss: -496.532440]\n",
      "[Epoch 1/200] [Batch 44/100] [D loss: -692.849487] [G loss: -426.096832]\n",
      "[Epoch 1/200] [Batch 45/100] [D loss: -571.534241] [G loss: -426.096832]\n",
      "[Epoch 1/200] [Batch 46/100] [D loss: -587.949036] [G loss: -426.096832]\n",
      "[Epoch 1/200] [Batch 47/100] [D loss: -594.921570] [G loss: -426.096832]\n",
      "[Epoch 1/200] [Batch 48/100] [D loss: -619.310059] [G loss: -426.096832]\n",
      "[Epoch 1/200] [Batch 49/100] [D loss: -717.261597] [G loss: -433.731262]\n",
      "[Epoch 1/200] [Batch 50/100] [D loss: -538.974121] [G loss: -433.731262]\n",
      "[Epoch 1/200] [Batch 51/100] [D loss: -569.820190] [G loss: -433.731262]\n",
      "[Epoch 1/200] [Batch 52/100] [D loss: -645.395264] [G loss: -433.731262]\n",
      "[Epoch 1/200] [Batch 53/100] [D loss: -756.025574] [G loss: -433.731262]\n",
      "[Epoch 1/200] [Batch 54/100] [D loss: -669.042725] [G loss: -507.126038]\n",
      "[Epoch 1/200] [Batch 55/100] [D loss: -594.688293] [G loss: -507.126038]\n",
      "[Epoch 1/200] [Batch 56/100] [D loss: -679.340210] [G loss: -507.126038]\n",
      "[Epoch 1/200] [Batch 57/100] [D loss: -647.866272] [G loss: -507.126038]\n",
      "[Epoch 1/200] [Batch 58/100] [D loss: -484.795776] [G loss: -507.126038]\n",
      "[Epoch 1/200] [Batch 59/100] [D loss: -583.030151] [G loss: -531.214905]\n",
      "[Epoch 1/200] [Batch 60/100] [D loss: -690.588013] [G loss: -531.214905]\n",
      "[Epoch 1/200] [Batch 61/100] [D loss: -624.234863] [G loss: -531.214905]\n",
      "[Epoch 1/200] [Batch 62/100] [D loss: -603.643555] [G loss: -531.214905]\n",
      "[Epoch 1/200] [Batch 63/100] [D loss: -580.844360] [G loss: -531.214905]\n",
      "[Epoch 1/200] [Batch 64/100] [D loss: -718.648193] [G loss: -495.805511]\n",
      "[Epoch 1/200] [Batch 65/100] [D loss: -595.043213] [G loss: -495.805511]\n",
      "[Epoch 1/200] [Batch 66/100] [D loss: -634.515198] [G loss: -495.805511]\n",
      "[Epoch 1/200] [Batch 67/100] [D loss: -625.659546] [G loss: -495.805511]\n",
      "[Epoch 1/200] [Batch 68/100] [D loss: -674.459595] [G loss: -495.805511]\n",
      "[Epoch 1/200] [Batch 69/100] [D loss: -566.840759] [G loss: -416.674469]\n",
      "[Epoch 1/200] [Batch 70/100] [D loss: -659.177368] [G loss: -416.674469]\n",
      "[Epoch 1/200] [Batch 71/100] [D loss: -717.737671] [G loss: -416.674469]\n",
      "[Epoch 1/200] [Batch 72/100] [D loss: -552.282593] [G loss: -416.674469]\n",
      "[Epoch 1/200] [Batch 73/100] [D loss: -627.941040] [G loss: -416.674469]\n",
      "[Epoch 1/200] [Batch 74/100] [D loss: -615.342285] [G loss: -420.930176]\n",
      "[Epoch 1/200] [Batch 75/100] [D loss: -599.078796] [G loss: -420.930176]\n",
      "[Epoch 1/200] [Batch 76/100] [D loss: -718.313782] [G loss: -420.930176]\n",
      "[Epoch 1/200] [Batch 77/100] [D loss: -582.919067] [G loss: -420.930176]\n",
      "[Epoch 1/200] [Batch 78/100] [D loss: -589.245300] [G loss: -420.930176]\n",
      "[Epoch 1/200] [Batch 79/100] [D loss: -661.971802] [G loss: -407.966492]\n",
      "[Epoch 1/200] [Batch 80/100] [D loss: -689.219116] [G loss: -407.966492]\n",
      "[Epoch 1/200] [Batch 81/100] [D loss: -672.104797] [G loss: -407.966492]\n",
      "[Epoch 1/200] [Batch 82/100] [D loss: -676.517761] [G loss: -407.966492]\n",
      "[Epoch 1/200] [Batch 83/100] [D loss: -619.433289] [G loss: -407.966492]\n",
      "[Epoch 1/200] [Batch 84/100] [D loss: -629.774170] [G loss: -545.657715]\n",
      "[Epoch 1/200] [Batch 85/100] [D loss: -669.225525] [G loss: -545.657715]\n",
      "[Epoch 1/200] [Batch 86/100] [D loss: -571.490906] [G loss: -545.657715]\n",
      "[Epoch 1/200] [Batch 87/100] [D loss: -536.233459] [G loss: -545.657715]\n",
      "[Epoch 1/200] [Batch 88/100] [D loss: -704.786133] [G loss: -545.657715]\n",
      "[Epoch 1/200] [Batch 89/100] [D loss: -664.527222] [G loss: -412.688354]\n",
      "[Epoch 1/200] [Batch 90/100] [D loss: -644.959290] [G loss: -412.688354]\n",
      "[Epoch 1/200] [Batch 91/100] [D loss: -647.961487] [G loss: -412.688354]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 82426, 82427, 82432, 82437) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b3ec2ec63023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msynthethic_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_synthetic_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mholo_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholo_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 82426, 82427, 82432, 82437) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batches_per_epoch = 100\n",
    "\n",
    "sample_interval = 10\n",
    "Tensor = torch.cuda.FloatTensor if is_cuda else torch.FloatTensor\n",
    "adv_loss = 'wgan-gp'\n",
    "lambda_gp = 10\n",
    "train_gen_every = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    real_images = enumerate(train_holodec_loader)\n",
    "    synthethic_images = iter(train_synthetic_loader)\n",
    "    \n",
    "    for i, (holo_img, holo_label) in real_images:\n",
    "        \n",
    "        try:\n",
    "            synth_img, synth_label = next(synthethic_images)\n",
    "        except:\n",
    "            synthethic_images = iter(train_synthetic_loader)\n",
    "            synth_img, synth_label = next(synthethic_images)\n",
    "            \n",
    "        if holo_img.shape[0] != synth_img.shape[0]:\n",
    "            continue\n",
    "                \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(holo_img.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(holo_img.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(holo_img.type(Tensor))\n",
    "        synthethic_imgs = Variable(synth_img.type(Tensor))\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (holo_img.shape[0], latent_dim))))\n",
    "        # Generate a batch of images\n",
    "        gen_noise = generator(z)\n",
    "        # Add to the synthetic images\n",
    "        gen_imgs = 0.5 * (synthethic_imgs + gen_noise)\n",
    "        # Discriminate the fake images\n",
    "        verdict = discriminator(gen_imgs)\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        requires_grad(generator, True)\n",
    "        requires_grad(discriminator, False)\n",
    "        \n",
    "        if (i + 1) % train_gen_every == 0 or i == 0:\n",
    "            \n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            if adv_loss == 'wgan-gp':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'hinge':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'bce':\n",
    "                g_loss = adversarial_loss(verdict, valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        requires_grad(generator, False)\n",
    "        requires_grad(discriminator, True)\n",
    "        \n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        if adv_loss == 'wgan-gp':\n",
    "            real_loss = -torch.mean(discriminator(real_imgs))\n",
    "            fake_loss = discriminator(gen_imgs.detach()).mean()\n",
    "        elif adv_loss == 'hinge':\n",
    "            real_loss = torch.nn.ReLU()(1.0 - discriminator(real_imgs)).mean()\n",
    "            fake_loss = torch.nn.ReLU()(1.0 + discriminator(gen_imgs.detach())).mean()\n",
    "        elif adv_loss == 'bce':\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = real_loss + fake_loss\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        if adv_loss == 'wgan-gp':\n",
    "            # Compute gradient penalty\n",
    "            alpha = torch.rand(real_imgs.size(0), 1, 1, 1).cuda().expand_as(real_imgs)\n",
    "            interpolated = Variable(alpha * real_imgs.data + (1 - alpha) * gen_imgs.data, requires_grad=True)\n",
    "            out = discriminator(interpolated)\n",
    "\n",
    "            grad = torch.autograd.grad(outputs=out,\n",
    "                                       inputs=interpolated,\n",
    "                                       grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                       retain_graph=True,\n",
    "                                       create_graph=True,\n",
    "                                       only_inputs=True)[0]\n",
    "\n",
    "            grad = grad.view(grad.size(0), -1)\n",
    "            grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "            d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "\n",
    "            # Backward + Optimize\n",
    "            d_loss_reg = lambda_gp * d_loss_gp\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss_reg.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, n_epochs, i, batches_per_epoch, d_loss.item(), g_loss.item())\n",
    "        )\n",
    "        \n",
    "        if (i + 1) % sample_interval == 0:\n",
    "            save_image(synthethic_imgs.data[:25], f\"../results/gan/images/synth_{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "            save_image(real_imgs.data[:25], f\"../results/gan/images/real_{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "            save_image(gen_imgs.data[:25], f\"../results/gan/images/pred_{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "            \n",
    "        with open(\"../results/gan/training_log.csv\", \"a+\") as fid:\n",
    "            fid.write(f\"{epoch},{i},{d_loss.item()},{d_loss_reg.item()},{g_loss.item()}\\n\")\n",
    "            \n",
    "        if (i + 1) == batches_per_epoch:\n",
    "            torch.save(generator.state_dict(), f'../results/gan/images/generator_{epoch}.pt')\n",
    "            torch.save(discriminator.state_dict(), f'../results/gan/images/discriminator_{epoch}.pt')\n",
    "            torch.save(optimizer_G.state_dict(), f'../results/gan/images/gen_optimizer_{epoch}.pt')\n",
    "            torch.save(optimizer_D.state_dict(), f'../results/gan/images/dis_optimizer_{epoch}.pt')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/gan/training_log.csv\", \"r\") as fid:\n",
    "    lines = np.array([[float(g) for g in f.strip(\"\\n\").split(\",\")] for f in fid.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b3f9290af90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_updates = range(len(lines[:, 0]))\n",
    "\n",
    "plt.plot(batch_updates, lines[:, 2])\n",
    "plt.plot(batch_updates, lines[:, 3])\n",
    "plt.plot(batch_updates, lines[:, 4])\n",
    "plt.legend([\"d-loss\", \"d-reg-loss\", \"g-loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator(z_dim = latent_dim)\n",
    "discriminator = Discriminator(color_dim = 1)\n",
    "\n",
    "if is_cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()), lr=g_learning_rate, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(filter(lambda p: p.requires_grad, discriminator.parameters()), lr=d_learning_rate, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1000] [Batch 0/100] [D loss: -0.000258] [D reg: 8.509409] [G loss: -0.004816]\n",
      "[Epoch 0/1000] [Batch 1/100] [D loss: -202.073975] [D reg: 3051.933838] [G loss: -0.004816]\n",
      "[Epoch 0/1000] [Batch 2/100] [D loss: -101.769745] [D reg: 1.639710] [G loss: -0.004816]\n",
      "[Epoch 0/1000] [Batch 3/100] [D loss: -154.818710] [D reg: 1.505449] [G loss: -0.004816]\n",
      "[Epoch 0/1000] [Batch 4/100] [D loss: -253.639297] [D reg: 13.927910] [G loss: -266.518250]\n",
      "[Epoch 0/1000] [Batch 5/100] [D loss: -370.689789] [D reg: 43.939140] [G loss: -266.518250]\n",
      "[Epoch 0/1000] [Batch 6/100] [D loss: -501.073761] [D reg: 134.474579] [G loss: -266.518250]\n",
      "[Epoch 0/1000] [Batch 7/100] [D loss: -627.082520] [D reg: 170.801880] [G loss: -266.518250]\n",
      "[Epoch 0/1000] [Batch 8/100] [D loss: -832.774963] [D reg: 263.925476] [G loss: -266.518250]\n",
      "[Epoch 0/1000] [Batch 9/100] [D loss: -1016.583374] [D reg: 357.937073] [G loss: -640.814575]\n",
      "[Epoch 0/1000] [Batch 10/100] [D loss: -1047.090088] [D reg: 418.811279] [G loss: -640.814575]\n",
      "[Epoch 0/1000] [Batch 11/100] [D loss: -1269.361694] [D reg: 397.732422] [G loss: -640.814575]\n",
      "[Epoch 0/1000] [Batch 12/100] [D loss: -1492.163086] [D reg: 606.167053] [G loss: -640.814575]\n",
      "[Epoch 0/1000] [Batch 13/100] [D loss: -1592.366089] [D reg: 649.232483] [G loss: -640.814575]\n",
      "[Epoch 0/1000] [Batch 14/100] [D loss: -1509.324341] [D reg: 548.135986] [G loss: -1040.305786]\n",
      "[Epoch 0/1000] [Batch 15/100] [D loss: -1972.473633] [D reg: 684.083130] [G loss: -1040.305786]\n",
      "[Epoch 0/1000] [Batch 16/100] [D loss: -2202.075195] [D reg: 841.630981] [G loss: -1040.305786]\n",
      "[Epoch 0/1000] [Batch 17/100] [D loss: -2113.981445] [D reg: 710.598267] [G loss: -1040.305786]\n",
      "[Epoch 0/1000] [Batch 18/100] [D loss: -2401.209229] [D reg: 827.271484] [G loss: -1040.305786]\n",
      "[Epoch 0/1000] [Batch 19/100] [D loss: -2862.220947] [D reg: 1065.858276] [G loss: -2397.260254]\n",
      "[Epoch 0/1000] [Batch 20/100] [D loss: -3032.916504] [D reg: 889.577576] [G loss: -2397.260254]\n",
      "[Epoch 0/1000] [Batch 21/100] [D loss: -3135.208496] [D reg: 1234.956055] [G loss: -2397.260254]\n",
      "[Epoch 0/1000] [Batch 22/100] [D loss: -3516.688965] [D reg: 938.010376] [G loss: -2397.260254]\n",
      "[Epoch 0/1000] [Batch 23/100] [D loss: -3896.896484] [D reg: 1406.240479] [G loss: -2397.260254]\n",
      "[Epoch 0/1000] [Batch 24/100] [D loss: -3957.189941] [D reg: 1211.509766] [G loss: -3902.034180]\n",
      "[Epoch 0/1000] [Batch 25/100] [D loss: -4094.310303] [D reg: 1652.410767] [G loss: -3902.034180]\n",
      "[Epoch 0/1000] [Batch 26/100] [D loss: -4282.891602] [D reg: 1219.131958] [G loss: -3902.034180]\n",
      "[Epoch 0/1000] [Batch 27/100] [D loss: -4525.403809] [D reg: 1577.019287] [G loss: -3902.034180]\n",
      "[Epoch 0/1000] [Batch 28/100] [D loss: -4882.269531] [D reg: 1481.651611] [G loss: -3902.034180]\n",
      "[Epoch 0/1000] [Batch 29/100] [D loss: -4572.180664] [D reg: 1693.519287] [G loss: -5322.063477]\n",
      "[Epoch 0/1000] [Batch 30/100] [D loss: -4821.575195] [D reg: 1374.603149] [G loss: -5322.063477]\n",
      "[Epoch 0/1000] [Batch 31/100] [D loss: -5247.806152] [D reg: 1604.952393] [G loss: -5322.063477]\n",
      "[Epoch 0/1000] [Batch 32/100] [D loss: -5709.807617] [D reg: 1968.931885] [G loss: -5322.063477]\n",
      "[Epoch 0/1000] [Batch 33/100] [D loss: -5224.863281] [D reg: 1965.791992] [G loss: -5322.063477]\n",
      "[Epoch 0/1000] [Batch 34/100] [D loss: -5739.862793] [D reg: 1639.569824] [G loss: -6777.952148]\n",
      "[Epoch 0/1000] [Batch 35/100] [D loss: -6027.144043] [D reg: 1913.087769] [G loss: -6777.952148]\n",
      "[Epoch 0/1000] [Batch 36/100] [D loss: -5573.745605] [D reg: 2175.436523] [G loss: -6777.952148]\n",
      "[Epoch 0/1000] [Batch 37/100] [D loss: -6079.376953] [D reg: 2178.687988] [G loss: -6777.952148]\n",
      "[Epoch 0/1000] [Batch 38/100] [D loss: -6501.822266] [D reg: 1598.610596] [G loss: -6777.952148]\n",
      "[Epoch 0/1000] [Batch 39/100] [D loss: -7581.802734] [D reg: 2446.901123] [G loss: -8676.342773]\n",
      "[Epoch 0/1000] [Batch 40/100] [D loss: -6528.318359] [D reg: 1957.072876] [G loss: -8676.342773]\n",
      "[Epoch 0/1000] [Batch 41/100] [D loss: -7138.708496] [D reg: 2704.058105] [G loss: -8676.342773]\n",
      "[Epoch 0/1000] [Batch 42/100] [D loss: -7114.449219] [D reg: 2506.075195] [G loss: -8676.342773]\n",
      "[Epoch 0/1000] [Batch 43/100] [D loss: -6508.668945] [D reg: 3081.676758] [G loss: -8676.342773]\n",
      "[Epoch 0/1000] [Batch 44/100] [D loss: -6572.013672] [D reg: 2016.190796] [G loss: -9744.096680]\n",
      "[Epoch 0/1000] [Batch 45/100] [D loss: -7473.683594] [D reg: 2129.008789] [G loss: -9744.096680]\n",
      "[Epoch 0/1000] [Batch 46/100] [D loss: -8790.709961] [D reg: 2327.427979] [G loss: -9744.096680]\n",
      "[Epoch 0/1000] [Batch 47/100] [D loss: -8001.575195] [D reg: 2958.518066] [G loss: -9744.096680]\n",
      "[Epoch 0/1000] [Batch 48/100] [D loss: -7757.490234] [D reg: 2427.390869] [G loss: -9744.096680]\n",
      "[Epoch 0/1000] [Batch 49/100] [D loss: -8012.212891] [D reg: 2607.809570] [G loss: -11780.468750]\n",
      "[Epoch 0/1000] [Batch 50/100] [D loss: -9309.695312] [D reg: 2711.378906] [G loss: -11780.468750]\n",
      "[Epoch 0/1000] [Batch 51/100] [D loss: -8673.901367] [D reg: 2971.726562] [G loss: -11780.468750]\n",
      "[Epoch 0/1000] [Batch 52/100] [D loss: -9288.375977] [D reg: 3052.705078] [G loss: -11780.468750]\n",
      "[Epoch 0/1000] [Batch 53/100] [D loss: -8288.346680] [D reg: 2893.924316] [G loss: -11780.468750]\n",
      "[Epoch 0/1000] [Batch 54/100] [D loss: -9231.266602] [D reg: 3436.704102] [G loss: -14105.868164]\n",
      "[Epoch 0/1000] [Batch 55/100] [D loss: -9185.287109] [D reg: 2601.441406] [G loss: -14105.868164]\n",
      "[Epoch 0/1000] [Batch 56/100] [D loss: -10413.068359] [D reg: 3215.098877] [G loss: -14105.868164]\n",
      "[Epoch 0/1000] [Batch 57/100] [D loss: -9325.846680] [D reg: 3286.755615] [G loss: -14105.868164]\n",
      "[Epoch 0/1000] [Batch 58/100] [D loss: -10110.957031] [D reg: 3497.429688] [G loss: -14105.868164]\n",
      "[Epoch 0/1000] [Batch 59/100] [D loss: -10773.428711] [D reg: 3036.828369] [G loss: -15295.995117]\n",
      "[Epoch 0/1000] [Batch 60/100] [D loss: -10983.776367] [D reg: 3431.590332] [G loss: -15295.995117]\n",
      "[Epoch 0/1000] [Batch 61/100] [D loss: -10280.093750] [D reg: 3758.352539] [G loss: -15295.995117]\n",
      "[Epoch 0/1000] [Batch 62/100] [D loss: -10825.514648] [D reg: 3476.311035] [G loss: -15295.995117]\n",
      "[Epoch 0/1000] [Batch 63/100] [D loss: -12308.983398] [D reg: 3738.935059] [G loss: -15295.995117]\n",
      "[Epoch 0/1000] [Batch 64/100] [D loss: -12337.021484] [D reg: 4087.121582] [G loss: -18133.132812]\n",
      "[Epoch 0/1000] [Batch 65/100] [D loss: -10996.931641] [D reg: 3530.011963] [G loss: -18133.132812]\n",
      "[Epoch 0/1000] [Batch 66/100] [D loss: -11707.597656] [D reg: 3679.367188] [G loss: -18133.132812]\n",
      "[Epoch 0/1000] [Batch 67/100] [D loss: -14503.752930] [D reg: 4191.470215] [G loss: -18133.132812]\n",
      "[Epoch 0/1000] [Batch 68/100] [D loss: -11563.337891] [D reg: 3464.447021] [G loss: -18133.132812]\n",
      "[Epoch 0/1000] [Batch 69/100] [D loss: -11744.557617] [D reg: 3592.434570] [G loss: -19944.839844]\n",
      "[Epoch 0/1000] [Batch 70/100] [D loss: -13235.359375] [D reg: 3907.464600] [G loss: -19944.839844]\n",
      "[Epoch 0/1000] [Batch 71/100] [D loss: -11834.714844] [D reg: 4057.388916] [G loss: -19944.839844]\n",
      "[Epoch 0/1000] [Batch 72/100] [D loss: -12902.937500] [D reg: 3435.693848] [G loss: -19944.839844]\n",
      "[Epoch 0/1000] [Batch 73/100] [D loss: -12875.527344] [D reg: 4826.315430] [G loss: -19944.839844]\n",
      "[Epoch 0/1000] [Batch 74/100] [D loss: -13461.219727] [D reg: 3860.216309] [G loss: -21683.257812]\n",
      "[Epoch 0/1000] [Batch 75/100] [D loss: -14794.384766] [D reg: 5081.981934] [G loss: -21683.257812]\n",
      "[Epoch 0/1000] [Batch 76/100] [D loss: -14500.409180] [D reg: 3966.073730] [G loss: -21683.257812]\n",
      "[Epoch 0/1000] [Batch 77/100] [D loss: -13145.707031] [D reg: 4035.551514] [G loss: -21683.257812]\n",
      "[Epoch 0/1000] [Batch 78/100] [D loss: -14302.656250] [D reg: 4277.980469] [G loss: -21683.257812]\n",
      "[Epoch 0/1000] [Batch 79/100] [D loss: -13788.131836] [D reg: 4700.411133] [G loss: -24491.193359]\n",
      "[Epoch 0/1000] [Batch 80/100] [D loss: -14566.135742] [D reg: 5724.687012] [G loss: -24491.193359]\n",
      "[Epoch 0/1000] [Batch 81/100] [D loss: -17530.941406] [D reg: 4912.929199] [G loss: -24491.193359]\n",
      "[Epoch 0/1000] [Batch 82/100] [D loss: -15781.340820] [D reg: 5776.205078] [G loss: -24491.193359]\n",
      "[Epoch 0/1000] [Batch 83/100] [D loss: -13782.250000] [D reg: 5490.027344] [G loss: -24491.193359]\n",
      "[Epoch 0/1000] [Batch 84/100] [D loss: -15329.738281] [D reg: 4531.463867] [G loss: -24654.554688]\n",
      "[Epoch 0/1000] [Batch 85/100] [D loss: -16687.093750] [D reg: 4433.895996] [G loss: -24654.554688]\n",
      "[Epoch 0/1000] [Batch 86/100] [D loss: -18603.656250] [D reg: 5756.816895] [G loss: -24654.554688]\n",
      "[Epoch 0/1000] [Batch 87/100] [D loss: -17005.474609] [D reg: 5390.657227] [G loss: -24654.554688]\n",
      "[Epoch 0/1000] [Batch 88/100] [D loss: -17100.513672] [D reg: 5911.252441] [G loss: -24654.554688]\n",
      "[Epoch 0/1000] [Batch 89/100] [D loss: -19155.505859] [D reg: 5727.405273] [G loss: -27647.744141]\n",
      "[Epoch 0/1000] [Batch 90/100] [D loss: -17118.785156] [D reg: 5404.466797] [G loss: -27647.744141]\n",
      "[Epoch 0/1000] [Batch 91/100] [D loss: -17983.314453] [D reg: 6233.788086] [G loss: -27647.744141]\n",
      "[Epoch 0/1000] [Batch 92/100] [D loss: -18599.052734] [D reg: 6120.144531] [G loss: -27647.744141]\n",
      "[Epoch 0/1000] [Batch 93/100] [D loss: -16916.392578] [D reg: 5736.201172] [G loss: -27647.744141]\n",
      "[Epoch 0/1000] [Batch 94/100] [D loss: -19463.527344] [D reg: 6115.813477] [G loss: -32214.148438]\n",
      "[Epoch 0/1000] [Batch 95/100] [D loss: -18193.470703] [D reg: 5196.458984] [G loss: -32214.148438]\n",
      "[Epoch 0/1000] [Batch 96/100] [D loss: -17814.179688] [D reg: 4863.605957] [G loss: -32214.148438]\n",
      "[Epoch 0/1000] [Batch 97/100] [D loss: -20560.318359] [D reg: 6283.216797] [G loss: -32214.148438]\n",
      "[Epoch 0/1000] [Batch 98/100] [D loss: -20135.640625] [D reg: 5182.358398] [G loss: -32214.148438]\n",
      "[Epoch 0/1000] [Batch 99/100] [D loss: -19137.839844] [D reg: 5347.556641] [G loss: -32077.162109]\n",
      "[Epoch 1/1000] [Batch 0/100] [D loss: -21946.779297] [D reg: 6249.265625] [G loss: -36842.421875]\n",
      "[Epoch 1/1000] [Batch 1/100] [D loss: -21853.650391] [D reg: 5291.916992] [G loss: -36842.421875]\n",
      "[Epoch 1/1000] [Batch 2/100] [D loss: -19135.609375] [D reg: 6068.623535] [G loss: -36842.421875]\n",
      "[Epoch 1/1000] [Batch 3/100] [D loss: -20719.023438] [D reg: 5463.534180] [G loss: -36842.421875]\n",
      "[Epoch 1/1000] [Batch 4/100] [D loss: -20842.611328] [D reg: 7650.910645] [G loss: -38193.503906]\n",
      "[Epoch 1/1000] [Batch 5/100] [D loss: -22851.521484] [D reg: 6412.355957] [G loss: -38193.503906]\n",
      "[Epoch 1/1000] [Batch 6/100] [D loss: -22454.210938] [D reg: 5836.916504] [G loss: -38193.503906]\n",
      "[Epoch 1/1000] [Batch 7/100] [D loss: -22298.281250] [D reg: 6008.264648] [G loss: -38193.503906]\n",
      "[Epoch 1/1000] [Batch 8/100] [D loss: -21301.189453] [D reg: 6496.982422] [G loss: -38193.503906]\n",
      "[Epoch 1/1000] [Batch 9/100] [D loss: -26533.001953] [D reg: 7866.032227] [G loss: -43507.054688]\n",
      "[Epoch 1/1000] [Batch 10/100] [D loss: -22296.091797] [D reg: 8286.068359] [G loss: -43507.054688]\n",
      "[Epoch 1/1000] [Batch 11/100] [D loss: -22149.623047] [D reg: 6170.772461] [G loss: -43507.054688]\n",
      "[Epoch 1/1000] [Batch 12/100] [D loss: -25388.640625] [D reg: 7288.327637] [G loss: -43507.054688]\n",
      "[Epoch 1/1000] [Batch 13/100] [D loss: -25328.435547] [D reg: 7535.951172] [G loss: -43507.054688]\n",
      "[Epoch 1/1000] [Batch 14/100] [D loss: -25038.285156] [D reg: 5531.480469] [G loss: -46219.312500]\n",
      "[Epoch 1/1000] [Batch 15/100] [D loss: -24903.390625] [D reg: 6242.043457] [G loss: -46219.312500]\n",
      "[Epoch 1/1000] [Batch 16/100] [D loss: -28470.949219] [D reg: 5845.052734] [G loss: -46219.312500]\n",
      "[Epoch 1/1000] [Batch 17/100] [D loss: -26954.107422] [D reg: 8956.203125] [G loss: -46219.312500]\n",
      "[Epoch 1/1000] [Batch 18/100] [D loss: -25089.601562] [D reg: 8212.117188] [G loss: -46219.312500]\n",
      "[Epoch 1/1000] [Batch 19/100] [D loss: -25197.478516] [D reg: 6852.927246] [G loss: -48217.250000]\n",
      "[Epoch 1/1000] [Batch 20/100] [D loss: -29460.986328] [D reg: 6487.816406] [G loss: -48217.250000]\n",
      "[Epoch 1/1000] [Batch 21/100] [D loss: -25448.279297] [D reg: 7280.911133] [G loss: -48217.250000]\n",
      "[Epoch 1/1000] [Batch 22/100] [D loss: -29671.964844] [D reg: 8795.258789] [G loss: -48217.250000]\n",
      "[Epoch 1/1000] [Batch 23/100] [D loss: -25007.771484] [D reg: 6476.981445] [G loss: -48217.250000]\n",
      "[Epoch 1/1000] [Batch 24/100] [D loss: -28902.646484] [D reg: 7567.209473] [G loss: -56001.132812]\n",
      "[Epoch 1/1000] [Batch 25/100] [D loss: -30733.201172] [D reg: 7260.458984] [G loss: -56001.132812]\n",
      "[Epoch 1/1000] [Batch 26/100] [D loss: -31788.339844] [D reg: 8791.673828] [G loss: -56001.132812]\n",
      "[Epoch 1/1000] [Batch 27/100] [D loss: -28089.001953] [D reg: 8098.009766] [G loss: -56001.132812]\n",
      "[Epoch 1/1000] [Batch 28/100] [D loss: -29103.343750] [D reg: 7373.442383] [G loss: -56001.132812]\n",
      "[Epoch 1/1000] [Batch 29/100] [D loss: -31959.927734] [D reg: 9839.875000] [G loss: -56277.046875]\n",
      "[Epoch 1/1000] [Batch 30/100] [D loss: -30091.732422] [D reg: 7843.105469] [G loss: -56277.046875]\n",
      "[Epoch 1/1000] [Batch 31/100] [D loss: -31296.160156] [D reg: 7787.097168] [G loss: -56277.046875]\n",
      "[Epoch 1/1000] [Batch 32/100] [D loss: -33996.210938] [D reg: 8223.817383] [G loss: -56277.046875]\n",
      "[Epoch 1/1000] [Batch 33/100] [D loss: -29039.951172] [D reg: 9327.524414] [G loss: -56277.046875]\n",
      "[Epoch 1/1000] [Batch 34/100] [D loss: -34297.773438] [D reg: 7550.136719] [G loss: -62313.750000]\n",
      "[Epoch 1/1000] [Batch 35/100] [D loss: -35774.625000] [D reg: 8582.201172] [G loss: -62313.750000]\n",
      "[Epoch 1/1000] [Batch 36/100] [D loss: -34868.691406] [D reg: 9265.070312] [G loss: -62313.750000]\n",
      "[Epoch 1/1000] [Batch 37/100] [D loss: -34253.851562] [D reg: 7957.021973] [G loss: -62313.750000]\n",
      "[Epoch 1/1000] [Batch 38/100] [D loss: -33270.566406] [D reg: 7925.708008] [G loss: -62313.750000]\n",
      "[Epoch 1/1000] [Batch 39/100] [D loss: -36554.808594] [D reg: 9189.693359] [G loss: -69359.156250]\n",
      "[Epoch 1/1000] [Batch 40/100] [D loss: -35700.121094] [D reg: 7273.120117] [G loss: -69359.156250]\n",
      "[Epoch 1/1000] [Batch 41/100] [D loss: -38762.640625] [D reg: 7520.680664] [G loss: -69359.156250]\n",
      "[Epoch 1/1000] [Batch 42/100] [D loss: -41245.660156] [D reg: 9171.137695] [G loss: -69359.156250]\n",
      "[Epoch 1/1000] [Batch 43/100] [D loss: -38844.664062] [D reg: 7866.092773] [G loss: -69359.156250]\n",
      "[Epoch 1/1000] [Batch 44/100] [D loss: -38120.343750] [D reg: 10319.517578] [G loss: -74650.726562]\n",
      "[Epoch 1/1000] [Batch 45/100] [D loss: -40864.230469] [D reg: 9711.105469] [G loss: -74650.726562]\n",
      "[Epoch 1/1000] [Batch 46/100] [D loss: -41160.453125] [D reg: 11551.189453] [G loss: -74650.726562]\n",
      "[Epoch 1/1000] [Batch 47/100] [D loss: -41043.042969] [D reg: 8314.380859] [G loss: -74650.726562]\n",
      "[Epoch 1/1000] [Batch 48/100] [D loss: -38470.996094] [D reg: 9210.769531] [G loss: -74650.726562]\n",
      "[Epoch 1/1000] [Batch 49/100] [D loss: -41193.859375] [D reg: 10522.286133] [G loss: -80184.609375]\n",
      "[Epoch 1/1000] [Batch 50/100] [D loss: -40978.574219] [D reg: 7611.562500] [G loss: -80184.609375]\n",
      "[Epoch 1/1000] [Batch 51/100] [D loss: -43467.585938] [D reg: 10818.460938] [G loss: -80184.609375]\n",
      "[Epoch 1/1000] [Batch 52/100] [D loss: -40110.203125] [D reg: 10593.627930] [G loss: -80184.609375]\n",
      "[Epoch 1/1000] [Batch 53/100] [D loss: -42556.710938] [D reg: 9028.907227] [G loss: -80184.609375]\n",
      "[Epoch 1/1000] [Batch 54/100] [D loss: -48597.433594] [D reg: 12235.258789] [G loss: -83951.835938]\n",
      "[Epoch 1/1000] [Batch 55/100] [D loss: -40306.625000] [D reg: 10326.001953] [G loss: -83951.835938]\n",
      "[Epoch 1/1000] [Batch 56/100] [D loss: -45395.511719] [D reg: 10117.901367] [G loss: -83951.835938]\n",
      "[Epoch 1/1000] [Batch 57/100] [D loss: -50499.117188] [D reg: 13471.962891] [G loss: -83951.835938]\n",
      "[Epoch 1/1000] [Batch 58/100] [D loss: -47836.203125] [D reg: 10379.467773] [G loss: -83951.835938]\n",
      "[Epoch 1/1000] [Batch 59/100] [D loss: -48412.246094] [D reg: 9618.418945] [G loss: -87173.531250]\n",
      "[Epoch 1/1000] [Batch 60/100] [D loss: -45861.519531] [D reg: 9621.571289] [G loss: -87173.531250]\n",
      "[Epoch 1/1000] [Batch 61/100] [D loss: -49424.503906] [D reg: 10494.666992] [G loss: -87173.531250]\n",
      "[Epoch 1/1000] [Batch 62/100] [D loss: -44571.613281] [D reg: 10678.168945] [G loss: -87173.531250]\n",
      "[Epoch 1/1000] [Batch 63/100] [D loss: -51750.750000] [D reg: 13064.268555] [G loss: -87173.531250]\n",
      "[Epoch 1/1000] [Batch 64/100] [D loss: -44621.593750] [D reg: 10349.570312] [G loss: -95134.921875]\n",
      "[Epoch 1/1000] [Batch 65/100] [D loss: -45490.828125] [D reg: 10318.565430] [G loss: -95134.921875]\n",
      "[Epoch 1/1000] [Batch 66/100] [D loss: -50896.800781] [D reg: 9187.990234] [G loss: -95134.921875]\n",
      "[Epoch 1/1000] [Batch 67/100] [D loss: -48390.390625] [D reg: 9019.226562] [G loss: -95134.921875]\n",
      "[Epoch 1/1000] [Batch 68/100] [D loss: -54166.617188] [D reg: 12565.671875] [G loss: -95134.921875]\n",
      "[Epoch 1/1000] [Batch 69/100] [D loss: -50045.867188] [D reg: 11122.846680] [G loss: -93437.312500]\n",
      "[Epoch 1/1000] [Batch 70/100] [D loss: -55743.023438] [D reg: 11493.115234] [G loss: -93437.312500]\n",
      "[Epoch 1/1000] [Batch 71/100] [D loss: -56150.945312] [D reg: 11189.371094] [G loss: -93437.312500]\n",
      "[Epoch 1/1000] [Batch 72/100] [D loss: -56965.937500] [D reg: 10670.892578] [G loss: -93437.312500]\n",
      "[Epoch 1/1000] [Batch 73/100] [D loss: -57548.667969] [D reg: 14630.073242] [G loss: -93437.312500]\n",
      "[Epoch 1/1000] [Batch 74/100] [D loss: -51187.640625] [D reg: 10235.009766] [G loss: -107933.656250]\n",
      "[Epoch 1/1000] [Batch 75/100] [D loss: -53277.269531] [D reg: 10770.688477] [G loss: -107933.656250]\n",
      "[Epoch 1/1000] [Batch 76/100] [D loss: -59135.859375] [D reg: 13717.775391] [G loss: -107933.656250]\n",
      "[Epoch 1/1000] [Batch 77/100] [D loss: -61770.039062] [D reg: 14156.792969] [G loss: -107933.656250]\n",
      "[Epoch 1/1000] [Batch 78/100] [D loss: -55714.984375] [D reg: 13232.638672] [G loss: -107933.656250]\n",
      "[Epoch 1/1000] [Batch 79/100] [D loss: -59309.511719] [D reg: 12314.005859] [G loss: -116731.421875]\n",
      "[Epoch 1/1000] [Batch 80/100] [D loss: -63917.203125] [D reg: 10888.240234] [G loss: -116731.421875]\n",
      "[Epoch 1/1000] [Batch 81/100] [D loss: -65969.640625] [D reg: 11604.333984] [G loss: -116731.421875]\n",
      "[Epoch 1/1000] [Batch 82/100] [D loss: -62280.308594] [D reg: 12520.332031] [G loss: -116731.421875]\n",
      "[Epoch 1/1000] [Batch 83/100] [D loss: -70941.921875] [D reg: 13980.802734] [G loss: -116731.421875]\n",
      "[Epoch 1/1000] [Batch 84/100] [D loss: -67030.703125] [D reg: 14597.597656] [G loss: -130580.734375]\n",
      "[Epoch 1/1000] [Batch 85/100] [D loss: -67425.367188] [D reg: 11913.214844] [G loss: -130580.734375]\n",
      "[Epoch 1/1000] [Batch 86/100] [D loss: -62782.578125] [D reg: 12821.560547] [G loss: -130580.734375]\n",
      "[Epoch 1/1000] [Batch 87/100] [D loss: -54471.226562] [D reg: 11917.675781] [G loss: -130580.734375]\n",
      "[Epoch 1/1000] [Batch 88/100] [D loss: -77979.109375] [D reg: 13911.970703] [G loss: -130580.734375]\n",
      "[Epoch 1/1000] [Batch 89/100] [D loss: -72667.281250] [D reg: 10777.553711] [G loss: -133427.312500]\n",
      "[Epoch 1/1000] [Batch 90/100] [D loss: -68823.617188] [D reg: 13339.455078] [G loss: -133427.312500]\n",
      "[Epoch 1/1000] [Batch 91/100] [D loss: -64170.234375] [D reg: 13822.055664] [G loss: -133427.312500]\n",
      "[Epoch 1/1000] [Batch 92/100] [D loss: -81591.953125] [D reg: 12439.237305] [G loss: -133427.312500]\n",
      "[Epoch 1/1000] [Batch 93/100] [D loss: -68096.312500] [D reg: 13734.809570] [G loss: -133427.312500]\n",
      "[Epoch 1/1000] [Batch 94/100] [D loss: -72850.093750] [D reg: 17664.017578] [G loss: -144188.296875]\n",
      "[Epoch 1/1000] [Batch 95/100] [D loss: -68532.500000] [D reg: 13412.977539] [G loss: -144188.296875]\n",
      "[Epoch 1/1000] [Batch 96/100] [D loss: -67360.343750] [D reg: 14185.871094] [G loss: -144188.296875]\n",
      "[Epoch 1/1000] [Batch 97/100] [D loss: -81460.476562] [D reg: 12625.939453] [G loss: -144188.296875]\n",
      "[Epoch 1/1000] [Batch 98/100] [D loss: -76277.406250] [D reg: 16916.375000] [G loss: -144188.296875]\n",
      "[Epoch 1/1000] [Batch 99/100] [D loss: -71856.625000] [D reg: 12269.369141] [G loss: -157303.500000]\n",
      "[Epoch 2/1000] [Batch 0/100] [D loss: -78173.015625] [D reg: 13259.406250] [G loss: -155263.937500]\n",
      "[Epoch 2/1000] [Batch 1/100] [D loss: -80787.007812] [D reg: 17168.269531] [G loss: -155263.937500]\n",
      "[Epoch 2/1000] [Batch 2/100] [D loss: -83627.671875] [D reg: 13290.662109] [G loss: -155263.937500]\n",
      "[Epoch 2/1000] [Batch 3/100] [D loss: -79651.062500] [D reg: 13807.580078] [G loss: -155263.937500]\n",
      "[Epoch 2/1000] [Batch 4/100] [D loss: -82965.265625] [D reg: 13684.689453] [G loss: -168380.187500]\n",
      "[Epoch 2/1000] [Batch 5/100] [D loss: -82478.843750] [D reg: 15803.276367] [G loss: -168380.187500]\n",
      "[Epoch 2/1000] [Batch 6/100] [D loss: -88172.820312] [D reg: 15746.356445] [G loss: -168380.187500]\n",
      "[Epoch 2/1000] [Batch 7/100] [D loss: -86049.328125] [D reg: 13269.019531] [G loss: -168380.187500]\n",
      "[Epoch 2/1000] [Batch 8/100] [D loss: -78860.914062] [D reg: 12361.392578] [G loss: -168380.187500]\n",
      "[Epoch 2/1000] [Batch 9/100] [D loss: -97841.484375] [D reg: 14355.317383] [G loss: -169490.296875]\n",
      "[Epoch 2/1000] [Batch 10/100] [D loss: -92563.015625] [D reg: 13724.601562] [G loss: -169490.296875]\n",
      "[Epoch 2/1000] [Batch 11/100] [D loss: -94652.953125] [D reg: 14745.322266] [G loss: -169490.296875]\n",
      "[Epoch 2/1000] [Batch 12/100] [D loss: -98612.375000] [D reg: 18849.015625] [G loss: -169490.296875]\n",
      "[Epoch 2/1000] [Batch 13/100] [D loss: -95102.843750] [D reg: 14247.583008] [G loss: -169490.296875]\n",
      "[Epoch 2/1000] [Batch 14/100] [D loss: -88401.257812] [D reg: 15618.031250] [G loss: -172827.250000]\n",
      "[Epoch 2/1000] [Batch 15/100] [D loss: -103138.390625] [D reg: 18960.304688] [G loss: -172827.250000]\n",
      "[Epoch 2/1000] [Batch 16/100] [D loss: -93085.421875] [D reg: 20510.888672] [G loss: -172827.250000]\n",
      "[Epoch 2/1000] [Batch 17/100] [D loss: -97743.656250] [D reg: 15691.987305] [G loss: -172827.250000]\n",
      "[Epoch 2/1000] [Batch 18/100] [D loss: -105663.148438] [D reg: 14584.956055] [G loss: -172827.250000]\n",
      "[Epoch 2/1000] [Batch 19/100] [D loss: -97693.250000] [D reg: 14098.906250] [G loss: -192153.781250]\n",
      "[Epoch 2/1000] [Batch 20/100] [D loss: -108543.703125] [D reg: 17008.800781] [G loss: -192153.781250]\n",
      "[Epoch 2/1000] [Batch 21/100] [D loss: -105430.882812] [D reg: 20500.919922] [G loss: -192153.781250]\n",
      "[Epoch 2/1000] [Batch 22/100] [D loss: -109083.085938] [D reg: 23744.394531] [G loss: -192153.781250]\n",
      "[Epoch 2/1000] [Batch 23/100] [D loss: -103883.406250] [D reg: 23458.447266] [G loss: -192153.781250]\n",
      "[Epoch 2/1000] [Batch 24/100] [D loss: -100508.921875] [D reg: 18624.632812] [G loss: -212080.203125]\n",
      "[Epoch 2/1000] [Batch 25/100] [D loss: -117330.484375] [D reg: 17506.511719] [G loss: -212080.203125]\n",
      "[Epoch 2/1000] [Batch 26/100] [D loss: -107357.468750] [D reg: 16058.666016] [G loss: -212080.203125]\n",
      "[Epoch 2/1000] [Batch 27/100] [D loss: -100529.453125] [D reg: 17290.035156] [G loss: -212080.203125]\n",
      "[Epoch 2/1000] [Batch 28/100] [D loss: -113063.617188] [D reg: 20523.361328] [G loss: -212080.203125]\n",
      "[Epoch 2/1000] [Batch 29/100] [D loss: -99234.375000] [D reg: 14063.037109] [G loss: -211787.484375]\n",
      "[Epoch 2/1000] [Batch 30/100] [D loss: -111730.718750] [D reg: 20019.853516] [G loss: -211787.484375]\n",
      "[Epoch 2/1000] [Batch 31/100] [D loss: -103300.953125] [D reg: 18480.369141] [G loss: -211787.484375]\n",
      "[Epoch 2/1000] [Batch 32/100] [D loss: -100921.523438] [D reg: 13601.144531] [G loss: -211787.484375]\n",
      "[Epoch 2/1000] [Batch 33/100] [D loss: -106354.359375] [D reg: 18912.716797] [G loss: -211787.484375]\n",
      "[Epoch 2/1000] [Batch 34/100] [D loss: -117470.765625] [D reg: 20913.792969] [G loss: -237416.625000]\n",
      "[Epoch 2/1000] [Batch 35/100] [D loss: -108338.132812] [D reg: 17837.847656] [G loss: -237416.625000]\n",
      "[Epoch 2/1000] [Batch 36/100] [D loss: -103561.382812] [D reg: 14516.928711] [G loss: -237416.625000]\n",
      "[Epoch 2/1000] [Batch 37/100] [D loss: -127629.031250] [D reg: 36832.296875] [G loss: -237416.625000]\n",
      "[Epoch 2/1000] [Batch 38/100] [D loss: -120414.265625] [D reg: 22028.154297] [G loss: -237416.625000]\n",
      "[Epoch 2/1000] [Batch 39/100] [D loss: -121595.093750] [D reg: 15863.332031] [G loss: -233478.546875]\n",
      "[Epoch 2/1000] [Batch 40/100] [D loss: -110017.218750] [D reg: 15508.068359] [G loss: -233478.546875]\n",
      "[Epoch 2/1000] [Batch 41/100] [D loss: -130411.171875] [D reg: 15372.764648] [G loss: -233478.546875]\n",
      "[Epoch 2/1000] [Batch 42/100] [D loss: -121764.250000] [D reg: 15403.787109] [G loss: -233478.546875]\n",
      "[Epoch 2/1000] [Batch 43/100] [D loss: -122627.039062] [D reg: 21750.791016] [G loss: -233478.546875]\n",
      "[Epoch 2/1000] [Batch 44/100] [D loss: -134938.453125] [D reg: 15974.893555] [G loss: -249515.546875]\n",
      "[Epoch 2/1000] [Batch 45/100] [D loss: -141387.421875] [D reg: 12521.767578] [G loss: -249515.546875]\n",
      "[Epoch 2/1000] [Batch 46/100] [D loss: -142286.593750] [D reg: 18498.998047] [G loss: -249515.546875]\n",
      "[Epoch 2/1000] [Batch 47/100] [D loss: -124459.445312] [D reg: 19834.287109] [G loss: -249515.546875]\n",
      "[Epoch 2/1000] [Batch 48/100] [D loss: -135933.421875] [D reg: 15066.179688] [G loss: -249515.546875]\n",
      "[Epoch 2/1000] [Batch 49/100] [D loss: -151193.968750] [D reg: 19910.291016] [G loss: -262644.812500]\n",
      "[Epoch 2/1000] [Batch 50/100] [D loss: -135412.625000] [D reg: 22512.679688] [G loss: -262644.812500]\n",
      "[Epoch 2/1000] [Batch 51/100] [D loss: -150428.671875] [D reg: 18994.738281] [G loss: -262644.812500]\n",
      "[Epoch 2/1000] [Batch 52/100] [D loss: -148045.750000] [D reg: 20461.380859] [G loss: -262644.812500]\n",
      "[Epoch 2/1000] [Batch 53/100] [D loss: -151468.671875] [D reg: 17630.857422] [G loss: -262644.812500]\n",
      "[Epoch 2/1000] [Batch 54/100] [D loss: -136535.562500] [D reg: 21616.533203] [G loss: -275494.437500]\n",
      "[Epoch 2/1000] [Batch 55/100] [D loss: -151561.781250] [D reg: 17354.060547] [G loss: -275494.437500]\n",
      "[Epoch 2/1000] [Batch 56/100] [D loss: -131239.875000] [D reg: 18841.113281] [G loss: -275494.437500]\n",
      "[Epoch 2/1000] [Batch 57/100] [D loss: -165687.703125] [D reg: 15501.712891] [G loss: -275494.437500]\n",
      "[Epoch 2/1000] [Batch 58/100] [D loss: -146605.828125] [D reg: 23182.265625] [G loss: -275494.437500]\n",
      "[Epoch 2/1000] [Batch 59/100] [D loss: -156601.156250] [D reg: 22068.425781] [G loss: -277539.656250]\n",
      "[Epoch 2/1000] [Batch 60/100] [D loss: -164146.000000] [D reg: 17205.957031] [G loss: -277539.656250]\n",
      "[Epoch 2/1000] [Batch 61/100] [D loss: -161192.828125] [D reg: 19481.896484] [G loss: -277539.656250]\n",
      "[Epoch 2/1000] [Batch 62/100] [D loss: -152659.406250] [D reg: 15775.291992] [G loss: -277539.656250]\n",
      "[Epoch 2/1000] [Batch 63/100] [D loss: -185739.625000] [D reg: 21787.236328] [G loss: -277539.656250]\n",
      "[Epoch 2/1000] [Batch 64/100] [D loss: -166037.156250] [D reg: 24002.406250] [G loss: -317149.062500]\n",
      "[Epoch 2/1000] [Batch 65/100] [D loss: -163865.875000] [D reg: 25057.753906] [G loss: -317149.062500]\n",
      "[Epoch 2/1000] [Batch 66/100] [D loss: -166186.531250] [D reg: 21379.794922] [G loss: -317149.062500]\n",
      "[Epoch 2/1000] [Batch 67/100] [D loss: -173105.531250] [D reg: 18607.591797] [G loss: -317149.062500]\n",
      "[Epoch 2/1000] [Batch 68/100] [D loss: -164701.531250] [D reg: 19692.921875] [G loss: -317149.062500]\n",
      "[Epoch 2/1000] [Batch 69/100] [D loss: -154482.031250] [D reg: 16541.480469] [G loss: -338973.937500]\n",
      "[Epoch 2/1000] [Batch 70/100] [D loss: -173858.562500] [D reg: 23203.242188] [G loss: -338973.937500]\n",
      "[Epoch 2/1000] [Batch 71/100] [D loss: -175671.328125] [D reg: 22154.585938] [G loss: -338973.937500]\n",
      "[Epoch 2/1000] [Batch 72/100] [D loss: -177779.750000] [D reg: 19911.541016] [G loss: -338973.937500]\n",
      "[Epoch 2/1000] [Batch 73/100] [D loss: -177564.312500] [D reg: 26138.414062] [G loss: -338973.937500]\n",
      "[Epoch 2/1000] [Batch 74/100] [D loss: -177891.531250] [D reg: 18968.960938] [G loss: -341241.156250]\n",
      "[Epoch 2/1000] [Batch 75/100] [D loss: -185162.734375] [D reg: 21914.175781] [G loss: -341241.156250]\n",
      "[Epoch 2/1000] [Batch 76/100] [D loss: -192410.656250] [D reg: 25449.132812] [G loss: -341241.156250]\n",
      "[Epoch 2/1000] [Batch 77/100] [D loss: -180729.593750] [D reg: 17384.345703] [G loss: -341241.156250]\n",
      "[Epoch 2/1000] [Batch 78/100] [D loss: -168891.734375] [D reg: 18795.875000] [G loss: -341241.156250]\n",
      "[Epoch 2/1000] [Batch 79/100] [D loss: -200994.062500] [D reg: 20654.785156] [G loss: -362175.156250]\n",
      "[Epoch 2/1000] [Batch 80/100] [D loss: -181176.968750] [D reg: 21488.146484] [G loss: -362175.156250]\n",
      "[Epoch 2/1000] [Batch 81/100] [D loss: -191392.906250] [D reg: 18098.716797] [G loss: -362175.156250]\n",
      "[Epoch 2/1000] [Batch 82/100] [D loss: -178602.093750] [D reg: 21285.634766] [G loss: -362175.156250]\n",
      "[Epoch 2/1000] [Batch 83/100] [D loss: -189426.125000] [D reg: 17256.648438] [G loss: -362175.156250]\n",
      "[Epoch 2/1000] [Batch 84/100] [D loss: -195852.250000] [D reg: 22941.365234] [G loss: -381654.093750]\n",
      "[Epoch 2/1000] [Batch 85/100] [D loss: -199838.843750] [D reg: 30803.023438] [G loss: -381654.093750]\n",
      "[Epoch 2/1000] [Batch 86/100] [D loss: -189633.609375] [D reg: 25722.707031] [G loss: -381654.093750]\n",
      "[Epoch 2/1000] [Batch 87/100] [D loss: -207440.125000] [D reg: 25085.126953] [G loss: -381654.093750]\n",
      "[Epoch 2/1000] [Batch 88/100] [D loss: -208209.437500] [D reg: 31289.033203] [G loss: -381654.093750]\n",
      "[Epoch 2/1000] [Batch 89/100] [D loss: -200005.046875] [D reg: 30258.193359] [G loss: -417979.625000]\n",
      "[Epoch 2/1000] [Batch 90/100] [D loss: -209659.875000] [D reg: 35987.597656] [G loss: -417979.625000]\n",
      "[Epoch 2/1000] [Batch 91/100] [D loss: -201104.812500] [D reg: 32195.343750] [G loss: -417979.625000]\n",
      "[Epoch 2/1000] [Batch 92/100] [D loss: -218008.468750] [D reg: 39087.398438] [G loss: -417979.625000]\n",
      "[Epoch 2/1000] [Batch 93/100] [D loss: -211067.218750] [D reg: 47286.125000] [G loss: -417979.625000]\n",
      "[Epoch 2/1000] [Batch 94/100] [D loss: -215654.968750] [D reg: 35922.171875] [G loss: -422814.125000]\n",
      "[Epoch 2/1000] [Batch 95/100] [D loss: -228843.781250] [D reg: 48465.390625] [G loss: -422814.125000]\n",
      "[Epoch 2/1000] [Batch 96/100] [D loss: -212677.953125] [D reg: 37835.996094] [G loss: -422814.125000]\n",
      "[Epoch 2/1000] [Batch 97/100] [D loss: -227085.734375] [D reg: 25552.382812] [G loss: -422814.125000]\n",
      "[Epoch 2/1000] [Batch 98/100] [D loss: -235743.031250] [D reg: 47752.148438] [G loss: -422814.125000]\n",
      "[Epoch 2/1000] [Batch 99/100] [D loss: -223958.187500] [D reg: 46986.601562] [G loss: -452013.500000]\n",
      "[Epoch 3/1000] [Batch 0/100] [D loss: -170211.125000] [D reg: 39877.359375] [G loss: -411058.625000]\n",
      "[Epoch 3/1000] [Batch 1/100] [D loss: -217412.937500] [D reg: 76193.203125] [G loss: -411058.625000]\n",
      "[Epoch 3/1000] [Batch 2/100] [D loss: -228629.000000] [D reg: 48386.523438] [G loss: -411058.625000]\n",
      "[Epoch 3/1000] [Batch 3/100] [D loss: -245268.578125] [D reg: 70314.101562] [G loss: -411058.625000]\n",
      "[Epoch 3/1000] [Batch 4/100] [D loss: -245143.593750] [D reg: 61437.062500] [G loss: -464402.281250]\n",
      "[Epoch 3/1000] [Batch 5/100] [D loss: -241867.578125] [D reg: 53108.570312] [G loss: -464402.281250]\n",
      "[Epoch 3/1000] [Batch 6/100] [D loss: -239291.359375] [D reg: 71631.015625] [G loss: -464402.281250]\n",
      "[Epoch 3/1000] [Batch 7/100] [D loss: -231693.093750] [D reg: 66248.500000] [G loss: -464402.281250]\n",
      "[Epoch 3/1000] [Batch 8/100] [D loss: -221156.812500] [D reg: 67078.507812] [G loss: -464402.281250]\n",
      "[Epoch 3/1000] [Batch 9/100] [D loss: -247362.000000] [D reg: 54276.617188] [G loss: -455002.937500]\n",
      "[Epoch 3/1000] [Batch 10/100] [D loss: -229716.250000] [D reg: 45488.183594] [G loss: -455002.937500]\n",
      "[Epoch 3/1000] [Batch 11/100] [D loss: -225962.468750] [D reg: 52730.531250] [G loss: -455002.937500]\n",
      "[Epoch 3/1000] [Batch 12/100] [D loss: -225739.031250] [D reg: 65321.648438] [G loss: -455002.937500]\n",
      "[Epoch 3/1000] [Batch 13/100] [D loss: -231279.281250] [D reg: 86604.023438] [G loss: -455002.937500]\n",
      "[Epoch 3/1000] [Batch 14/100] [D loss: -247751.078125] [D reg: 120410.437500] [G loss: -475598.593750]\n",
      "[Epoch 3/1000] [Batch 15/100] [D loss: -256148.890625] [D reg: 130571.281250] [G loss: -475598.593750]\n",
      "[Epoch 3/1000] [Batch 16/100] [D loss: -263285.750000] [D reg: 119694.960938] [G loss: -475598.593750]\n",
      "[Epoch 3/1000] [Batch 17/100] [D loss: -258879.984375] [D reg: 97908.687500] [G loss: -475598.593750]\n",
      "[Epoch 3/1000] [Batch 18/100] [D loss: -250373.125000] [D reg: 105289.250000] [G loss: -475598.593750]\n",
      "[Epoch 3/1000] [Batch 19/100] [D loss: -252361.000000] [D reg: 103147.406250] [G loss: -484036.000000]\n",
      "[Epoch 3/1000] [Batch 20/100] [D loss: -264710.000000] [D reg: 99232.687500] [G loss: -484036.000000]\n",
      "[Epoch 3/1000] [Batch 21/100] [D loss: -235899.500000] [D reg: 89228.109375] [G loss: -484036.000000]\n",
      "[Epoch 3/1000] [Batch 22/100] [D loss: -245045.625000] [D reg: 89530.718750] [G loss: -484036.000000]\n",
      "[Epoch 3/1000] [Batch 23/100] [D loss: -252518.750000] [D reg: 87469.218750] [G loss: -484036.000000]\n",
      "[Epoch 3/1000] [Batch 24/100] [D loss: -220677.000000] [D reg: 78544.882812] [G loss: -461855.750000]\n",
      "[Epoch 3/1000] [Batch 25/100] [D loss: -243946.312500] [D reg: 109488.242188] [G loss: -461855.750000]\n",
      "[Epoch 3/1000] [Batch 26/100] [D loss: -219294.750000] [D reg: 106670.031250] [G loss: -461855.750000]\n",
      "[Epoch 3/1000] [Batch 27/100] [D loss: -231070.906250] [D reg: 103867.203125] [G loss: -461855.750000]\n",
      "[Epoch 3/1000] [Batch 28/100] [D loss: -257782.515625] [D reg: 112291.062500] [G loss: -461855.750000]\n",
      "[Epoch 3/1000] [Batch 29/100] [D loss: -226912.000000] [D reg: 99575.609375] [G loss: -455070.500000]\n",
      "[Epoch 3/1000] [Batch 30/100] [D loss: -246397.140625] [D reg: 86997.843750] [G loss: -455070.500000]\n",
      "[Epoch 3/1000] [Batch 31/100] [D loss: -232217.281250] [D reg: 98484.312500] [G loss: -455070.500000]\n",
      "[Epoch 3/1000] [Batch 32/100] [D loss: -212652.718750] [D reg: 66294.250000] [G loss: -455070.500000]\n",
      "[Epoch 3/1000] [Batch 33/100] [D loss: -218977.546875] [D reg: 83250.851562] [G loss: -455070.500000]\n",
      "[Epoch 3/1000] [Batch 34/100] [D loss: -235677.781250] [D reg: 75875.296875] [G loss: -447416.906250]\n",
      "[Epoch 3/1000] [Batch 35/100] [D loss: -258919.968750] [D reg: 76399.296875] [G loss: -447416.906250]\n",
      "[Epoch 3/1000] [Batch 36/100] [D loss: -226763.500000] [D reg: 78162.953125] [G loss: -447416.906250]\n",
      "[Epoch 3/1000] [Batch 37/100] [D loss: -209358.781250] [D reg: 81836.125000] [G loss: -447416.906250]\n",
      "[Epoch 3/1000] [Batch 38/100] [D loss: -240586.109375] [D reg: 58751.156250] [G loss: -447416.906250]\n",
      "[Epoch 3/1000] [Batch 39/100] [D loss: -212334.500000] [D reg: 78458.062500] [G loss: -484034.125000]\n",
      "[Epoch 3/1000] [Batch 40/100] [D loss: -229994.687500] [D reg: 71946.468750] [G loss: -484034.125000]\n",
      "[Epoch 3/1000] [Batch 41/100] [D loss: -248330.718750] [D reg: 54030.761719] [G loss: -484034.125000]\n",
      "[Epoch 3/1000] [Batch 42/100] [D loss: -228396.328125] [D reg: 82616.187500] [G loss: -484034.125000]\n",
      "[Epoch 3/1000] [Batch 43/100] [D loss: -252710.093750] [D reg: 46200.343750] [G loss: -484034.125000]\n",
      "[Epoch 3/1000] [Batch 44/100] [D loss: -206654.250000] [D reg: 44962.597656] [G loss: -418148.562500]\n",
      "[Epoch 3/1000] [Batch 45/100] [D loss: -200547.312500] [D reg: 36922.234375] [G loss: -418148.562500]\n",
      "[Epoch 3/1000] [Batch 46/100] [D loss: -204211.718750] [D reg: 31818.349609] [G loss: -418148.562500]\n",
      "[Epoch 3/1000] [Batch 47/100] [D loss: -219644.093750] [D reg: 47289.550781] [G loss: -418148.562500]\n",
      "[Epoch 3/1000] [Batch 48/100] [D loss: -226823.875000] [D reg: 41902.230469] [G loss: -418148.562500]\n",
      "[Epoch 3/1000] [Batch 49/100] [D loss: -219439.500000] [D reg: 50314.265625] [G loss: -456896.125000]\n",
      "[Epoch 3/1000] [Batch 50/100] [D loss: -246804.875000] [D reg: 54946.699219] [G loss: -456896.125000]\n",
      "[Epoch 3/1000] [Batch 51/100] [D loss: -236992.843750] [D reg: 68386.640625] [G loss: -456896.125000]\n",
      "[Epoch 3/1000] [Batch 52/100] [D loss: -248945.093750] [D reg: 64104.367188] [G loss: -456896.125000]\n",
      "[Epoch 3/1000] [Batch 53/100] [D loss: -231503.250000] [D reg: 82381.710938] [G loss: -456896.125000]\n",
      "[Epoch 3/1000] [Batch 54/100] [D loss: -292267.000000] [D reg: 84414.671875] [G loss: -482705.218750]\n",
      "[Epoch 3/1000] [Batch 55/100] [D loss: -252609.625000] [D reg: 87410.218750] [G loss: -482705.218750]\n",
      "[Epoch 3/1000] [Batch 56/100] [D loss: -284614.593750] [D reg: 90399.421875] [G loss: -482705.218750]\n",
      "[Epoch 3/1000] [Batch 57/100] [D loss: -280286.375000] [D reg: 104532.046875] [G loss: -482705.218750]\n",
      "[Epoch 3/1000] [Batch 58/100] [D loss: -249259.000000] [D reg: 86286.781250] [G loss: -482705.218750]\n",
      "[Epoch 3/1000] [Batch 59/100] [D loss: -219486.250000] [D reg: 64786.894531] [G loss: -436753.000000]\n",
      "[Epoch 3/1000] [Batch 60/100] [D loss: -222635.937500] [D reg: 74015.546875] [G loss: -436753.000000]\n",
      "[Epoch 3/1000] [Batch 61/100] [D loss: -207228.687500] [D reg: 36843.859375] [G loss: -436753.000000]\n",
      "[Epoch 3/1000] [Batch 62/100] [D loss: -208728.750000] [D reg: 45730.902344] [G loss: -436753.000000]\n",
      "[Epoch 3/1000] [Batch 63/100] [D loss: -236861.453125] [D reg: 44249.367188] [G loss: -436753.000000]\n",
      "[Epoch 3/1000] [Batch 64/100] [D loss: -210263.250000] [D reg: 42580.523438] [G loss: -420431.625000]\n",
      "[Epoch 3/1000] [Batch 65/100] [D loss: -188449.843750] [D reg: 28944.757812] [G loss: -420431.625000]\n",
      "[Epoch 3/1000] [Batch 66/100] [D loss: -231699.093750] [D reg: 41188.925781] [G loss: -420431.625000]\n",
      "[Epoch 3/1000] [Batch 67/100] [D loss: -245135.734375] [D reg: 54618.089844] [G loss: -420431.625000]\n",
      "[Epoch 3/1000] [Batch 68/100] [D loss: -233005.468750] [D reg: 35149.773438] [G loss: -420431.625000]\n",
      "[Epoch 3/1000] [Batch 69/100] [D loss: -234028.078125] [D reg: 54737.234375] [G loss: -475257.687500]\n",
      "[Epoch 3/1000] [Batch 70/100] [D loss: -241554.812500] [D reg: 51339.789062] [G loss: -475257.687500]\n",
      "[Epoch 3/1000] [Batch 71/100] [D loss: -239964.312500] [D reg: 35457.085938] [G loss: -475257.687500]\n",
      "[Epoch 3/1000] [Batch 72/100] [D loss: -210686.406250] [D reg: 53356.503906] [G loss: -475257.687500]\n",
      "[Epoch 3/1000] [Batch 73/100] [D loss: -262670.687500] [D reg: 50902.468750] [G loss: -475257.687500]\n",
      "[Epoch 3/1000] [Batch 74/100] [D loss: -244797.031250] [D reg: 35997.035156] [G loss: -483198.156250]\n",
      "[Epoch 3/1000] [Batch 75/100] [D loss: -237815.656250] [D reg: 39491.332031] [G loss: -483198.156250]\n",
      "[Epoch 3/1000] [Batch 76/100] [D loss: -215381.718750] [D reg: 47098.808594] [G loss: -483198.156250]\n",
      "[Epoch 3/1000] [Batch 77/100] [D loss: -188794.562500] [D reg: 38766.035156] [G loss: -483198.156250]\n",
      "[Epoch 3/1000] [Batch 78/100] [D loss: -203587.312500] [D reg: 33589.640625] [G loss: -483198.156250]\n",
      "[Epoch 3/1000] [Batch 79/100] [D loss: -217294.281250] [D reg: 35422.339844] [G loss: -418838.687500]\n",
      "[Epoch 3/1000] [Batch 80/100] [D loss: -187163.218750] [D reg: 28747.089844] [G loss: -418838.687500]\n",
      "[Epoch 3/1000] [Batch 81/100] [D loss: -178515.546875] [D reg: 29071.425781] [G loss: -418838.687500]\n",
      "[Epoch 3/1000] [Batch 82/100] [D loss: -175854.218750] [D reg: 32195.453125] [G loss: -418838.687500]\n",
      "[Epoch 3/1000] [Batch 83/100] [D loss: -162255.000000] [D reg: 33138.289062] [G loss: -418838.687500]\n",
      "[Epoch 3/1000] [Batch 84/100] [D loss: -179960.437500] [D reg: 27888.257812] [G loss: -324495.343750]\n",
      "[Epoch 3/1000] [Batch 85/100] [D loss: -179728.875000] [D reg: 26442.203125] [G loss: -324495.343750]\n",
      "[Epoch 3/1000] [Batch 86/100] [D loss: -180578.406250] [D reg: 32276.240234] [G loss: -324495.343750]\n",
      "[Epoch 3/1000] [Batch 87/100] [D loss: -165846.906250] [D reg: 28100.863281] [G loss: -324495.343750]\n",
      "[Epoch 3/1000] [Batch 88/100] [D loss: -188612.312500] [D reg: 22087.480469] [G loss: -324495.343750]\n",
      "[Epoch 3/1000] [Batch 89/100] [D loss: -174006.718750] [D reg: 28246.328125] [G loss: -351961.625000]\n",
      "[Epoch 3/1000] [Batch 90/100] [D loss: -206423.281250] [D reg: 33275.269531] [G loss: -351961.625000]\n",
      "[Epoch 3/1000] [Batch 91/100] [D loss: -225492.156250] [D reg: 38041.425781] [G loss: -351961.625000]\n",
      "[Epoch 3/1000] [Batch 92/100] [D loss: -222360.390625] [D reg: 38299.085938] [G loss: -351961.625000]\n",
      "[Epoch 3/1000] [Batch 93/100] [D loss: -216898.078125] [D reg: 51129.304688] [G loss: -351961.625000]\n",
      "[Epoch 3/1000] [Batch 94/100] [D loss: -204650.421875] [D reg: 34737.125000] [G loss: -388637.281250]\n",
      "[Epoch 3/1000] [Batch 95/100] [D loss: -195503.593750] [D reg: 27650.791016] [G loss: -388637.281250]\n",
      "[Epoch 3/1000] [Batch 96/100] [D loss: -186975.000000] [D reg: 21212.222656] [G loss: -388637.281250]\n",
      "[Epoch 3/1000] [Batch 97/100] [D loss: -186348.156250] [D reg: 24373.544922] [G loss: -388637.281250]\n",
      "[Epoch 3/1000] [Batch 98/100] [D loss: -174640.968750] [D reg: 30271.396484] [G loss: -388637.281250]\n",
      "[Epoch 3/1000] [Batch 99/100] [D loss: -180173.953125] [D reg: 29734.507812] [G loss: -351274.656250]\n",
      "[Epoch 4/1000] [Batch 0/100] [D loss: -131151.843750] [D reg: 17003.628906] [G loss: -298820.750000]\n",
      "[Epoch 4/1000] [Batch 1/100] [D loss: -137015.250000] [D reg: 17402.410156] [G loss: -298820.750000]\n",
      "[Epoch 4/1000] [Batch 2/100] [D loss: -156579.203125] [D reg: 17979.218750] [G loss: -298820.750000]\n",
      "[Epoch 4/1000] [Batch 3/100] [D loss: -155041.812500] [D reg: 23089.312500] [G loss: -298820.750000]\n",
      "[Epoch 4/1000] [Batch 4/100] [D loss: -142148.562500] [D reg: 24646.582031] [G loss: -326734.406250]\n",
      "[Epoch 4/1000] [Batch 5/100] [D loss: -180476.093750] [D reg: 22275.402344] [G loss: -326734.406250]\n",
      "[Epoch 4/1000] [Batch 6/100] [D loss: -172154.484375] [D reg: 24288.472656] [G loss: -326734.406250]\n",
      "[Epoch 4/1000] [Batch 7/100] [D loss: -202778.875000] [D reg: 27852.914062] [G loss: -326734.406250]\n",
      "[Epoch 4/1000] [Batch 8/100] [D loss: -174771.171875] [D reg: 24954.865234] [G loss: -326734.406250]\n",
      "[Epoch 4/1000] [Batch 9/100] [D loss: -161130.578125] [D reg: 30716.949219] [G loss: -319838.125000]\n",
      "[Epoch 4/1000] [Batch 10/100] [D loss: -153564.859375] [D reg: 27380.654297] [G loss: -319838.125000]\n",
      "[Epoch 4/1000] [Batch 11/100] [D loss: -163719.390625] [D reg: 26460.742188] [G loss: -319838.125000]\n",
      "[Epoch 4/1000] [Batch 12/100] [D loss: -162702.750000] [D reg: 18793.410156] [G loss: -319838.125000]\n",
      "[Epoch 4/1000] [Batch 13/100] [D loss: -149055.750000] [D reg: 17122.886719] [G loss: -319838.125000]\n",
      "[Epoch 4/1000] [Batch 14/100] [D loss: -154289.187500] [D reg: 17766.070312] [G loss: -292428.250000]\n",
      "[Epoch 4/1000] [Batch 15/100] [D loss: -143595.625000] [D reg: 21922.250000] [G loss: -292428.250000]\n",
      "[Epoch 4/1000] [Batch 16/100] [D loss: -190168.218750] [D reg: 27922.933594] [G loss: -292428.250000]\n",
      "[Epoch 4/1000] [Batch 17/100] [D loss: -155958.125000] [D reg: 25044.404297] [G loss: -292428.250000]\n",
      "[Epoch 4/1000] [Batch 18/100] [D loss: -165978.578125] [D reg: 21032.281250] [G loss: -292428.250000]\n",
      "[Epoch 4/1000] [Batch 19/100] [D loss: -150679.750000] [D reg: 20870.810547] [G loss: -318698.593750]\n",
      "[Epoch 4/1000] [Batch 20/100] [D loss: -181041.187500] [D reg: 22828.447266] [G loss: -318698.593750]\n",
      "[Epoch 4/1000] [Batch 21/100] [D loss: -157540.609375] [D reg: 15342.895508] [G loss: -318698.593750]\n",
      "[Epoch 4/1000] [Batch 22/100] [D loss: -137030.156250] [D reg: 16352.552734] [G loss: -318698.593750]\n",
      "[Epoch 4/1000] [Batch 23/100] [D loss: -127814.015625] [D reg: 17983.218750] [G loss: -318698.593750]\n",
      "[Epoch 4/1000] [Batch 24/100] [D loss: -138596.343750] [D reg: 12251.195312] [G loss: -256232.000000]\n",
      "[Epoch 4/1000] [Batch 25/100] [D loss: -155915.593750] [D reg: 16457.082031] [G loss: -256232.000000]\n",
      "[Epoch 4/1000] [Batch 26/100] [D loss: -163693.375000] [D reg: 15338.505859] [G loss: -256232.000000]\n",
      "[Epoch 4/1000] [Batch 27/100] [D loss: -176078.250000] [D reg: 19482.570312] [G loss: -256232.000000]\n",
      "[Epoch 4/1000] [Batch 28/100] [D loss: -140876.296875] [D reg: 14602.934570] [G loss: -256232.000000]\n",
      "[Epoch 4/1000] [Batch 29/100] [D loss: -140909.937500] [D reg: 16948.449219] [G loss: -282843.718750]\n",
      "[Epoch 4/1000] [Batch 30/100] [D loss: -146170.312500] [D reg: 13328.715820] [G loss: -282843.718750]\n",
      "[Epoch 4/1000] [Batch 31/100] [D loss: -125753.968750] [D reg: 12726.279297] [G loss: -282843.718750]\n",
      "[Epoch 4/1000] [Batch 32/100] [D loss: -128566.343750] [D reg: 14644.272461] [G loss: -282843.718750]\n",
      "[Epoch 4/1000] [Batch 33/100] [D loss: -142788.031250] [D reg: 16193.016602] [G loss: -282843.718750]\n",
      "[Epoch 4/1000] [Batch 34/100] [D loss: -166066.312500] [D reg: 17012.970703] [G loss: -320823.218750]\n",
      "[Epoch 4/1000] [Batch 35/100] [D loss: -193289.984375] [D reg: 22117.835938] [G loss: -320823.218750]\n",
      "[Epoch 4/1000] [Batch 36/100] [D loss: -152851.078125] [D reg: 13711.811523] [G loss: -320823.218750]\n",
      "[Epoch 4/1000] [Batch 37/100] [D loss: -187444.343750] [D reg: 22081.109375] [G loss: -320823.218750]\n",
      "[Epoch 4/1000] [Batch 38/100] [D loss: -195209.828125] [D reg: 22791.312500] [G loss: -320823.218750]\n",
      "[Epoch 4/1000] [Batch 39/100] [D loss: -248320.687500] [D reg: 35102.343750] [G loss: -459959.437500]\n",
      "[Epoch 4/1000] [Batch 40/100] [D loss: -184855.218750] [D reg: 25260.171875] [G loss: -459959.437500]\n",
      "[Epoch 4/1000] [Batch 41/100] [D loss: -173205.406250] [D reg: 22348.785156] [G loss: -459959.437500]\n",
      "[Epoch 4/1000] [Batch 42/100] [D loss: -182317.078125] [D reg: 23162.732422] [G loss: -459959.437500]\n",
      "[Epoch 4/1000] [Batch 43/100] [D loss: -167321.968750] [D reg: 23429.789062] [G loss: -459959.437500]\n",
      "[Epoch 4/1000] [Batch 44/100] [D loss: -169654.203125] [D reg: 24046.650391] [G loss: -328915.000000]\n",
      "[Epoch 4/1000] [Batch 45/100] [D loss: -179323.421875] [D reg: 17275.994141] [G loss: -328915.000000]\n",
      "[Epoch 4/1000] [Batch 46/100] [D loss: -154635.437500] [D reg: 20217.732422] [G loss: -328915.000000]\n",
      "[Epoch 4/1000] [Batch 47/100] [D loss: -153717.468750] [D reg: 14782.068359] [G loss: -328915.000000]\n",
      "[Epoch 4/1000] [Batch 48/100] [D loss: -154017.468750] [D reg: 17770.535156] [G loss: -328915.000000]\n",
      "[Epoch 4/1000] [Batch 49/100] [D loss: -160024.406250] [D reg: 19060.701172] [G loss: -307874.531250]\n",
      "[Epoch 4/1000] [Batch 50/100] [D loss: -180664.750000] [D reg: 21279.312500] [G loss: -307874.531250]\n",
      "[Epoch 4/1000] [Batch 51/100] [D loss: -161508.890625] [D reg: 20105.878906] [G loss: -307874.531250]\n",
      "[Epoch 4/1000] [Batch 52/100] [D loss: -170824.281250] [D reg: 18708.410156] [G loss: -307874.531250]\n",
      "[Epoch 4/1000] [Batch 53/100] [D loss: -151098.781250] [D reg: 12680.486328] [G loss: -307874.531250]\n",
      "[Epoch 4/1000] [Batch 54/100] [D loss: -168719.765625] [D reg: 16296.052734] [G loss: -325381.875000]\n",
      "[Epoch 4/1000] [Batch 55/100] [D loss: -162448.031250] [D reg: 12459.515625] [G loss: -325381.875000]\n",
      "[Epoch 4/1000] [Batch 56/100] [D loss: -150852.687500] [D reg: 15151.758789] [G loss: -325381.875000]\n",
      "[Epoch 4/1000] [Batch 57/100] [D loss: -165405.437500] [D reg: 17309.457031] [G loss: -325381.875000]\n",
      "[Epoch 4/1000] [Batch 58/100] [D loss: -182375.062500] [D reg: 22177.992188] [G loss: -325381.875000]\n",
      "[Epoch 4/1000] [Batch 59/100] [D loss: -230501.093750] [D reg: 36603.308594] [G loss: -461671.562500]\n",
      "[Epoch 4/1000] [Batch 60/100] [D loss: -182938.343750] [D reg: 28149.421875] [G loss: -461671.562500]\n",
      "[Epoch 4/1000] [Batch 61/100] [D loss: -167631.890625] [D reg: 14316.173828] [G loss: -461671.562500]\n",
      "[Epoch 4/1000] [Batch 62/100] [D loss: -145053.312500] [D reg: 14296.997070] [G loss: -461671.562500]\n",
      "[Epoch 4/1000] [Batch 63/100] [D loss: -131891.906250] [D reg: 15429.064453] [G loss: -461671.562500]\n",
      "[Epoch 4/1000] [Batch 64/100] [D loss: -159011.718750] [D reg: 15460.947266] [G loss: -305499.562500]\n",
      "[Epoch 4/1000] [Batch 65/100] [D loss: -151514.343750] [D reg: 15813.583008] [G loss: -305499.562500]\n",
      "[Epoch 4/1000] [Batch 66/100] [D loss: -155707.859375] [D reg: 13751.668945] [G loss: -305499.562500]\n",
      "[Epoch 4/1000] [Batch 67/100] [D loss: -149104.578125] [D reg: 16358.745117] [G loss: -305499.562500]\n",
      "[Epoch 4/1000] [Batch 68/100] [D loss: -160472.843750] [D reg: 19934.660156] [G loss: -305499.562500]\n",
      "[Epoch 4/1000] [Batch 69/100] [D loss: -148612.140625] [D reg: 20302.166016] [G loss: -327264.375000]\n",
      "[Epoch 4/1000] [Batch 70/100] [D loss: -168842.703125] [D reg: 25864.914062] [G loss: -327264.375000]\n",
      "[Epoch 4/1000] [Batch 71/100] [D loss: -191624.171875] [D reg: 33387.984375] [G loss: -327264.375000]\n",
      "[Epoch 4/1000] [Batch 72/100] [D loss: -183903.500000] [D reg: 25124.988281] [G loss: -327264.375000]\n",
      "[Epoch 4/1000] [Batch 73/100] [D loss: -196706.296875] [D reg: 26146.875000] [G loss: -327264.375000]\n",
      "[Epoch 4/1000] [Batch 74/100] [D loss: -188560.328125] [D reg: 20150.958984] [G loss: -355340.312500]\n",
      "[Epoch 4/1000] [Batch 75/100] [D loss: -206941.625000] [D reg: 26691.591797] [G loss: -355340.312500]\n",
      "[Epoch 4/1000] [Batch 76/100] [D loss: -163635.515625] [D reg: 17057.732422] [G loss: -355340.312500]\n",
      "[Epoch 4/1000] [Batch 77/100] [D loss: -164400.312500] [D reg: 16957.150391] [G loss: -355340.312500]\n",
      "[Epoch 4/1000] [Batch 78/100] [D loss: -157021.625000] [D reg: 16157.677734] [G loss: -355340.312500]\n",
      "[Epoch 4/1000] [Batch 79/100] [D loss: -163277.531250] [D reg: 19698.810547] [G loss: -308953.750000]\n",
      "[Epoch 4/1000] [Batch 80/100] [D loss: -175992.046875] [D reg: 21716.427734] [G loss: -308953.750000]\n",
      "[Epoch 4/1000] [Batch 81/100] [D loss: -174331.859375] [D reg: 21427.445312] [G loss: -308953.750000]\n",
      "[Epoch 4/1000] [Batch 82/100] [D loss: -186620.406250] [D reg: 34764.296875] [G loss: -308953.750000]\n",
      "[Epoch 4/1000] [Batch 83/100] [D loss: -268184.875000] [D reg: 59808.000000] [G loss: -308953.750000]\n",
      "[Epoch 4/1000] [Batch 84/100] [D loss: -238948.359375] [D reg: 35069.242188] [G loss: -428816.343750]\n",
      "[Epoch 4/1000] [Batch 85/100] [D loss: -260104.531250] [D reg: 54101.070312] [G loss: -428816.343750]\n",
      "[Epoch 4/1000] [Batch 86/100] [D loss: -328734.687500] [D reg: 101357.171875] [G loss: -428816.343750]\n",
      "[Epoch 4/1000] [Batch 87/100] [D loss: -360251.500000] [D reg: 165521.281250] [G loss: -428816.343750]\n",
      "[Epoch 4/1000] [Batch 88/100] [D loss: -341314.000000] [D reg: 201697.437500] [G loss: -428816.343750]\n",
      "[Epoch 4/1000] [Batch 89/100] [D loss: -397105.218750] [D reg: 220843.312500] [G loss: -710231.687500]\n",
      "[Epoch 4/1000] [Batch 90/100] [D loss: -359333.312500] [D reg: 163128.921875] [G loss: -710231.687500]\n",
      "[Epoch 4/1000] [Batch 91/100] [D loss: -383066.031250] [D reg: 165151.656250] [G loss: -710231.687500]\n",
      "[Epoch 4/1000] [Batch 92/100] [D loss: -408095.062500] [D reg: 168706.484375] [G loss: -710231.687500]\n",
      "[Epoch 4/1000] [Batch 93/100] [D loss: -358434.000000] [D reg: 145438.984375] [G loss: -710231.687500]\n",
      "[Epoch 4/1000] [Batch 94/100] [D loss: -378298.125000] [D reg: 188758.890625] [G loss: -717489.125000]\n",
      "[Epoch 4/1000] [Batch 95/100] [D loss: -325487.500000] [D reg: 164989.875000] [G loss: -717489.125000]\n",
      "[Epoch 4/1000] [Batch 96/100] [D loss: -342130.281250] [D reg: 132515.578125] [G loss: -717489.125000]\n",
      "[Epoch 4/1000] [Batch 97/100] [D loss: -344623.687500] [D reg: 107122.570312] [G loss: -717489.125000]\n",
      "[Epoch 4/1000] [Batch 98/100] [D loss: -314779.187500] [D reg: 115287.203125] [G loss: -717489.125000]\n",
      "[Epoch 4/1000] [Batch 99/100] [D loss: -323279.687500] [D reg: 97569.203125] [G loss: -611715.125000]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batches_per_epoch = 100\n",
    "\n",
    "sample_interval = 10\n",
    "Tensor = torch.cuda.FloatTensor if is_cuda else torch.FloatTensor\n",
    "adv_loss = 'wgan-gp'\n",
    "lambda_gp = 10\n",
    "train_gen_every = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    real_images = enumerate(train_holodec_loader)\n",
    "    synthethic_images = iter(train_synthetic_loader)\n",
    "    \n",
    "    for i, (holo_img, holo_label) in real_images:\n",
    "        \n",
    "        try:\n",
    "            synth_img, synth_label = next(synthethic_images)\n",
    "        except:\n",
    "            synthethic_images = iter(train_synthetic_loader)\n",
    "            synth_img, synth_label = next(synthethic_images)\n",
    "            \n",
    "        if holo_img.shape[0] != synth_img.shape[0]:\n",
    "            continue\n",
    "                \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(holo_img.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(holo_img.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(holo_img.type(Tensor))\n",
    "        synthethic_imgs = Variable(synth_img.type(Tensor))\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (holo_img.shape[0], latent_dim))))\n",
    "        # Generate a batch of images\n",
    "        gen_noise = generator(z)\n",
    "        # Add to the synthetic images\n",
    "        gen_imgs = gen_noise #0.5 * (synthethic_imgs + gen_noise)\n",
    "        # Discriminate the fake images\n",
    "        verdict = discriminator(gen_imgs)\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        requires_grad(generator, True)\n",
    "        requires_grad(discriminator, False)\n",
    "        \n",
    "        if (i + 1) % train_gen_every == 0 or i == 0:\n",
    "            \n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            if adv_loss == 'wgan-gp':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'hinge':\n",
    "                g_loss = -verdict.mean()\n",
    "            elif adv_loss == 'bce':\n",
    "                g_loss = adversarial_loss(verdict, valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        requires_grad(generator, False)\n",
    "        requires_grad(discriminator, True)\n",
    "        \n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        if adv_loss == 'wgan-gp':\n",
    "            real_loss = -torch.mean(discriminator(synthethic_imgs))\n",
    "            fake_loss = discriminator(gen_imgs.detach()).mean() / 2\n",
    "            #fake_loss += discriminator(synthethic_imgs.detach()).mean() / 2\n",
    "        elif adv_loss == 'hinge':\n",
    "            real_loss = torch.nn.ReLU()(1.0 - discriminator(real_imgs)).mean()\n",
    "            fake_loss = torch.nn.ReLU()(1.0 + discriminator(gen_imgs.detach())).mean() / 2\n",
    "            #fake_loss += torch.nn.ReLU()(1.0 + discriminator(synthethic_imgs.detach())).mean() / 2\n",
    "        elif adv_loss == 'bce':\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) / 2\n",
    "            #fake_loss += adversarial_loss(discriminator(synthethic_imgs.detach()), fake) / 2\n",
    "        d_loss = real_loss + fake_loss\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        if adv_loss == 'wgan-gp':\n",
    "            # Compute gradient penalty\n",
    "            alpha = torch.rand(synthethic_imgs.size(0), 1, 1, 1).cuda().expand_as(synthethic_imgs)\n",
    "            interpolated = Variable(alpha * synthethic_imgs.data + (1 - alpha) * gen_imgs.data, requires_grad=True)\n",
    "            out = discriminator(interpolated)\n",
    "\n",
    "            grad = torch.autograd.grad(outputs=out,\n",
    "                                       inputs=interpolated,\n",
    "                                       grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                       retain_graph=True,\n",
    "                                       create_graph=True,\n",
    "                                       only_inputs=True)[0]\n",
    "\n",
    "            grad = grad.view(grad.size(0), -1)\n",
    "            grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "            d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "\n",
    "            # Backward + Optimize\n",
    "            d_loss_reg = lambda_gp * d_loss_gp\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss_reg.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [D reg: %f] [G loss: %f]\"\n",
    "            % (epoch, n_epochs, i, batches_per_epoch, d_loss.item(), d_loss_reg.item(), g_loss.item())\n",
    "        )\n",
    "        \n",
    "        if (i + 1) % sample_interval == 0:\n",
    "            save_image(synthethic_imgs.data[:25], f\"../results/gan/images/synth_{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "            save_image(real_imgs.data[:25], f\"../results/gan/images/real_{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "            save_image(gen_imgs.data[:25], f\"../results/gan/images/pred_{epoch}_{i}.png\", nrow=5, normalize=True)\n",
    "            \n",
    "        with open(\"../results/gan/training_log.csv\", \"a+\") as fid:\n",
    "            fid.write(f\"{epoch},{i},{d_loss.item()},{d_loss_reg.item()},{g_loss.item()}\\n\")\n",
    "            \n",
    "        if (i + 1) == batches_per_epoch:\n",
    "            torch.save(generator.state_dict(), f'../results/gan/images/generator_{epoch}.pt')\n",
    "            torch.save(discriminator.state_dict(), f'../results/gan/images/discriminator_{epoch}.pt')\n",
    "            torch.save(optimizer_G.state_dict(), f'../results/gan/images/gen_optimizer_{epoch}.pt')\n",
    "            torch.save(optimizer_D.state_dict(), f'../results/gan/images/dis_optimizer_{epoch}.pt')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
